Running experiment with attn_quant=Symmetric
Namespace(model='deit_small', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=1024, val_batchsize=128, num_workers=8, intsoftmax_exp_n=15, intgelu_exp_n=29, attn_quant='Symmetric')
Model: deit_small_patch16_224
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 149
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : Symmetric
    Number of QuantMatMul: 24
................................calib done
Test: [  0/391]	Time  4.162 ( 4.162)	Acc@1  28.91 ( 28.91)	Acc@5  35.94 ( 35.94)
Test: [ 50/391]	Time  1.639 ( 1.681)	Acc@1  25.78 ( 19.32)	Acc@5  39.84 ( 27.25)
Test: [100/391]	Time  1.626 ( 1.655)	Acc@1  31.25 ( 20.09)	Acc@5  50.78 ( 33.63)
Test: [150/391]	Time  0.897 ( 1.443)	Acc@1   2.34 ( 18.24)	Acc@5   7.03 ( 31.29)
Test: [200/391]	Time  0.897 ( 1.307)	Acc@1   5.47 ( 16.46)	Acc@5  13.28 ( 27.99)
Test: [250/391]	Time  0.897 ( 1.226)	Acc@1   4.69 ( 14.68)	Acc@5  14.84 ( 25.48)
Test: [300/391]	Time  0.900 ( 1.171)	Acc@1  14.84 ( 13.74)	Acc@5  19.53 ( 23.87)
Test: [350/391]	Time  0.897 ( 1.132)	Acc@1   9.38 ( 13.10)	Acc@5  17.19 ( 22.99)
 * Prec@1 12.266 Prec@5 21.502
Time: 468.85
-----------------------------------------------------
Running experiment with attn_quant=Log2_2x_Quantizer_int
Namespace(model='deit_small', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=1024, val_batchsize=128, num_workers=8, intsoftmax_exp_n=15, intgelu_exp_n=29, attn_quant='Log2_2x_Quantizer_int')
Model: deit_small_patch16_224
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 12
    - type : Log2_2x_Quantizer_int
    Number of QuantMatMul: 24
................................calib done
Test: [  0/391]	Time  3.245 ( 3.245)	Acc@1  89.06 ( 89.06)	Acc@5  99.22 ( 99.22)
Test: [ 50/391]	Time  1.286 ( 1.324)	Acc@1  96.09 ( 83.27)	Acc@5  99.22 ( 95.85)
Test: [100/391]	Time  1.287 ( 1.305)	Acc@1  84.38 ( 82.00)	Acc@5  96.88 ( 96.02)
Test: [150/391]	Time  1.285 ( 1.299)	Acc@1  77.34 ( 81.93)	Acc@5  94.53 ( 96.06)
Test: [200/391]	Time  1.285 ( 1.296)	Acc@1  73.44 ( 79.34)	Acc@5  92.97 ( 94.65)
Test: [250/391]	Time  1.285 ( 1.294)	Acc@1  85.16 ( 78.16)	Acc@5  90.62 ( 93.89)
Test: [300/391]	Time  1.287 ( 1.292)	Acc@1  82.03 ( 77.16)	Acc@5  91.41 ( 93.18)
Test: [350/391]	Time  1.287 ( 1.292)	Acc@1  66.41 ( 76.40)	Acc@5  89.84 ( 92.78)
 * Prec@1 76.280 Prec@5 92.822
Time: 522.67
-----------------------------------------------------
Running experiment with attn_quant=Log2_Quantizer_int
Namespace(model='deit_small', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=1024, val_batchsize=128, num_workers=8, intsoftmax_exp_n=15, intgelu_exp_n=29, attn_quant='Log2_Quantizer_int')
Model: deit_small_patch16_224
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : Log2_Quantizer_int
    Number of QuantMatMul: 24
................................calib done
Test: [  0/391]	Time  3.108 ( 3.108)	Acc@1  70.31 ( 70.31)	Acc@5  87.50 ( 87.50)
Test: [ 50/391]	Time  0.911 ( 0.954)	Acc@1  84.38 ( 69.72)	Acc@5  96.09 ( 87.79)
Test: [100/391]	Time  0.912 ( 0.933)	Acc@1  71.88 ( 69.57)	Acc@5  92.97 ( 88.79)
Test: [150/391]	Time  0.911 ( 0.926)	Acc@1  64.84 ( 70.08)	Acc@5  89.06 ( 89.16)
Test: [200/391]	Time  0.910 ( 0.922)	Acc@1  50.00 ( 66.31)	Acc@5  80.47 ( 86.01)
Test: [250/391]	Time  0.912 ( 0.920)	Acc@1  65.62 ( 64.17)	Acc@5  76.56 ( 84.33)
Test: [300/391]	Time  0.913 ( 0.919)	Acc@1  66.41 ( 62.40)	Acc@5  78.91 ( 82.87)
Test: [350/391]	Time  0.911 ( 0.917)	Acc@1  41.41 ( 61.07)	Acc@5  71.09 ( 81.97)
 * Prec@1 61.106 Prec@5 82.010
Time: 373.18
-----------------------------------------------------
Running experiment with attn_quant=Log2_Quantizer_fp
Namespace(model='deit_small', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=1024, val_batchsize=128, num_workers=8, intsoftmax_exp_n=15, intgelu_exp_n=29, attn_quant='Log2_Quantizer_fp')
Model: deit_small_patch16_224
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : Log2_Quantizer_fp
    Number of QuantMatMul: 24
................................calib done
Test: [  0/391]	Time  3.102 ( 3.102)	Acc@1  86.72 ( 86.72)	Acc@5  98.44 ( 98.44)
Test: [ 50/391]	Time  0.879 ( 0.923)	Acc@1  96.09 ( 82.49)	Acc@5 100.00 ( 95.51)
Test: [100/391]	Time  0.881 ( 0.902)	Acc@1  85.16 ( 81.29)	Acc@5  96.88 ( 95.68)
Test: [150/391]	Time  0.879 ( 0.895)	Acc@1  74.22 ( 81.29)	Acc@5  94.53 ( 95.79)
Test: [200/391]	Time  0.879 ( 0.891)	Acc@1  74.22 ( 78.63)	Acc@5  92.97 ( 94.33)
Test: [250/391]	Time  0.880 ( 0.889)	Acc@1  85.94 ( 77.50)	Acc@5  90.62 ( 93.62)
Test: [300/391]	Time  0.882 ( 0.887)	Acc@1  81.25 ( 76.57)	Acc@5  92.19 ( 92.99)
Test: [350/391]	Time  0.880 ( 0.886)	Acc@1  74.22 ( 75.76)	Acc@5  87.50 ( 92.62)
 * Prec@1 75.766 Prec@5 92.686
Time: 360.71
-----------------------------------------------------
Running experiment with attn_quant=LogSqrt2_Quantizer_fp,
usage: quant_train.py [-h]
                      [--model {deit_tiny,deit_small,deit_base,vit_base,vit_large,swin_tiny,swin_small,swin_base}]
                      [--dataset DIR] [--nb-classes NB_CLASSES]
                      [--device DEVICE] [--print-freq PRINT_FREQ]
                      [--seed SEED] [--output-dir OUTPUT_DIR]
                      [--calib_batchsize CALIB_BATCHSIZE]
                      [--calib_images CALIB_IMAGES]
                      [--val_batchsize VAL_BATCHSIZE]
                      [--num_workers NUM_WORKERS]
                      [--intsoftmax_exp_n INTSOFTMAX_EXP_N]
                      [--intgelu_exp_n INTGELU_EXP_N]
                      [--attn_quant {Symmetric,Log2_2x_Quantizer_int,Log2_Quantizer_int,Log2_Quantizer_fp,LogSqrt2_Quantizer_fp,NoQuant}]
quant_train.py: error: argument --attn_quant: invalid choice: 'LogSqrt2_Quantizer_fp,' (choose from 'Symmetric', 'Log2_2x_Quantizer_int', 'Log2_Quantizer_int', 'Log2_Quantizer_fp', 'LogSqrt2_Quantizer_fp', 'NoQuant')
-----------------------------------------------------
Running experiment with attn_quant=NoQuant
Namespace(model='deit_small', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=1024, val_batchsize=128, num_workers=8, intsoftmax_exp_n=15, intgelu_exp_n=29, attn_quant='NoQuant')
Model: deit_small_patch16_224
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
................................calib done
Test: [  0/391]	Time  3.059 ( 3.059)	Acc@1  88.28 ( 88.28)	Acc@5  99.22 ( 99.22)
Test: [ 50/391]	Time  0.845 ( 0.889)	Acc@1  94.53 ( 83.07)	Acc@5  99.22 ( 95.88)
Test: [100/391]	Time  0.848 ( 0.868)	Acc@1  85.16 ( 81.89)	Acc@5  96.09 ( 96.10)
Test: [150/391]	Time  0.845 ( 0.861)	Acc@1  75.78 ( 81.93)	Acc@5  96.88 ( 96.27)
Test: [200/391]	Time  0.846 ( 0.857)	Acc@1  72.66 ( 79.41)	Acc@5  92.97 ( 94.85)
Test: [250/391]	Time  0.846 ( 0.855)	Acc@1  86.72 ( 78.09)	Acc@5  90.62 ( 94.02)
Test: [300/391]	Time  0.847 ( 0.853)	Acc@1  80.47 ( 77.09)	Acc@5  91.41 ( 93.42)
Test: [350/391]	Time  0.846 ( 0.852)	Acc@1  67.97 ( 76.34)	Acc@5  85.94 ( 93.00)
 * Prec@1 76.314 Prec@5 93.018
Time: 346.75
-----------------------------------------------------
