Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=0, intgelu_exp_n=0
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=0, intgelu_exp_n=0, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  0
IntGELU    | n:  0
IntSoftmax | n:  0
IntGELU    | n:  0
IntSoftmax | n:  0
IntGELU    | n:  0
IntSoftmax | n:  0
IntGELU    | n:  0
IntSoftmax | n:  0
IntGELU    | n:  0
IntSoftmax | n:  0
IntGELU    | n:  0
IntSoftmax | n:  0
IntGELU    | n:  0
IntSoftmax | n:  0
IntGELU    | n:  0
IntSoftmax | n:  0
IntGELU    | n:  0
IntSoftmax | n:  0
IntGELU    | n:  0
IntSoftmax | n:  0
IntGELU    | n:  0
IntSoftmax | n:  0
IntGELU    | n:  0
.calib done
Test: [  0/391]	Time  3.930 ( 3.930)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.94
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=0, intgelu_exp_n=1
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=0, intgelu_exp_n=1, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  0
IntGELU    | n:  1
IntSoftmax | n:  0
IntGELU    | n:  1
IntSoftmax | n:  0
IntGELU    | n:  1
IntSoftmax | n:  0
IntGELU    | n:  1
IntSoftmax | n:  0
IntGELU    | n:  1
IntSoftmax | n:  0
IntGELU    | n:  1
IntSoftmax | n:  0
IntGELU    | n:  1
IntSoftmax | n:  0
IntGELU    | n:  1
IntSoftmax | n:  0
IntGELU    | n:  1
IntSoftmax | n:  0
IntGELU    | n:  1
IntSoftmax | n:  0
IntGELU    | n:  1
IntSoftmax | n:  0
IntGELU    | n:  1
.calib done
Test: [  0/391]	Time  3.938 ( 3.938)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.00
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=0, intgelu_exp_n=2
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=0, intgelu_exp_n=2, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  0
IntGELU    | n:  2
IntSoftmax | n:  0
IntGELU    | n:  2
IntSoftmax | n:  0
IntGELU    | n:  2
IntSoftmax | n:  0
IntGELU    | n:  2
IntSoftmax | n:  0
IntGELU    | n:  2
IntSoftmax | n:  0
IntGELU    | n:  2
IntSoftmax | n:  0
IntGELU    | n:  2
IntSoftmax | n:  0
IntGELU    | n:  2
IntSoftmax | n:  0
IntGELU    | n:  2
IntSoftmax | n:  0
IntGELU    | n:  2
IntSoftmax | n:  0
IntGELU    | n:  2
IntSoftmax | n:  0
IntGELU    | n:  2
.calib done
Test: [  0/391]	Time  3.859 ( 3.859)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.89
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=0, intgelu_exp_n=3
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=0, intgelu_exp_n=3, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  0
IntGELU    | n:  3
IntSoftmax | n:  0
IntGELU    | n:  3
IntSoftmax | n:  0
IntGELU    | n:  3
IntSoftmax | n:  0
IntGELU    | n:  3
IntSoftmax | n:  0
IntGELU    | n:  3
IntSoftmax | n:  0
IntGELU    | n:  3
IntSoftmax | n:  0
IntGELU    | n:  3
IntSoftmax | n:  0
IntGELU    | n:  3
IntSoftmax | n:  0
IntGELU    | n:  3
IntSoftmax | n:  0
IntGELU    | n:  3
IntSoftmax | n:  0
IntGELU    | n:  3
IntSoftmax | n:  0
IntGELU    | n:  3
.calib done
Test: [  0/391]	Time  3.929 ( 3.929)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.94
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=0, intgelu_exp_n=4
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=0, intgelu_exp_n=4, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  0
IntGELU    | n:  4
IntSoftmax | n:  0
IntGELU    | n:  4
IntSoftmax | n:  0
IntGELU    | n:  4
IntSoftmax | n:  0
IntGELU    | n:  4
IntSoftmax | n:  0
IntGELU    | n:  4
IntSoftmax | n:  0
IntGELU    | n:  4
IntSoftmax | n:  0
IntGELU    | n:  4
IntSoftmax | n:  0
IntGELU    | n:  4
IntSoftmax | n:  0
IntGELU    | n:  4
IntSoftmax | n:  0
IntGELU    | n:  4
IntSoftmax | n:  0
IntGELU    | n:  4
IntSoftmax | n:  0
IntGELU    | n:  4
.calib done
Test: [  0/391]	Time  3.641 ( 3.641)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.68
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=0, intgelu_exp_n=5
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=0, intgelu_exp_n=5, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  0
IntGELU    | n:  5
IntSoftmax | n:  0
IntGELU    | n:  5
IntSoftmax | n:  0
IntGELU    | n:  5
IntSoftmax | n:  0
IntGELU    | n:  5
IntSoftmax | n:  0
IntGELU    | n:  5
IntSoftmax | n:  0
IntGELU    | n:  5
IntSoftmax | n:  0
IntGELU    | n:  5
IntSoftmax | n:  0
IntGELU    | n:  5
IntSoftmax | n:  0
IntGELU    | n:  5
IntSoftmax | n:  0
IntGELU    | n:  5
IntSoftmax | n:  0
IntGELU    | n:  5
IntSoftmax | n:  0
IntGELU    | n:  5
.calib done
Test: [  0/391]	Time  3.787 ( 3.787)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.83
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=0, intgelu_exp_n=6
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=0, intgelu_exp_n=6, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  0
IntGELU    | n:  6
IntSoftmax | n:  0
IntGELU    | n:  6
IntSoftmax | n:  0
IntGELU    | n:  6
IntSoftmax | n:  0
IntGELU    | n:  6
IntSoftmax | n:  0
IntGELU    | n:  6
IntSoftmax | n:  0
IntGELU    | n:  6
IntSoftmax | n:  0
IntGELU    | n:  6
IntSoftmax | n:  0
IntGELU    | n:  6
IntSoftmax | n:  0
IntGELU    | n:  6
IntSoftmax | n:  0
IntGELU    | n:  6
IntSoftmax | n:  0
IntGELU    | n:  6
IntSoftmax | n:  0
IntGELU    | n:  6
.calib done
Test: [  0/391]	Time  3.936 ( 3.936)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.91
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=0, intgelu_exp_n=7
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=0, intgelu_exp_n=7, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  0
IntGELU    | n:  7
IntSoftmax | n:  0
IntGELU    | n:  7
IntSoftmax | n:  0
IntGELU    | n:  7
IntSoftmax | n:  0
IntGELU    | n:  7
IntSoftmax | n:  0
IntGELU    | n:  7
IntSoftmax | n:  0
IntGELU    | n:  7
IntSoftmax | n:  0
IntGELU    | n:  7
IntSoftmax | n:  0
IntGELU    | n:  7
IntSoftmax | n:  0
IntGELU    | n:  7
IntSoftmax | n:  0
IntGELU    | n:  7
IntSoftmax | n:  0
IntGELU    | n:  7
IntSoftmax | n:  0
IntGELU    | n:  7
.calib done
Test: [  0/391]	Time  3.935 ( 3.935)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=0, intgelu_exp_n=8
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=0, intgelu_exp_n=8, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  0
IntGELU    | n:  8
IntSoftmax | n:  0
IntGELU    | n:  8
IntSoftmax | n:  0
IntGELU    | n:  8
IntSoftmax | n:  0
IntGELU    | n:  8
IntSoftmax | n:  0
IntGELU    | n:  8
IntSoftmax | n:  0
IntGELU    | n:  8
IntSoftmax | n:  0
IntGELU    | n:  8
IntSoftmax | n:  0
IntGELU    | n:  8
IntSoftmax | n:  0
IntGELU    | n:  8
IntSoftmax | n:  0
IntGELU    | n:  8
IntSoftmax | n:  0
IntGELU    | n:  8
IntSoftmax | n:  0
IntGELU    | n:  8
.calib done
Test: [  0/391]	Time  3.627 ( 3.627)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.70
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=0, intgelu_exp_n=9
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=0, intgelu_exp_n=9, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  0
IntGELU    | n:  9
IntSoftmax | n:  0
IntGELU    | n:  9
IntSoftmax | n:  0
IntGELU    | n:  9
IntSoftmax | n:  0
IntGELU    | n:  9
IntSoftmax | n:  0
IntGELU    | n:  9
IntSoftmax | n:  0
IntGELU    | n:  9
IntSoftmax | n:  0
IntGELU    | n:  9
IntSoftmax | n:  0
IntGELU    | n:  9
IntSoftmax | n:  0
IntGELU    | n:  9
IntSoftmax | n:  0
IntGELU    | n:  9
IntSoftmax | n:  0
IntGELU    | n:  9
IntSoftmax | n:  0
IntGELU    | n:  9
.calib done
Test: [  0/391]	Time  3.939 ( 3.939)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=0, intgelu_exp_n=10
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=0, intgelu_exp_n=10, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  0
IntGELU    | n:  10
IntSoftmax | n:  0
IntGELU    | n:  10
IntSoftmax | n:  0
IntGELU    | n:  10
IntSoftmax | n:  0
IntGELU    | n:  10
IntSoftmax | n:  0
IntGELU    | n:  10
IntSoftmax | n:  0
IntGELU    | n:  10
IntSoftmax | n:  0
IntGELU    | n:  10
IntSoftmax | n:  0
IntGELU    | n:  10
IntSoftmax | n:  0
IntGELU    | n:  10
IntSoftmax | n:  0
IntGELU    | n:  10
IntSoftmax | n:  0
IntGELU    | n:  10
IntSoftmax | n:  0
IntGELU    | n:  10
.calib done
Test: [  0/391]	Time  3.932 ( 3.932)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.93
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=0, intgelu_exp_n=11
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=0, intgelu_exp_n=11, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  0
IntGELU    | n:  11
IntSoftmax | n:  0
IntGELU    | n:  11
IntSoftmax | n:  0
IntGELU    | n:  11
IntSoftmax | n:  0
IntGELU    | n:  11
IntSoftmax | n:  0
IntGELU    | n:  11
IntSoftmax | n:  0
IntGELU    | n:  11
IntSoftmax | n:  0
IntGELU    | n:  11
IntSoftmax | n:  0
IntGELU    | n:  11
IntSoftmax | n:  0
IntGELU    | n:  11
IntSoftmax | n:  0
IntGELU    | n:  11
IntSoftmax | n:  0
IntGELU    | n:  11
IntSoftmax | n:  0
IntGELU    | n:  11
.calib done
Test: [  0/391]	Time  3.936 ( 3.936)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=0, intgelu_exp_n=12
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=0, intgelu_exp_n=12, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  0
IntGELU    | n:  12
IntSoftmax | n:  0
IntGELU    | n:  12
IntSoftmax | n:  0
IntGELU    | n:  12
IntSoftmax | n:  0
IntGELU    | n:  12
IntSoftmax | n:  0
IntGELU    | n:  12
IntSoftmax | n:  0
IntGELU    | n:  12
IntSoftmax | n:  0
IntGELU    | n:  12
IntSoftmax | n:  0
IntGELU    | n:  12
IntSoftmax | n:  0
IntGELU    | n:  12
IntSoftmax | n:  0
IntGELU    | n:  12
IntSoftmax | n:  0
IntGELU    | n:  12
IntSoftmax | n:  0
IntGELU    | n:  12
.calib done
Test: [  0/391]	Time  3.961 ( 3.961)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.02
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=0, intgelu_exp_n=13
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=0, intgelu_exp_n=13, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  0
IntGELU    | n:  13
IntSoftmax | n:  0
IntGELU    | n:  13
IntSoftmax | n:  0
IntGELU    | n:  13
IntSoftmax | n:  0
IntGELU    | n:  13
IntSoftmax | n:  0
IntGELU    | n:  13
IntSoftmax | n:  0
IntGELU    | n:  13
IntSoftmax | n:  0
IntGELU    | n:  13
IntSoftmax | n:  0
IntGELU    | n:  13
IntSoftmax | n:  0
IntGELU    | n:  13
IntSoftmax | n:  0
IntGELU    | n:  13
IntSoftmax | n:  0
IntGELU    | n:  13
IntSoftmax | n:  0
IntGELU    | n:  13
.calib done
Test: [  0/391]	Time  3.890 ( 3.890)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.90
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=0, intgelu_exp_n=14
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=0, intgelu_exp_n=14, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  0
IntGELU    | n:  14
IntSoftmax | n:  0
IntGELU    | n:  14
IntSoftmax | n:  0
IntGELU    | n:  14
IntSoftmax | n:  0
IntGELU    | n:  14
IntSoftmax | n:  0
IntGELU    | n:  14
IntSoftmax | n:  0
IntGELU    | n:  14
IntSoftmax | n:  0
IntGELU    | n:  14
IntSoftmax | n:  0
IntGELU    | n:  14
IntSoftmax | n:  0
IntGELU    | n:  14
IntSoftmax | n:  0
IntGELU    | n:  14
IntSoftmax | n:  0
IntGELU    | n:  14
IntSoftmax | n:  0
IntGELU    | n:  14
.calib done
Test: [  0/391]	Time  3.677 ( 3.677)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.72
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=0, intgelu_exp_n=15
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=0, intgelu_exp_n=15, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  0
IntGELU    | n:  15
IntSoftmax | n:  0
IntGELU    | n:  15
IntSoftmax | n:  0
IntGELU    | n:  15
IntSoftmax | n:  0
IntGELU    | n:  15
IntSoftmax | n:  0
IntGELU    | n:  15
IntSoftmax | n:  0
IntGELU    | n:  15
IntSoftmax | n:  0
IntGELU    | n:  15
IntSoftmax | n:  0
IntGELU    | n:  15
IntSoftmax | n:  0
IntGELU    | n:  15
IntSoftmax | n:  0
IntGELU    | n:  15
IntSoftmax | n:  0
IntGELU    | n:  15
IntSoftmax | n:  0
IntGELU    | n:  15
.calib done
Test: [  0/391]	Time  3.724 ( 3.724)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.79
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=0, intgelu_exp_n=16
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=0, intgelu_exp_n=16, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  0
IntGELU    | n:  16
IntSoftmax | n:  0
IntGELU    | n:  16
IntSoftmax | n:  0
IntGELU    | n:  16
IntSoftmax | n:  0
IntGELU    | n:  16
IntSoftmax | n:  0
IntGELU    | n:  16
IntSoftmax | n:  0
IntGELU    | n:  16
IntSoftmax | n:  0
IntGELU    | n:  16
IntSoftmax | n:  0
IntGELU    | n:  16
IntSoftmax | n:  0
IntGELU    | n:  16
IntSoftmax | n:  0
IntGELU    | n:  16
IntSoftmax | n:  0
IntGELU    | n:  16
IntSoftmax | n:  0
IntGELU    | n:  16
.calib done
Test: [  0/391]	Time  3.929 ( 3.929)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=0, intgelu_exp_n=17
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=0, intgelu_exp_n=17, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  0
IntGELU    | n:  17
IntSoftmax | n:  0
IntGELU    | n:  17
IntSoftmax | n:  0
IntGELU    | n:  17
IntSoftmax | n:  0
IntGELU    | n:  17
IntSoftmax | n:  0
IntGELU    | n:  17
IntSoftmax | n:  0
IntGELU    | n:  17
IntSoftmax | n:  0
IntGELU    | n:  17
IntSoftmax | n:  0
IntGELU    | n:  17
IntSoftmax | n:  0
IntGELU    | n:  17
IntSoftmax | n:  0
IntGELU    | n:  17
IntSoftmax | n:  0
IntGELU    | n:  17
IntSoftmax | n:  0
IntGELU    | n:  17
.calib done
Test: [  0/391]	Time  3.944 ( 3.944)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=0, intgelu_exp_n=18
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=0, intgelu_exp_n=18, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  0
IntGELU    | n:  18
IntSoftmax | n:  0
IntGELU    | n:  18
IntSoftmax | n:  0
IntGELU    | n:  18
IntSoftmax | n:  0
IntGELU    | n:  18
IntSoftmax | n:  0
IntGELU    | n:  18
IntSoftmax | n:  0
IntGELU    | n:  18
IntSoftmax | n:  0
IntGELU    | n:  18
IntSoftmax | n:  0
IntGELU    | n:  18
IntSoftmax | n:  0
IntGELU    | n:  18
IntSoftmax | n:  0
IntGELU    | n:  18
IntSoftmax | n:  0
IntGELU    | n:  18
IntSoftmax | n:  0
IntGELU    | n:  18
.calib done
Test: [  0/391]	Time  3.940 ( 3.940)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.95
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=0, intgelu_exp_n=19
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=0, intgelu_exp_n=19, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  0
IntGELU    | n:  19
IntSoftmax | n:  0
IntGELU    | n:  19
IntSoftmax | n:  0
IntGELU    | n:  19
IntSoftmax | n:  0
IntGELU    | n:  19
IntSoftmax | n:  0
IntGELU    | n:  19
IntSoftmax | n:  0
IntGELU    | n:  19
IntSoftmax | n:  0
IntGELU    | n:  19
IntSoftmax | n:  0
IntGELU    | n:  19
IntSoftmax | n:  0
IntGELU    | n:  19
IntSoftmax | n:  0
IntGELU    | n:  19
IntSoftmax | n:  0
IntGELU    | n:  19
IntSoftmax | n:  0
IntGELU    | n:  19
.calib done
Test: [  0/391]	Time  3.773 ( 3.773)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.82
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=0, intgelu_exp_n=20
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=0, intgelu_exp_n=20, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  0
IntGELU    | n:  20
IntSoftmax | n:  0
IntGELU    | n:  20
IntSoftmax | n:  0
IntGELU    | n:  20
IntSoftmax | n:  0
IntGELU    | n:  20
IntSoftmax | n:  0
IntGELU    | n:  20
IntSoftmax | n:  0
IntGELU    | n:  20
IntSoftmax | n:  0
IntGELU    | n:  20
IntSoftmax | n:  0
IntGELU    | n:  20
IntSoftmax | n:  0
IntGELU    | n:  20
IntSoftmax | n:  0
IntGELU    | n:  20
IntSoftmax | n:  0
IntGELU    | n:  20
IntSoftmax | n:  0
IntGELU    | n:  20
.calib done
Test: [  0/391]	Time  3.710 ( 3.710)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.85
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=0, intgelu_exp_n=21
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=0, intgelu_exp_n=21, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  0
IntGELU    | n:  21
IntSoftmax | n:  0
IntGELU    | n:  21
IntSoftmax | n:  0
IntGELU    | n:  21
IntSoftmax | n:  0
IntGELU    | n:  21
IntSoftmax | n:  0
IntGELU    | n:  21
IntSoftmax | n:  0
IntGELU    | n:  21
IntSoftmax | n:  0
IntGELU    | n:  21
IntSoftmax | n:  0
IntGELU    | n:  21
IntSoftmax | n:  0
IntGELU    | n:  21
IntSoftmax | n:  0
IntGELU    | n:  21
IntSoftmax | n:  0
IntGELU    | n:  21
IntSoftmax | n:  0
IntGELU    | n:  21
.calib done
Test: [  0/391]	Time  3.847 ( 3.847)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.91
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=0, intgelu_exp_n=22
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=0, intgelu_exp_n=22, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  0
IntGELU    | n:  22
IntSoftmax | n:  0
IntGELU    | n:  22
IntSoftmax | n:  0
IntGELU    | n:  22
IntSoftmax | n:  0
IntGELU    | n:  22
IntSoftmax | n:  0
IntGELU    | n:  22
IntSoftmax | n:  0
IntGELU    | n:  22
IntSoftmax | n:  0
IntGELU    | n:  22
IntSoftmax | n:  0
IntGELU    | n:  22
IntSoftmax | n:  0
IntGELU    | n:  22
IntSoftmax | n:  0
IntGELU    | n:  22
IntSoftmax | n:  0
IntGELU    | n:  22
IntSoftmax | n:  0
IntGELU    | n:  22
.calib done
Test: [  0/391]	Time  3.696 ( 3.696)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.80
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=0, intgelu_exp_n=23
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=0, intgelu_exp_n=23, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  0
IntGELU    | n:  23
IntSoftmax | n:  0
IntGELU    | n:  23
IntSoftmax | n:  0
IntGELU    | n:  23
IntSoftmax | n:  0
IntGELU    | n:  23
IntSoftmax | n:  0
IntGELU    | n:  23
IntSoftmax | n:  0
IntGELU    | n:  23
IntSoftmax | n:  0
IntGELU    | n:  23
IntSoftmax | n:  0
IntGELU    | n:  23
IntSoftmax | n:  0
IntGELU    | n:  23
IntSoftmax | n:  0
IntGELU    | n:  23
IntSoftmax | n:  0
IntGELU    | n:  23
IntSoftmax | n:  0
IntGELU    | n:  23
.calib done
Test: [  0/391]	Time  4.026 ( 4.026)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.02
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=0, intgelu_exp_n=24
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=0, intgelu_exp_n=24, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  0
IntGELU    | n:  24
IntSoftmax | n:  0
IntGELU    | n:  24
IntSoftmax | n:  0
IntGELU    | n:  24
IntSoftmax | n:  0
IntGELU    | n:  24
IntSoftmax | n:  0
IntGELU    | n:  24
IntSoftmax | n:  0
IntGELU    | n:  24
IntSoftmax | n:  0
IntGELU    | n:  24
IntSoftmax | n:  0
IntGELU    | n:  24
IntSoftmax | n:  0
IntGELU    | n:  24
IntSoftmax | n:  0
IntGELU    | n:  24
IntSoftmax | n:  0
IntGELU    | n:  24
IntSoftmax | n:  0
IntGELU    | n:  24
.calib done
Test: [  0/391]	Time  3.679 ( 3.679)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.71
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=0, intgelu_exp_n=25
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=0, intgelu_exp_n=25, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  0
IntGELU    | n:  25
IntSoftmax | n:  0
IntGELU    | n:  25
IntSoftmax | n:  0
IntGELU    | n:  25
IntSoftmax | n:  0
IntGELU    | n:  25
IntSoftmax | n:  0
IntGELU    | n:  25
IntSoftmax | n:  0
IntGELU    | n:  25
IntSoftmax | n:  0
IntGELU    | n:  25
IntSoftmax | n:  0
IntGELU    | n:  25
IntSoftmax | n:  0
IntGELU    | n:  25
IntSoftmax | n:  0
IntGELU    | n:  25
IntSoftmax | n:  0
IntGELU    | n:  25
IntSoftmax | n:  0
IntGELU    | n:  25
.calib done
Test: [  0/391]	Time  3.965 ( 3.965)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.07
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=0, intgelu_exp_n=26
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=0, intgelu_exp_n=26, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  0
IntGELU    | n:  26
IntSoftmax | n:  0
IntGELU    | n:  26
IntSoftmax | n:  0
IntGELU    | n:  26
IntSoftmax | n:  0
IntGELU    | n:  26
IntSoftmax | n:  0
IntGELU    | n:  26
IntSoftmax | n:  0
IntGELU    | n:  26
IntSoftmax | n:  0
IntGELU    | n:  26
IntSoftmax | n:  0
IntGELU    | n:  26
IntSoftmax | n:  0
IntGELU    | n:  26
IntSoftmax | n:  0
IntGELU    | n:  26
IntSoftmax | n:  0
IntGELU    | n:  26
IntSoftmax | n:  0
IntGELU    | n:  26
.calib done
Test: [  0/391]	Time  3.631 ( 3.631)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.72
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=0, intgelu_exp_n=27
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=0, intgelu_exp_n=27, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  0
IntGELU    | n:  27
IntSoftmax | n:  0
IntGELU    | n:  27
IntSoftmax | n:  0
IntGELU    | n:  27
IntSoftmax | n:  0
IntGELU    | n:  27
IntSoftmax | n:  0
IntGELU    | n:  27
IntSoftmax | n:  0
IntGELU    | n:  27
IntSoftmax | n:  0
IntGELU    | n:  27
IntSoftmax | n:  0
IntGELU    | n:  27
IntSoftmax | n:  0
IntGELU    | n:  27
IntSoftmax | n:  0
IntGELU    | n:  27
IntSoftmax | n:  0
IntGELU    | n:  27
IntSoftmax | n:  0
IntGELU    | n:  27
.calib done
Test: [  0/391]	Time  3.856 ( 3.856)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.94
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=0, intgelu_exp_n=28
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=0, intgelu_exp_n=28, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  0
IntGELU    | n:  28
IntSoftmax | n:  0
IntGELU    | n:  28
IntSoftmax | n:  0
IntGELU    | n:  28
IntSoftmax | n:  0
IntGELU    | n:  28
IntSoftmax | n:  0
IntGELU    | n:  28
IntSoftmax | n:  0
IntGELU    | n:  28
IntSoftmax | n:  0
IntGELU    | n:  28
IntSoftmax | n:  0
IntGELU    | n:  28
IntSoftmax | n:  0
IntGELU    | n:  28
IntSoftmax | n:  0
IntGELU    | n:  28
IntSoftmax | n:  0
IntGELU    | n:  28
IntSoftmax | n:  0
IntGELU    | n:  28
.calib done
Test: [  0/391]	Time  3.984 ( 3.984)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.08
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=0, intgelu_exp_n=29
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=0, intgelu_exp_n=29, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  0
IntGELU    | n:  29
IntSoftmax | n:  0
IntGELU    | n:  29
IntSoftmax | n:  0
IntGELU    | n:  29
IntSoftmax | n:  0
IntGELU    | n:  29
IntSoftmax | n:  0
IntGELU    | n:  29
IntSoftmax | n:  0
IntGELU    | n:  29
IntSoftmax | n:  0
IntGELU    | n:  29
IntSoftmax | n:  0
IntGELU    | n:  29
IntSoftmax | n:  0
IntGELU    | n:  29
IntSoftmax | n:  0
IntGELU    | n:  29
IntSoftmax | n:  0
IntGELU    | n:  29
IntSoftmax | n:  0
IntGELU    | n:  29
.calib done
Test: [  0/391]	Time  3.960 ( 3.960)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.07
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=0, intgelu_exp_n=30
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=0, intgelu_exp_n=30, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  0
IntGELU    | n:  30
IntSoftmax | n:  0
IntGELU    | n:  30
IntSoftmax | n:  0
IntGELU    | n:  30
IntSoftmax | n:  0
IntGELU    | n:  30
IntSoftmax | n:  0
IntGELU    | n:  30
IntSoftmax | n:  0
IntGELU    | n:  30
IntSoftmax | n:  0
IntGELU    | n:  30
IntSoftmax | n:  0
IntGELU    | n:  30
IntSoftmax | n:  0
IntGELU    | n:  30
IntSoftmax | n:  0
IntGELU    | n:  30
IntSoftmax | n:  0
IntGELU    | n:  30
IntSoftmax | n:  0
IntGELU    | n:  30
.calib done
Test: [  0/391]	Time  3.924 ( 3.924)	Acc@1   0.00 (  0.00)	Acc@5   0.78 (  0.78)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.781
Time: 9.08
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=0, intgelu_exp_n=31
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=0, intgelu_exp_n=31, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  0
IntGELU    | n:  31
IntSoftmax | n:  0
IntGELU    | n:  31
IntSoftmax | n:  0
IntGELU    | n:  31
IntSoftmax | n:  0
IntGELU    | n:  31
IntSoftmax | n:  0
IntGELU    | n:  31
IntSoftmax | n:  0
IntGELU    | n:  31
IntSoftmax | n:  0
IntGELU    | n:  31
IntSoftmax | n:  0
IntGELU    | n:  31
IntSoftmax | n:  0
IntGELU    | n:  31
IntSoftmax | n:  0
IntGELU    | n:  31
IntSoftmax | n:  0
IntGELU    | n:  31
IntSoftmax | n:  0
IntGELU    | n:  31
.calib done
Test: [  0/391]	Time  4.007 ( 4.007)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.32
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=1, intgelu_exp_n=0
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=1, intgelu_exp_n=0, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  1
IntGELU    | n:  0
IntSoftmax | n:  1
IntGELU    | n:  0
IntSoftmax | n:  1
IntGELU    | n:  0
IntSoftmax | n:  1
IntGELU    | n:  0
IntSoftmax | n:  1
IntGELU    | n:  0
IntSoftmax | n:  1
IntGELU    | n:  0
IntSoftmax | n:  1
IntGELU    | n:  0
IntSoftmax | n:  1
IntGELU    | n:  0
IntSoftmax | n:  1
IntGELU    | n:  0
IntSoftmax | n:  1
IntGELU    | n:  0
IntSoftmax | n:  1
IntGELU    | n:  0
IntSoftmax | n:  1
IntGELU    | n:  0
.calib done
Test: [  0/391]	Time  3.884 ( 3.884)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.98
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=1, intgelu_exp_n=1
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=1, intgelu_exp_n=1, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  1
IntGELU    | n:  1
IntSoftmax | n:  1
IntGELU    | n:  1
IntSoftmax | n:  1
IntGELU    | n:  1
IntSoftmax | n:  1
IntGELU    | n:  1
IntSoftmax | n:  1
IntGELU    | n:  1
IntSoftmax | n:  1
IntGELU    | n:  1
IntSoftmax | n:  1
IntGELU    | n:  1
IntSoftmax | n:  1
IntGELU    | n:  1
IntSoftmax | n:  1
IntGELU    | n:  1
IntSoftmax | n:  1
IntGELU    | n:  1
IntSoftmax | n:  1
IntGELU    | n:  1
IntSoftmax | n:  1
IntGELU    | n:  1
.calib done
Test: [  0/391]	Time  3.895 ( 3.895)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.95
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=1, intgelu_exp_n=2
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=1, intgelu_exp_n=2, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  1
IntGELU    | n:  2
IntSoftmax | n:  1
IntGELU    | n:  2
IntSoftmax | n:  1
IntGELU    | n:  2
IntSoftmax | n:  1
IntGELU    | n:  2
IntSoftmax | n:  1
IntGELU    | n:  2
IntSoftmax | n:  1
IntGELU    | n:  2
IntSoftmax | n:  1
IntGELU    | n:  2
IntSoftmax | n:  1
IntGELU    | n:  2
IntSoftmax | n:  1
IntGELU    | n:  2
IntSoftmax | n:  1
IntGELU    | n:  2
IntSoftmax | n:  1
IntGELU    | n:  2
IntSoftmax | n:  1
IntGELU    | n:  2
.calib done
Test: [  0/391]	Time  3.713 ( 3.713)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.79
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=1, intgelu_exp_n=3
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=1, intgelu_exp_n=3, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  1
IntGELU    | n:  3
IntSoftmax | n:  1
IntGELU    | n:  3
IntSoftmax | n:  1
IntGELU    | n:  3
IntSoftmax | n:  1
IntGELU    | n:  3
IntSoftmax | n:  1
IntGELU    | n:  3
IntSoftmax | n:  1
IntGELU    | n:  3
IntSoftmax | n:  1
IntGELU    | n:  3
IntSoftmax | n:  1
IntGELU    | n:  3
IntSoftmax | n:  1
IntGELU    | n:  3
IntSoftmax | n:  1
IntGELU    | n:  3
IntSoftmax | n:  1
IntGELU    | n:  3
IntSoftmax | n:  1
IntGELU    | n:  3
.calib done
Test: [  0/391]	Time  3.696 ( 3.696)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.78
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=1, intgelu_exp_n=4
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=1, intgelu_exp_n=4, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  1
IntGELU    | n:  4
IntSoftmax | n:  1
IntGELU    | n:  4
IntSoftmax | n:  1
IntGELU    | n:  4
IntSoftmax | n:  1
IntGELU    | n:  4
IntSoftmax | n:  1
IntGELU    | n:  4
IntSoftmax | n:  1
IntGELU    | n:  4
IntSoftmax | n:  1
IntGELU    | n:  4
IntSoftmax | n:  1
IntGELU    | n:  4
IntSoftmax | n:  1
IntGELU    | n:  4
IntSoftmax | n:  1
IntGELU    | n:  4
IntSoftmax | n:  1
IntGELU    | n:  4
IntSoftmax | n:  1
IntGELU    | n:  4
.calib done
Test: [  0/391]	Time  3.864 ( 3.864)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.91
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=1, intgelu_exp_n=5
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=1, intgelu_exp_n=5, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  1
IntGELU    | n:  5
IntSoftmax | n:  1
IntGELU    | n:  5
IntSoftmax | n:  1
IntGELU    | n:  5
IntSoftmax | n:  1
IntGELU    | n:  5
IntSoftmax | n:  1
IntGELU    | n:  5
IntSoftmax | n:  1
IntGELU    | n:  5
IntSoftmax | n:  1
IntGELU    | n:  5
IntSoftmax | n:  1
IntGELU    | n:  5
IntSoftmax | n:  1
IntGELU    | n:  5
IntSoftmax | n:  1
IntGELU    | n:  5
IntSoftmax | n:  1
IntGELU    | n:  5
IntSoftmax | n:  1
IntGELU    | n:  5
.calib done
Test: [  0/391]	Time  3.978 ( 3.978)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.01
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=1, intgelu_exp_n=6
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=1, intgelu_exp_n=6, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  1
IntGELU    | n:  6
IntSoftmax | n:  1
IntGELU    | n:  6
IntSoftmax | n:  1
IntGELU    | n:  6
IntSoftmax | n:  1
IntGELU    | n:  6
IntSoftmax | n:  1
IntGELU    | n:  6
IntSoftmax | n:  1
IntGELU    | n:  6
IntSoftmax | n:  1
IntGELU    | n:  6
IntSoftmax | n:  1
IntGELU    | n:  6
IntSoftmax | n:  1
IntGELU    | n:  6
IntSoftmax | n:  1
IntGELU    | n:  6
IntSoftmax | n:  1
IntGELU    | n:  6
IntSoftmax | n:  1
IntGELU    | n:  6
.calib done
Test: [  0/391]	Time  3.962 ( 3.962)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.09
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=1, intgelu_exp_n=7
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=1, intgelu_exp_n=7, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  1
IntGELU    | n:  7
IntSoftmax | n:  1
IntGELU    | n:  7
IntSoftmax | n:  1
IntGELU    | n:  7
IntSoftmax | n:  1
IntGELU    | n:  7
IntSoftmax | n:  1
IntGELU    | n:  7
IntSoftmax | n:  1
IntGELU    | n:  7
IntSoftmax | n:  1
IntGELU    | n:  7
IntSoftmax | n:  1
IntGELU    | n:  7
IntSoftmax | n:  1
IntGELU    | n:  7
IntSoftmax | n:  1
IntGELU    | n:  7
IntSoftmax | n:  1
IntGELU    | n:  7
IntSoftmax | n:  1
IntGELU    | n:  7
.calib done
Test: [  0/391]	Time  3.910 ( 3.910)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.90
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=1, intgelu_exp_n=8
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=1, intgelu_exp_n=8, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  1
IntGELU    | n:  8
IntSoftmax | n:  1
IntGELU    | n:  8
IntSoftmax | n:  1
IntGELU    | n:  8
IntSoftmax | n:  1
IntGELU    | n:  8
IntSoftmax | n:  1
IntGELU    | n:  8
IntSoftmax | n:  1
IntGELU    | n:  8
IntSoftmax | n:  1
IntGELU    | n:  8
IntSoftmax | n:  1
IntGELU    | n:  8
IntSoftmax | n:  1
IntGELU    | n:  8
IntSoftmax | n:  1
IntGELU    | n:  8
IntSoftmax | n:  1
IntGELU    | n:  8
IntSoftmax | n:  1
IntGELU    | n:  8
.calib done
Test: [  0/391]	Time  3.868 ( 3.868)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=1, intgelu_exp_n=9
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=1, intgelu_exp_n=9, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  1
IntGELU    | n:  9
IntSoftmax | n:  1
IntGELU    | n:  9
IntSoftmax | n:  1
IntGELU    | n:  9
IntSoftmax | n:  1
IntGELU    | n:  9
IntSoftmax | n:  1
IntGELU    | n:  9
IntSoftmax | n:  1
IntGELU    | n:  9
IntSoftmax | n:  1
IntGELU    | n:  9
IntSoftmax | n:  1
IntGELU    | n:  9
IntSoftmax | n:  1
IntGELU    | n:  9
IntSoftmax | n:  1
IntGELU    | n:  9
IntSoftmax | n:  1
IntGELU    | n:  9
IntSoftmax | n:  1
IntGELU    | n:  9
.calib done
Test: [  0/391]	Time  3.874 ( 3.874)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.91
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=1, intgelu_exp_n=10
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=1, intgelu_exp_n=10, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  1
IntGELU    | n:  10
IntSoftmax | n:  1
IntGELU    | n:  10
IntSoftmax | n:  1
IntGELU    | n:  10
IntSoftmax | n:  1
IntGELU    | n:  10
IntSoftmax | n:  1
IntGELU    | n:  10
IntSoftmax | n:  1
IntGELU    | n:  10
IntSoftmax | n:  1
IntGELU    | n:  10
IntSoftmax | n:  1
IntGELU    | n:  10
IntSoftmax | n:  1
IntGELU    | n:  10
IntSoftmax | n:  1
IntGELU    | n:  10
IntSoftmax | n:  1
IntGELU    | n:  10
IntSoftmax | n:  1
IntGELU    | n:  10
.calib done
Test: [  0/391]	Time  3.977 ( 3.977)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.00
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=1, intgelu_exp_n=11
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=1, intgelu_exp_n=11, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  1
IntGELU    | n:  11
IntSoftmax | n:  1
IntGELU    | n:  11
IntSoftmax | n:  1
IntGELU    | n:  11
IntSoftmax | n:  1
IntGELU    | n:  11
IntSoftmax | n:  1
IntGELU    | n:  11
IntSoftmax | n:  1
IntGELU    | n:  11
IntSoftmax | n:  1
IntGELU    | n:  11
IntSoftmax | n:  1
IntGELU    | n:  11
IntSoftmax | n:  1
IntGELU    | n:  11
IntSoftmax | n:  1
IntGELU    | n:  11
IntSoftmax | n:  1
IntGELU    | n:  11
IntSoftmax | n:  1
IntGELU    | n:  11
.calib done
Test: [  0/391]	Time  3.954 ( 3.954)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=1, intgelu_exp_n=12
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=1, intgelu_exp_n=12, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  1
IntGELU    | n:  12
IntSoftmax | n:  1
IntGELU    | n:  12
IntSoftmax | n:  1
IntGELU    | n:  12
IntSoftmax | n:  1
IntGELU    | n:  12
IntSoftmax | n:  1
IntGELU    | n:  12
IntSoftmax | n:  1
IntGELU    | n:  12
IntSoftmax | n:  1
IntGELU    | n:  12
IntSoftmax | n:  1
IntGELU    | n:  12
IntSoftmax | n:  1
IntGELU    | n:  12
IntSoftmax | n:  1
IntGELU    | n:  12
IntSoftmax | n:  1
IntGELU    | n:  12
IntSoftmax | n:  1
IntGELU    | n:  12
.calib done
Test: [  0/391]	Time  3.843 ( 3.843)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.89
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=1, intgelu_exp_n=13
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=1, intgelu_exp_n=13, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  1
IntGELU    | n:  13
IntSoftmax | n:  1
IntGELU    | n:  13
IntSoftmax | n:  1
IntGELU    | n:  13
IntSoftmax | n:  1
IntGELU    | n:  13
IntSoftmax | n:  1
IntGELU    | n:  13
IntSoftmax | n:  1
IntGELU    | n:  13
IntSoftmax | n:  1
IntGELU    | n:  13
IntSoftmax | n:  1
IntGELU    | n:  13
IntSoftmax | n:  1
IntGELU    | n:  13
IntSoftmax | n:  1
IntGELU    | n:  13
IntSoftmax | n:  1
IntGELU    | n:  13
IntSoftmax | n:  1
IntGELU    | n:  13
.calib done
Test: [  0/391]	Time  3.711 ( 3.711)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.74
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=1, intgelu_exp_n=14
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=1, intgelu_exp_n=14, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  1
IntGELU    | n:  14
IntSoftmax | n:  1
IntGELU    | n:  14
IntSoftmax | n:  1
IntGELU    | n:  14
IntSoftmax | n:  1
IntGELU    | n:  14
IntSoftmax | n:  1
IntGELU    | n:  14
IntSoftmax | n:  1
IntGELU    | n:  14
IntSoftmax | n:  1
IntGELU    | n:  14
IntSoftmax | n:  1
IntGELU    | n:  14
IntSoftmax | n:  1
IntGELU    | n:  14
IntSoftmax | n:  1
IntGELU    | n:  14
IntSoftmax | n:  1
IntGELU    | n:  14
IntSoftmax | n:  1
IntGELU    | n:  14
.calib done
Test: [  0/391]	Time  3.912 ( 3.912)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.99
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=1, intgelu_exp_n=15
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=1, intgelu_exp_n=15, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  1
IntGELU    | n:  15
IntSoftmax | n:  1
IntGELU    | n:  15
IntSoftmax | n:  1
IntGELU    | n:  15
IntSoftmax | n:  1
IntGELU    | n:  15
IntSoftmax | n:  1
IntGELU    | n:  15
IntSoftmax | n:  1
IntGELU    | n:  15
IntSoftmax | n:  1
IntGELU    | n:  15
IntSoftmax | n:  1
IntGELU    | n:  15
IntSoftmax | n:  1
IntGELU    | n:  15
IntSoftmax | n:  1
IntGELU    | n:  15
IntSoftmax | n:  1
IntGELU    | n:  15
IntSoftmax | n:  1
IntGELU    | n:  15
.calib done
Test: [  0/391]	Time  3.750 ( 3.750)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.87
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=1, intgelu_exp_n=16
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=1, intgelu_exp_n=16, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  1
IntGELU    | n:  16
IntSoftmax | n:  1
IntGELU    | n:  16
IntSoftmax | n:  1
IntGELU    | n:  16
IntSoftmax | n:  1
IntGELU    | n:  16
IntSoftmax | n:  1
IntGELU    | n:  16
IntSoftmax | n:  1
IntGELU    | n:  16
IntSoftmax | n:  1
IntGELU    | n:  16
IntSoftmax | n:  1
IntGELU    | n:  16
IntSoftmax | n:  1
IntGELU    | n:  16
IntSoftmax | n:  1
IntGELU    | n:  16
IntSoftmax | n:  1
IntGELU    | n:  16
IntSoftmax | n:  1
IntGELU    | n:  16
.calib done
Test: [  0/391]	Time  3.889 ( 3.889)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.95
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=1, intgelu_exp_n=17
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=1, intgelu_exp_n=17, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  1
IntGELU    | n:  17
IntSoftmax | n:  1
IntGELU    | n:  17
IntSoftmax | n:  1
IntGELU    | n:  17
IntSoftmax | n:  1
IntGELU    | n:  17
IntSoftmax | n:  1
IntGELU    | n:  17
IntSoftmax | n:  1
IntGELU    | n:  17
IntSoftmax | n:  1
IntGELU    | n:  17
IntSoftmax | n:  1
IntGELU    | n:  17
IntSoftmax | n:  1
IntGELU    | n:  17
IntSoftmax | n:  1
IntGELU    | n:  17
IntSoftmax | n:  1
IntGELU    | n:  17
IntSoftmax | n:  1
IntGELU    | n:  17
.calib done
Test: [  0/391]	Time  3.751 ( 3.751)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.80
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=1, intgelu_exp_n=18
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=1, intgelu_exp_n=18, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  1
IntGELU    | n:  18
IntSoftmax | n:  1
IntGELU    | n:  18
IntSoftmax | n:  1
IntGELU    | n:  18
IntSoftmax | n:  1
IntGELU    | n:  18
IntSoftmax | n:  1
IntGELU    | n:  18
IntSoftmax | n:  1
IntGELU    | n:  18
IntSoftmax | n:  1
IntGELU    | n:  18
IntSoftmax | n:  1
IntGELU    | n:  18
IntSoftmax | n:  1
IntGELU    | n:  18
IntSoftmax | n:  1
IntGELU    | n:  18
IntSoftmax | n:  1
IntGELU    | n:  18
IntSoftmax | n:  1
IntGELU    | n:  18
.calib done
Test: [  0/391]	Time  3.794 ( 3.794)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.87
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=1, intgelu_exp_n=19
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=1, intgelu_exp_n=19, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  1
IntGELU    | n:  19
IntSoftmax | n:  1
IntGELU    | n:  19
IntSoftmax | n:  1
IntGELU    | n:  19
IntSoftmax | n:  1
IntGELU    | n:  19
IntSoftmax | n:  1
IntGELU    | n:  19
IntSoftmax | n:  1
IntGELU    | n:  19
IntSoftmax | n:  1
IntGELU    | n:  19
IntSoftmax | n:  1
IntGELU    | n:  19
IntSoftmax | n:  1
IntGELU    | n:  19
IntSoftmax | n:  1
IntGELU    | n:  19
IntSoftmax | n:  1
IntGELU    | n:  19
IntSoftmax | n:  1
IntGELU    | n:  19
.calib done
Test: [  0/391]	Time  3.981 ( 3.981)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.00
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=1, intgelu_exp_n=20
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=1, intgelu_exp_n=20, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  1
IntGELU    | n:  20
IntSoftmax | n:  1
IntGELU    | n:  20
IntSoftmax | n:  1
IntGELU    | n:  20
IntSoftmax | n:  1
IntGELU    | n:  20
IntSoftmax | n:  1
IntGELU    | n:  20
IntSoftmax | n:  1
IntGELU    | n:  20
IntSoftmax | n:  1
IntGELU    | n:  20
IntSoftmax | n:  1
IntGELU    | n:  20
IntSoftmax | n:  1
IntGELU    | n:  20
IntSoftmax | n:  1
IntGELU    | n:  20
IntSoftmax | n:  1
IntGELU    | n:  20
IntSoftmax | n:  1
IntGELU    | n:  20
.calib done
Test: [  0/391]	Time  4.020 ( 4.020)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.07
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=1, intgelu_exp_n=21
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=1, intgelu_exp_n=21, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  1
IntGELU    | n:  21
IntSoftmax | n:  1
IntGELU    | n:  21
IntSoftmax | n:  1
IntGELU    | n:  21
IntSoftmax | n:  1
IntGELU    | n:  21
IntSoftmax | n:  1
IntGELU    | n:  21
IntSoftmax | n:  1
IntGELU    | n:  21
IntSoftmax | n:  1
IntGELU    | n:  21
IntSoftmax | n:  1
IntGELU    | n:  21
IntSoftmax | n:  1
IntGELU    | n:  21
IntSoftmax | n:  1
IntGELU    | n:  21
IntSoftmax | n:  1
IntGELU    | n:  21
IntSoftmax | n:  1
IntGELU    | n:  21
.calib done
Test: [  0/391]	Time  3.873 ( 3.873)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.06
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=1, intgelu_exp_n=22
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=1, intgelu_exp_n=22, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  1
IntGELU    | n:  22
IntSoftmax | n:  1
IntGELU    | n:  22
IntSoftmax | n:  1
IntGELU    | n:  22
IntSoftmax | n:  1
IntGELU    | n:  22
IntSoftmax | n:  1
IntGELU    | n:  22
IntSoftmax | n:  1
IntGELU    | n:  22
IntSoftmax | n:  1
IntGELU    | n:  22
IntSoftmax | n:  1
IntGELU    | n:  22
IntSoftmax | n:  1
IntGELU    | n:  22
IntSoftmax | n:  1
IntGELU    | n:  22
IntSoftmax | n:  1
IntGELU    | n:  22
IntSoftmax | n:  1
IntGELU    | n:  22
.calib done
Test: [  0/391]	Time  4.059 ( 4.059)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.06
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=1, intgelu_exp_n=23
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=1, intgelu_exp_n=23, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  1
IntGELU    | n:  23
IntSoftmax | n:  1
IntGELU    | n:  23
IntSoftmax | n:  1
IntGELU    | n:  23
IntSoftmax | n:  1
IntGELU    | n:  23
IntSoftmax | n:  1
IntGELU    | n:  23
IntSoftmax | n:  1
IntGELU    | n:  23
IntSoftmax | n:  1
IntGELU    | n:  23
IntSoftmax | n:  1
IntGELU    | n:  23
IntSoftmax | n:  1
IntGELU    | n:  23
IntSoftmax | n:  1
IntGELU    | n:  23
IntSoftmax | n:  1
IntGELU    | n:  23
IntSoftmax | n:  1
IntGELU    | n:  23
.calib done
Test: [  0/391]	Time  4.032 ( 4.032)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.07
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=1, intgelu_exp_n=24
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=1, intgelu_exp_n=24, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  1
IntGELU    | n:  24
IntSoftmax | n:  1
IntGELU    | n:  24
IntSoftmax | n:  1
IntGELU    | n:  24
IntSoftmax | n:  1
IntGELU    | n:  24
IntSoftmax | n:  1
IntGELU    | n:  24
IntSoftmax | n:  1
IntGELU    | n:  24
IntSoftmax | n:  1
IntGELU    | n:  24
IntSoftmax | n:  1
IntGELU    | n:  24
IntSoftmax | n:  1
IntGELU    | n:  24
IntSoftmax | n:  1
IntGELU    | n:  24
IntSoftmax | n:  1
IntGELU    | n:  24
IntSoftmax | n:  1
IntGELU    | n:  24
.calib done
Test: [  0/391]	Time  3.833 ( 3.833)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.02
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=1, intgelu_exp_n=25
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=1, intgelu_exp_n=25, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  1
IntGELU    | n:  25
IntSoftmax | n:  1
IntGELU    | n:  25
IntSoftmax | n:  1
IntGELU    | n:  25
IntSoftmax | n:  1
IntGELU    | n:  25
IntSoftmax | n:  1
IntGELU    | n:  25
IntSoftmax | n:  1
IntGELU    | n:  25
IntSoftmax | n:  1
IntGELU    | n:  25
IntSoftmax | n:  1
IntGELU    | n:  25
IntSoftmax | n:  1
IntGELU    | n:  25
IntSoftmax | n:  1
IntGELU    | n:  25
IntSoftmax | n:  1
IntGELU    | n:  25
IntSoftmax | n:  1
IntGELU    | n:  25
.calib done
Test: [  0/391]	Time  3.658 ( 3.658)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.82
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=1, intgelu_exp_n=26
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=1, intgelu_exp_n=26, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  1
IntGELU    | n:  26
IntSoftmax | n:  1
IntGELU    | n:  26
IntSoftmax | n:  1
IntGELU    | n:  26
IntSoftmax | n:  1
IntGELU    | n:  26
IntSoftmax | n:  1
IntGELU    | n:  26
IntSoftmax | n:  1
IntGELU    | n:  26
IntSoftmax | n:  1
IntGELU    | n:  26
IntSoftmax | n:  1
IntGELU    | n:  26
IntSoftmax | n:  1
IntGELU    | n:  26
IntSoftmax | n:  1
IntGELU    | n:  26
IntSoftmax | n:  1
IntGELU    | n:  26
IntSoftmax | n:  1
IntGELU    | n:  26
.calib done
Test: [  0/391]	Time  3.903 ( 3.903)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.95
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=1, intgelu_exp_n=27
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=1, intgelu_exp_n=27, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  1
IntGELU    | n:  27
IntSoftmax | n:  1
IntGELU    | n:  27
IntSoftmax | n:  1
IntGELU    | n:  27
IntSoftmax | n:  1
IntGELU    | n:  27
IntSoftmax | n:  1
IntGELU    | n:  27
IntSoftmax | n:  1
IntGELU    | n:  27
IntSoftmax | n:  1
IntGELU    | n:  27
IntSoftmax | n:  1
IntGELU    | n:  27
IntSoftmax | n:  1
IntGELU    | n:  27
IntSoftmax | n:  1
IntGELU    | n:  27
IntSoftmax | n:  1
IntGELU    | n:  27
IntSoftmax | n:  1
IntGELU    | n:  27
.calib done
Test: [  0/391]	Time  3.835 ( 3.835)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.92
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=1, intgelu_exp_n=28
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=1, intgelu_exp_n=28, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  1
IntGELU    | n:  28
IntSoftmax | n:  1
IntGELU    | n:  28
IntSoftmax | n:  1
IntGELU    | n:  28
IntSoftmax | n:  1
IntGELU    | n:  28
IntSoftmax | n:  1
IntGELU    | n:  28
IntSoftmax | n:  1
IntGELU    | n:  28
IntSoftmax | n:  1
IntGELU    | n:  28
IntSoftmax | n:  1
IntGELU    | n:  28
IntSoftmax | n:  1
IntGELU    | n:  28
IntSoftmax | n:  1
IntGELU    | n:  28
IntSoftmax | n:  1
IntGELU    | n:  28
IntSoftmax | n:  1
IntGELU    | n:  28
.calib done
Test: [  0/391]	Time  3.828 ( 3.828)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.00
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=1, intgelu_exp_n=29
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=1, intgelu_exp_n=29, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  1
IntGELU    | n:  29
IntSoftmax | n:  1
IntGELU    | n:  29
IntSoftmax | n:  1
IntGELU    | n:  29
IntSoftmax | n:  1
IntGELU    | n:  29
IntSoftmax | n:  1
IntGELU    | n:  29
IntSoftmax | n:  1
IntGELU    | n:  29
IntSoftmax | n:  1
IntGELU    | n:  29
IntSoftmax | n:  1
IntGELU    | n:  29
IntSoftmax | n:  1
IntGELU    | n:  29
IntSoftmax | n:  1
IntGELU    | n:  29
IntSoftmax | n:  1
IntGELU    | n:  29
IntSoftmax | n:  1
IntGELU    | n:  29
.calib done
Test: [  0/391]	Time  3.895 ( 3.895)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.04
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=1, intgelu_exp_n=30
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=1, intgelu_exp_n=30, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  1
IntGELU    | n:  30
IntSoftmax | n:  1
IntGELU    | n:  30
IntSoftmax | n:  1
IntGELU    | n:  30
IntSoftmax | n:  1
IntGELU    | n:  30
IntSoftmax | n:  1
IntGELU    | n:  30
IntSoftmax | n:  1
IntGELU    | n:  30
IntSoftmax | n:  1
IntGELU    | n:  30
IntSoftmax | n:  1
IntGELU    | n:  30
IntSoftmax | n:  1
IntGELU    | n:  30
IntSoftmax | n:  1
IntGELU    | n:  30
IntSoftmax | n:  1
IntGELU    | n:  30
IntSoftmax | n:  1
IntGELU    | n:  30
.calib done
Test: [  0/391]	Time  3.864 ( 3.864)	Acc@1   0.00 (  0.00)	Acc@5   0.78 (  0.78)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.781
Time: 9.01
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=1, intgelu_exp_n=31
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=1, intgelu_exp_n=31, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  1
IntGELU    | n:  31
IntSoftmax | n:  1
IntGELU    | n:  31
IntSoftmax | n:  1
IntGELU    | n:  31
IntSoftmax | n:  1
IntGELU    | n:  31
IntSoftmax | n:  1
IntGELU    | n:  31
IntSoftmax | n:  1
IntGELU    | n:  31
IntSoftmax | n:  1
IntGELU    | n:  31
IntSoftmax | n:  1
IntGELU    | n:  31
IntSoftmax | n:  1
IntGELU    | n:  31
IntSoftmax | n:  1
IntGELU    | n:  31
IntSoftmax | n:  1
IntGELU    | n:  31
IntSoftmax | n:  1
IntGELU    | n:  31
.calib done
Test: [  0/391]	Time  3.643 ( 3.643)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.70
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=2, intgelu_exp_n=0
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=2, intgelu_exp_n=0, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  2
IntGELU    | n:  0
IntSoftmax | n:  2
IntGELU    | n:  0
IntSoftmax | n:  2
IntGELU    | n:  0
IntSoftmax | n:  2
IntGELU    | n:  0
IntSoftmax | n:  2
IntGELU    | n:  0
IntSoftmax | n:  2
IntGELU    | n:  0
IntSoftmax | n:  2
IntGELU    | n:  0
IntSoftmax | n:  2
IntGELU    | n:  0
IntSoftmax | n:  2
IntGELU    | n:  0
IntSoftmax | n:  2
IntGELU    | n:  0
IntSoftmax | n:  2
IntGELU    | n:  0
IntSoftmax | n:  2
IntGELU    | n:  0
.calib done
Test: [  0/391]	Time  3.845 ( 3.845)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.87
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=2, intgelu_exp_n=1
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=2, intgelu_exp_n=1, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  2
IntGELU    | n:  1
IntSoftmax | n:  2
IntGELU    | n:  1
IntSoftmax | n:  2
IntGELU    | n:  1
IntSoftmax | n:  2
IntGELU    | n:  1
IntSoftmax | n:  2
IntGELU    | n:  1
IntSoftmax | n:  2
IntGELU    | n:  1
IntSoftmax | n:  2
IntGELU    | n:  1
IntSoftmax | n:  2
IntGELU    | n:  1
IntSoftmax | n:  2
IntGELU    | n:  1
IntSoftmax | n:  2
IntGELU    | n:  1
IntSoftmax | n:  2
IntGELU    | n:  1
IntSoftmax | n:  2
IntGELU    | n:  1
.calib done
Test: [  0/391]	Time  4.136 ( 4.136)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.21
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=2, intgelu_exp_n=2
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=2, intgelu_exp_n=2, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  2
IntGELU    | n:  2
IntSoftmax | n:  2
IntGELU    | n:  2
IntSoftmax | n:  2
IntGELU    | n:  2
IntSoftmax | n:  2
IntGELU    | n:  2
IntSoftmax | n:  2
IntGELU    | n:  2
IntSoftmax | n:  2
IntGELU    | n:  2
IntSoftmax | n:  2
IntGELU    | n:  2
IntSoftmax | n:  2
IntGELU    | n:  2
IntSoftmax | n:  2
IntGELU    | n:  2
IntSoftmax | n:  2
IntGELU    | n:  2
IntSoftmax | n:  2
IntGELU    | n:  2
IntSoftmax | n:  2
IntGELU    | n:  2
.calib done
Test: [  0/391]	Time  4.118 ( 4.118)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.33
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=2, intgelu_exp_n=3
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=2, intgelu_exp_n=3, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  2
IntGELU    | n:  3
IntSoftmax | n:  2
IntGELU    | n:  3
IntSoftmax | n:  2
IntGELU    | n:  3
IntSoftmax | n:  2
IntGELU    | n:  3
IntSoftmax | n:  2
IntGELU    | n:  3
IntSoftmax | n:  2
IntGELU    | n:  3
IntSoftmax | n:  2
IntGELU    | n:  3
IntSoftmax | n:  2
IntGELU    | n:  3
IntSoftmax | n:  2
IntGELU    | n:  3
IntSoftmax | n:  2
IntGELU    | n:  3
IntSoftmax | n:  2
IntGELU    | n:  3
IntSoftmax | n:  2
IntGELU    | n:  3
.calib done
Test: [  0/391]	Time  3.861 ( 3.861)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.94
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=2, intgelu_exp_n=4
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=2, intgelu_exp_n=4, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  2
IntGELU    | n:  4
IntSoftmax | n:  2
IntGELU    | n:  4
IntSoftmax | n:  2
IntGELU    | n:  4
IntSoftmax | n:  2
IntGELU    | n:  4
IntSoftmax | n:  2
IntGELU    | n:  4
IntSoftmax | n:  2
IntGELU    | n:  4
IntSoftmax | n:  2
IntGELU    | n:  4
IntSoftmax | n:  2
IntGELU    | n:  4
IntSoftmax | n:  2
IntGELU    | n:  4
IntSoftmax | n:  2
IntGELU    | n:  4
IntSoftmax | n:  2
IntGELU    | n:  4
IntSoftmax | n:  2
IntGELU    | n:  4
.calib done
Test: [  0/391]	Time  3.737 ( 3.737)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.77
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=2, intgelu_exp_n=5
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=2, intgelu_exp_n=5, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  2
IntGELU    | n:  5
IntSoftmax | n:  2
IntGELU    | n:  5
IntSoftmax | n:  2
IntGELU    | n:  5
IntSoftmax | n:  2
IntGELU    | n:  5
IntSoftmax | n:  2
IntGELU    | n:  5
IntSoftmax | n:  2
IntGELU    | n:  5
IntSoftmax | n:  2
IntGELU    | n:  5
IntSoftmax | n:  2
IntGELU    | n:  5
IntSoftmax | n:  2
IntGELU    | n:  5
IntSoftmax | n:  2
IntGELU    | n:  5
IntSoftmax | n:  2
IntGELU    | n:  5
IntSoftmax | n:  2
IntGELU    | n:  5
.calib done
Test: [  0/391]	Time  3.972 ( 3.972)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.04
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=2, intgelu_exp_n=6
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=2, intgelu_exp_n=6, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  2
IntGELU    | n:  6
IntSoftmax | n:  2
IntGELU    | n:  6
IntSoftmax | n:  2
IntGELU    | n:  6
IntSoftmax | n:  2
IntGELU    | n:  6
IntSoftmax | n:  2
IntGELU    | n:  6
IntSoftmax | n:  2
IntGELU    | n:  6
IntSoftmax | n:  2
IntGELU    | n:  6
IntSoftmax | n:  2
IntGELU    | n:  6
IntSoftmax | n:  2
IntGELU    | n:  6
IntSoftmax | n:  2
IntGELU    | n:  6
IntSoftmax | n:  2
IntGELU    | n:  6
IntSoftmax | n:  2
IntGELU    | n:  6
.calib done
Test: [  0/391]	Time  3.897 ( 3.897)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.10
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=2, intgelu_exp_n=7
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=2, intgelu_exp_n=7, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  2
IntGELU    | n:  7
IntSoftmax | n:  2
IntGELU    | n:  7
IntSoftmax | n:  2
IntGELU    | n:  7
IntSoftmax | n:  2
IntGELU    | n:  7
IntSoftmax | n:  2
IntGELU    | n:  7
IntSoftmax | n:  2
IntGELU    | n:  7
IntSoftmax | n:  2
IntGELU    | n:  7
IntSoftmax | n:  2
IntGELU    | n:  7
IntSoftmax | n:  2
IntGELU    | n:  7
IntSoftmax | n:  2
IntGELU    | n:  7
IntSoftmax | n:  2
IntGELU    | n:  7
IntSoftmax | n:  2
IntGELU    | n:  7
.calib done
Test: [  0/391]	Time  3.896 ( 3.896)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.03
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=2, intgelu_exp_n=8
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=2, intgelu_exp_n=8, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  2
IntGELU    | n:  8
IntSoftmax | n:  2
IntGELU    | n:  8
IntSoftmax | n:  2
IntGELU    | n:  8
IntSoftmax | n:  2
IntGELU    | n:  8
IntSoftmax | n:  2
IntGELU    | n:  8
IntSoftmax | n:  2
IntGELU    | n:  8
IntSoftmax | n:  2
IntGELU    | n:  8
IntSoftmax | n:  2
IntGELU    | n:  8
IntSoftmax | n:  2
IntGELU    | n:  8
IntSoftmax | n:  2
IntGELU    | n:  8
IntSoftmax | n:  2
IntGELU    | n:  8
IntSoftmax | n:  2
IntGELU    | n:  8
.calib done
Test: [  0/391]	Time  3.846 ( 3.846)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.91
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=2, intgelu_exp_n=9
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=2, intgelu_exp_n=9, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  2
IntGELU    | n:  9
IntSoftmax | n:  2
IntGELU    | n:  9
IntSoftmax | n:  2
IntGELU    | n:  9
IntSoftmax | n:  2
IntGELU    | n:  9
IntSoftmax | n:  2
IntGELU    | n:  9
IntSoftmax | n:  2
IntGELU    | n:  9
IntSoftmax | n:  2
IntGELU    | n:  9
IntSoftmax | n:  2
IntGELU    | n:  9
IntSoftmax | n:  2
IntGELU    | n:  9
IntSoftmax | n:  2
IntGELU    | n:  9
IntSoftmax | n:  2
IntGELU    | n:  9
IntSoftmax | n:  2
IntGELU    | n:  9
.calib done
Test: [  0/391]	Time  3.930 ( 3.930)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.91
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=2, intgelu_exp_n=10
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=2, intgelu_exp_n=10, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  2
IntGELU    | n:  10
IntSoftmax | n:  2
IntGELU    | n:  10
IntSoftmax | n:  2
IntGELU    | n:  10
IntSoftmax | n:  2
IntGELU    | n:  10
IntSoftmax | n:  2
IntGELU    | n:  10
IntSoftmax | n:  2
IntGELU    | n:  10
IntSoftmax | n:  2
IntGELU    | n:  10
IntSoftmax | n:  2
IntGELU    | n:  10
IntSoftmax | n:  2
IntGELU    | n:  10
IntSoftmax | n:  2
IntGELU    | n:  10
IntSoftmax | n:  2
IntGELU    | n:  10
IntSoftmax | n:  2
IntGELU    | n:  10
.calib done
Test: [  0/391]	Time  3.902 ( 3.902)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=2, intgelu_exp_n=11
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=2, intgelu_exp_n=11, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  2
IntGELU    | n:  11
IntSoftmax | n:  2
IntGELU    | n:  11
IntSoftmax | n:  2
IntGELU    | n:  11
IntSoftmax | n:  2
IntGELU    | n:  11
IntSoftmax | n:  2
IntGELU    | n:  11
IntSoftmax | n:  2
IntGELU    | n:  11
IntSoftmax | n:  2
IntGELU    | n:  11
IntSoftmax | n:  2
IntGELU    | n:  11
IntSoftmax | n:  2
IntGELU    | n:  11
IntSoftmax | n:  2
IntGELU    | n:  11
IntSoftmax | n:  2
IntGELU    | n:  11
IntSoftmax | n:  2
IntGELU    | n:  11
.calib done
Test: [  0/391]	Time  4.028 ( 4.028)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.02
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=2, intgelu_exp_n=12
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=2, intgelu_exp_n=12, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  2
IntGELU    | n:  12
IntSoftmax | n:  2
IntGELU    | n:  12
IntSoftmax | n:  2
IntGELU    | n:  12
IntSoftmax | n:  2
IntGELU    | n:  12
IntSoftmax | n:  2
IntGELU    | n:  12
IntSoftmax | n:  2
IntGELU    | n:  12
IntSoftmax | n:  2
IntGELU    | n:  12
IntSoftmax | n:  2
IntGELU    | n:  12
IntSoftmax | n:  2
IntGELU    | n:  12
IntSoftmax | n:  2
IntGELU    | n:  12
IntSoftmax | n:  2
IntGELU    | n:  12
IntSoftmax | n:  2
IntGELU    | n:  12
.calib done
Test: [  0/391]	Time  3.865 ( 3.865)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.89
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=2, intgelu_exp_n=13
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=2, intgelu_exp_n=13, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  2
IntGELU    | n:  13
IntSoftmax | n:  2
IntGELU    | n:  13
IntSoftmax | n:  2
IntGELU    | n:  13
IntSoftmax | n:  2
IntGELU    | n:  13
IntSoftmax | n:  2
IntGELU    | n:  13
IntSoftmax | n:  2
IntGELU    | n:  13
IntSoftmax | n:  2
IntGELU    | n:  13
IntSoftmax | n:  2
IntGELU    | n:  13
IntSoftmax | n:  2
IntGELU    | n:  13
IntSoftmax | n:  2
IntGELU    | n:  13
IntSoftmax | n:  2
IntGELU    | n:  13
IntSoftmax | n:  2
IntGELU    | n:  13
.calib done
Test: [  0/391]	Time  3.984 ( 3.984)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.11
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=2, intgelu_exp_n=14
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=2, intgelu_exp_n=14, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  2
IntGELU    | n:  14
IntSoftmax | n:  2
IntGELU    | n:  14
IntSoftmax | n:  2
IntGELU    | n:  14
IntSoftmax | n:  2
IntGELU    | n:  14
IntSoftmax | n:  2
IntGELU    | n:  14
IntSoftmax | n:  2
IntGELU    | n:  14
IntSoftmax | n:  2
IntGELU    | n:  14
IntSoftmax | n:  2
IntGELU    | n:  14
IntSoftmax | n:  2
IntGELU    | n:  14
IntSoftmax | n:  2
IntGELU    | n:  14
IntSoftmax | n:  2
IntGELU    | n:  14
IntSoftmax | n:  2
IntGELU    | n:  14
.calib done
Test: [  0/391]	Time  3.812 ( 3.812)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.83
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=2, intgelu_exp_n=15
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=2, intgelu_exp_n=15, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  2
IntGELU    | n:  15
IntSoftmax | n:  2
IntGELU    | n:  15
IntSoftmax | n:  2
IntGELU    | n:  15
IntSoftmax | n:  2
IntGELU    | n:  15
IntSoftmax | n:  2
IntGELU    | n:  15
IntSoftmax | n:  2
IntGELU    | n:  15
IntSoftmax | n:  2
IntGELU    | n:  15
IntSoftmax | n:  2
IntGELU    | n:  15
IntSoftmax | n:  2
IntGELU    | n:  15
IntSoftmax | n:  2
IntGELU    | n:  15
IntSoftmax | n:  2
IntGELU    | n:  15
IntSoftmax | n:  2
IntGELU    | n:  15
.calib done
Test: [  0/391]	Time  3.894 ( 3.894)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=2, intgelu_exp_n=16
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=2, intgelu_exp_n=16, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  2
IntGELU    | n:  16
IntSoftmax | n:  2
IntGELU    | n:  16
IntSoftmax | n:  2
IntGELU    | n:  16
IntSoftmax | n:  2
IntGELU    | n:  16
IntSoftmax | n:  2
IntGELU    | n:  16
IntSoftmax | n:  2
IntGELU    | n:  16
IntSoftmax | n:  2
IntGELU    | n:  16
IntSoftmax | n:  2
IntGELU    | n:  16
IntSoftmax | n:  2
IntGELU    | n:  16
IntSoftmax | n:  2
IntGELU    | n:  16
IntSoftmax | n:  2
IntGELU    | n:  16
IntSoftmax | n:  2
IntGELU    | n:  16
.calib done
Test: [  0/391]	Time  3.710 ( 3.710)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.78
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=2, intgelu_exp_n=17
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=2, intgelu_exp_n=17, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  2
IntGELU    | n:  17
IntSoftmax | n:  2
IntGELU    | n:  17
IntSoftmax | n:  2
IntGELU    | n:  17
IntSoftmax | n:  2
IntGELU    | n:  17
IntSoftmax | n:  2
IntGELU    | n:  17
IntSoftmax | n:  2
IntGELU    | n:  17
IntSoftmax | n:  2
IntGELU    | n:  17
IntSoftmax | n:  2
IntGELU    | n:  17
IntSoftmax | n:  2
IntGELU    | n:  17
IntSoftmax | n:  2
IntGELU    | n:  17
IntSoftmax | n:  2
IntGELU    | n:  17
IntSoftmax | n:  2
IntGELU    | n:  17
.calib done
Test: [  0/391]	Time  3.881 ( 3.881)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.89
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=2, intgelu_exp_n=18
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=2, intgelu_exp_n=18, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  2
IntGELU    | n:  18
IntSoftmax | n:  2
IntGELU    | n:  18
IntSoftmax | n:  2
IntGELU    | n:  18
IntSoftmax | n:  2
IntGELU    | n:  18
IntSoftmax | n:  2
IntGELU    | n:  18
IntSoftmax | n:  2
IntGELU    | n:  18
IntSoftmax | n:  2
IntGELU    | n:  18
IntSoftmax | n:  2
IntGELU    | n:  18
IntSoftmax | n:  2
IntGELU    | n:  18
IntSoftmax | n:  2
IntGELU    | n:  18
IntSoftmax | n:  2
IntGELU    | n:  18
IntSoftmax | n:  2
IntGELU    | n:  18
.calib done
Test: [  0/391]	Time  3.941 ( 3.941)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.06
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=2, intgelu_exp_n=19
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=2, intgelu_exp_n=19, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  2
IntGELU    | n:  19
IntSoftmax | n:  2
IntGELU    | n:  19
IntSoftmax | n:  2
IntGELU    | n:  19
IntSoftmax | n:  2
IntGELU    | n:  19
IntSoftmax | n:  2
IntGELU    | n:  19
IntSoftmax | n:  2
IntGELU    | n:  19
IntSoftmax | n:  2
IntGELU    | n:  19
IntSoftmax | n:  2
IntGELU    | n:  19
IntSoftmax | n:  2
IntGELU    | n:  19
IntSoftmax | n:  2
IntGELU    | n:  19
IntSoftmax | n:  2
IntGELU    | n:  19
IntSoftmax | n:  2
IntGELU    | n:  19
.calib done
Test: [  0/391]	Time  3.816 ( 3.816)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.83
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=2, intgelu_exp_n=20
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=2, intgelu_exp_n=20, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  2
IntGELU    | n:  20
IntSoftmax | n:  2
IntGELU    | n:  20
IntSoftmax | n:  2
IntGELU    | n:  20
IntSoftmax | n:  2
IntGELU    | n:  20
IntSoftmax | n:  2
IntGELU    | n:  20
IntSoftmax | n:  2
IntGELU    | n:  20
IntSoftmax | n:  2
IntGELU    | n:  20
IntSoftmax | n:  2
IntGELU    | n:  20
IntSoftmax | n:  2
IntGELU    | n:  20
IntSoftmax | n:  2
IntGELU    | n:  20
IntSoftmax | n:  2
IntGELU    | n:  20
IntSoftmax | n:  2
IntGELU    | n:  20
.calib done
Test: [  0/391]	Time  3.951 ( 3.951)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.00
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=2, intgelu_exp_n=21
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=2, intgelu_exp_n=21, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  2
IntGELU    | n:  21
IntSoftmax | n:  2
IntGELU    | n:  21
IntSoftmax | n:  2
IntGELU    | n:  21
IntSoftmax | n:  2
IntGELU    | n:  21
IntSoftmax | n:  2
IntGELU    | n:  21
IntSoftmax | n:  2
IntGELU    | n:  21
IntSoftmax | n:  2
IntGELU    | n:  21
IntSoftmax | n:  2
IntGELU    | n:  21
IntSoftmax | n:  2
IntGELU    | n:  21
IntSoftmax | n:  2
IntGELU    | n:  21
IntSoftmax | n:  2
IntGELU    | n:  21
IntSoftmax | n:  2
IntGELU    | n:  21
.calib done
Test: [  0/391]	Time  3.936 ( 3.936)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.01
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=2, intgelu_exp_n=22
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=2, intgelu_exp_n=22, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  2
IntGELU    | n:  22
IntSoftmax | n:  2
IntGELU    | n:  22
IntSoftmax | n:  2
IntGELU    | n:  22
IntSoftmax | n:  2
IntGELU    | n:  22
IntSoftmax | n:  2
IntGELU    | n:  22
IntSoftmax | n:  2
IntGELU    | n:  22
IntSoftmax | n:  2
IntGELU    | n:  22
IntSoftmax | n:  2
IntGELU    | n:  22
IntSoftmax | n:  2
IntGELU    | n:  22
IntSoftmax | n:  2
IntGELU    | n:  22
IntSoftmax | n:  2
IntGELU    | n:  22
IntSoftmax | n:  2
IntGELU    | n:  22
.calib done
Test: [  0/391]	Time  3.690 ( 3.690)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.70
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=2, intgelu_exp_n=23
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=2, intgelu_exp_n=23, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  2
IntGELU    | n:  23
IntSoftmax | n:  2
IntGELU    | n:  23
IntSoftmax | n:  2
IntGELU    | n:  23
IntSoftmax | n:  2
IntGELU    | n:  23
IntSoftmax | n:  2
IntGELU    | n:  23
IntSoftmax | n:  2
IntGELU    | n:  23
IntSoftmax | n:  2
IntGELU    | n:  23
IntSoftmax | n:  2
IntGELU    | n:  23
IntSoftmax | n:  2
IntGELU    | n:  23
IntSoftmax | n:  2
IntGELU    | n:  23
IntSoftmax | n:  2
IntGELU    | n:  23
IntSoftmax | n:  2
IntGELU    | n:  23
.calib done
Test: [  0/391]	Time  3.660 ( 3.660)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.71
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=2, intgelu_exp_n=24
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=2, intgelu_exp_n=24, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  2
IntGELU    | n:  24
IntSoftmax | n:  2
IntGELU    | n:  24
IntSoftmax | n:  2
IntGELU    | n:  24
IntSoftmax | n:  2
IntGELU    | n:  24
IntSoftmax | n:  2
IntGELU    | n:  24
IntSoftmax | n:  2
IntGELU    | n:  24
IntSoftmax | n:  2
IntGELU    | n:  24
IntSoftmax | n:  2
IntGELU    | n:  24
IntSoftmax | n:  2
IntGELU    | n:  24
IntSoftmax | n:  2
IntGELU    | n:  24
IntSoftmax | n:  2
IntGELU    | n:  24
IntSoftmax | n:  2
IntGELU    | n:  24
.calib done
Test: [  0/391]	Time  3.783 ( 3.783)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.79
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=2, intgelu_exp_n=25
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=2, intgelu_exp_n=25, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  2
IntGELU    | n:  25
IntSoftmax | n:  2
IntGELU    | n:  25
IntSoftmax | n:  2
IntGELU    | n:  25
IntSoftmax | n:  2
IntGELU    | n:  25
IntSoftmax | n:  2
IntGELU    | n:  25
IntSoftmax | n:  2
IntGELU    | n:  25
IntSoftmax | n:  2
IntGELU    | n:  25
IntSoftmax | n:  2
IntGELU    | n:  25
IntSoftmax | n:  2
IntGELU    | n:  25
IntSoftmax | n:  2
IntGELU    | n:  25
IntSoftmax | n:  2
IntGELU    | n:  25
IntSoftmax | n:  2
IntGELU    | n:  25
.calib done
Test: [  0/391]	Time  3.939 ( 3.939)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.01
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=2, intgelu_exp_n=26
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=2, intgelu_exp_n=26, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  2
IntGELU    | n:  26
IntSoftmax | n:  2
IntGELU    | n:  26
IntSoftmax | n:  2
IntGELU    | n:  26
IntSoftmax | n:  2
IntGELU    | n:  26
IntSoftmax | n:  2
IntGELU    | n:  26
IntSoftmax | n:  2
IntGELU    | n:  26
IntSoftmax | n:  2
IntGELU    | n:  26
IntSoftmax | n:  2
IntGELU    | n:  26
IntSoftmax | n:  2
IntGELU    | n:  26
IntSoftmax | n:  2
IntGELU    | n:  26
IntSoftmax | n:  2
IntGELU    | n:  26
IntSoftmax | n:  2
IntGELU    | n:  26
.calib done
Test: [  0/391]	Time  3.755 ( 3.755)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.76
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=2, intgelu_exp_n=27
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=2, intgelu_exp_n=27, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  2
IntGELU    | n:  27
IntSoftmax | n:  2
IntGELU    | n:  27
IntSoftmax | n:  2
IntGELU    | n:  27
IntSoftmax | n:  2
IntGELU    | n:  27
IntSoftmax | n:  2
IntGELU    | n:  27
IntSoftmax | n:  2
IntGELU    | n:  27
IntSoftmax | n:  2
IntGELU    | n:  27
IntSoftmax | n:  2
IntGELU    | n:  27
IntSoftmax | n:  2
IntGELU    | n:  27
IntSoftmax | n:  2
IntGELU    | n:  27
IntSoftmax | n:  2
IntGELU    | n:  27
IntSoftmax | n:  2
IntGELU    | n:  27
.calib done
Test: [  0/391]	Time  3.949 ( 3.949)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.03
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=2, intgelu_exp_n=28
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=2, intgelu_exp_n=28, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  2
IntGELU    | n:  28
IntSoftmax | n:  2
IntGELU    | n:  28
IntSoftmax | n:  2
IntGELU    | n:  28
IntSoftmax | n:  2
IntGELU    | n:  28
IntSoftmax | n:  2
IntGELU    | n:  28
IntSoftmax | n:  2
IntGELU    | n:  28
IntSoftmax | n:  2
IntGELU    | n:  28
IntSoftmax | n:  2
IntGELU    | n:  28
IntSoftmax | n:  2
IntGELU    | n:  28
IntSoftmax | n:  2
IntGELU    | n:  28
IntSoftmax | n:  2
IntGELU    | n:  28
IntSoftmax | n:  2
IntGELU    | n:  28
.calib done
Test: [  0/391]	Time  4.002 ( 4.002)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=2, intgelu_exp_n=29
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=2, intgelu_exp_n=29, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  2
IntGELU    | n:  29
IntSoftmax | n:  2
IntGELU    | n:  29
IntSoftmax | n:  2
IntGELU    | n:  29
IntSoftmax | n:  2
IntGELU    | n:  29
IntSoftmax | n:  2
IntGELU    | n:  29
IntSoftmax | n:  2
IntGELU    | n:  29
IntSoftmax | n:  2
IntGELU    | n:  29
IntSoftmax | n:  2
IntGELU    | n:  29
IntSoftmax | n:  2
IntGELU    | n:  29
IntSoftmax | n:  2
IntGELU    | n:  29
IntSoftmax | n:  2
IntGELU    | n:  29
IntSoftmax | n:  2
IntGELU    | n:  29
.calib done
Test: [  0/391]	Time  3.858 ( 3.858)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=2, intgelu_exp_n=30
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=2, intgelu_exp_n=30, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  2
IntGELU    | n:  30
IntSoftmax | n:  2
IntGELU    | n:  30
IntSoftmax | n:  2
IntGELU    | n:  30
IntSoftmax | n:  2
IntGELU    | n:  30
IntSoftmax | n:  2
IntGELU    | n:  30
IntSoftmax | n:  2
IntGELU    | n:  30
IntSoftmax | n:  2
IntGELU    | n:  30
IntSoftmax | n:  2
IntGELU    | n:  30
IntSoftmax | n:  2
IntGELU    | n:  30
IntSoftmax | n:  2
IntGELU    | n:  30
IntSoftmax | n:  2
IntGELU    | n:  30
IntSoftmax | n:  2
IntGELU    | n:  30
.calib done
Test: [  0/391]	Time  3.801 ( 3.801)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.88
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=2, intgelu_exp_n=31
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=2, intgelu_exp_n=31, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  2
IntGELU    | n:  31
IntSoftmax | n:  2
IntGELU    | n:  31
IntSoftmax | n:  2
IntGELU    | n:  31
IntSoftmax | n:  2
IntGELU    | n:  31
IntSoftmax | n:  2
IntGELU    | n:  31
IntSoftmax | n:  2
IntGELU    | n:  31
IntSoftmax | n:  2
IntGELU    | n:  31
IntSoftmax | n:  2
IntGELU    | n:  31
IntSoftmax | n:  2
IntGELU    | n:  31
IntSoftmax | n:  2
IntGELU    | n:  31
IntSoftmax | n:  2
IntGELU    | n:  31
IntSoftmax | n:  2
IntGELU    | n:  31
.calib done
Test: [  0/391]	Time  4.041 ( 4.041)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.21
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=3, intgelu_exp_n=0
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=3, intgelu_exp_n=0, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  3
IntGELU    | n:  0
IntSoftmax | n:  3
IntGELU    | n:  0
IntSoftmax | n:  3
IntGELU    | n:  0
IntSoftmax | n:  3
IntGELU    | n:  0
IntSoftmax | n:  3
IntGELU    | n:  0
IntSoftmax | n:  3
IntGELU    | n:  0
IntSoftmax | n:  3
IntGELU    | n:  0
IntSoftmax | n:  3
IntGELU    | n:  0
IntSoftmax | n:  3
IntGELU    | n:  0
IntSoftmax | n:  3
IntGELU    | n:  0
IntSoftmax | n:  3
IntGELU    | n:  0
IntSoftmax | n:  3
IntGELU    | n:  0
.calib done
Test: [  0/391]	Time  3.860 ( 3.860)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.87
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=3, intgelu_exp_n=1
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=3, intgelu_exp_n=1, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  3
IntGELU    | n:  1
IntSoftmax | n:  3
IntGELU    | n:  1
IntSoftmax | n:  3
IntGELU    | n:  1
IntSoftmax | n:  3
IntGELU    | n:  1
IntSoftmax | n:  3
IntGELU    | n:  1
IntSoftmax | n:  3
IntGELU    | n:  1
IntSoftmax | n:  3
IntGELU    | n:  1
IntSoftmax | n:  3
IntGELU    | n:  1
IntSoftmax | n:  3
IntGELU    | n:  1
IntSoftmax | n:  3
IntGELU    | n:  1
IntSoftmax | n:  3
IntGELU    | n:  1
IntSoftmax | n:  3
IntGELU    | n:  1
.calib done
Test: [  0/391]	Time  4.086 ( 4.086)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.16
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=3, intgelu_exp_n=2
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=3, intgelu_exp_n=2, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  3
IntGELU    | n:  2
IntSoftmax | n:  3
IntGELU    | n:  2
IntSoftmax | n:  3
IntGELU    | n:  2
IntSoftmax | n:  3
IntGELU    | n:  2
IntSoftmax | n:  3
IntGELU    | n:  2
IntSoftmax | n:  3
IntGELU    | n:  2
IntSoftmax | n:  3
IntGELU    | n:  2
IntSoftmax | n:  3
IntGELU    | n:  2
IntSoftmax | n:  3
IntGELU    | n:  2
IntSoftmax | n:  3
IntGELU    | n:  2
IntSoftmax | n:  3
IntGELU    | n:  2
IntSoftmax | n:  3
IntGELU    | n:  2
.calib done
Test: [  0/391]	Time  3.874 ( 3.874)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=3, intgelu_exp_n=3
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=3, intgelu_exp_n=3, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  3
IntGELU    | n:  3
IntSoftmax | n:  3
IntGELU    | n:  3
IntSoftmax | n:  3
IntGELU    | n:  3
IntSoftmax | n:  3
IntGELU    | n:  3
IntSoftmax | n:  3
IntGELU    | n:  3
IntSoftmax | n:  3
IntGELU    | n:  3
IntSoftmax | n:  3
IntGELU    | n:  3
IntSoftmax | n:  3
IntGELU    | n:  3
IntSoftmax | n:  3
IntGELU    | n:  3
IntSoftmax | n:  3
IntGELU    | n:  3
IntSoftmax | n:  3
IntGELU    | n:  3
IntSoftmax | n:  3
IntGELU    | n:  3
.calib done
Test: [  0/391]	Time  3.782 ( 3.782)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.85
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=3, intgelu_exp_n=4
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=3, intgelu_exp_n=4, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  3
IntGELU    | n:  4
IntSoftmax | n:  3
IntGELU    | n:  4
IntSoftmax | n:  3
IntGELU    | n:  4
IntSoftmax | n:  3
IntGELU    | n:  4
IntSoftmax | n:  3
IntGELU    | n:  4
IntSoftmax | n:  3
IntGELU    | n:  4
IntSoftmax | n:  3
IntGELU    | n:  4
IntSoftmax | n:  3
IntGELU    | n:  4
IntSoftmax | n:  3
IntGELU    | n:  4
IntSoftmax | n:  3
IntGELU    | n:  4
IntSoftmax | n:  3
IntGELU    | n:  4
IntSoftmax | n:  3
IntGELU    | n:  4
.calib done
Test: [  0/391]	Time  3.758 ( 3.758)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.85
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=3, intgelu_exp_n=5
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=3, intgelu_exp_n=5, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  3
IntGELU    | n:  5
IntSoftmax | n:  3
IntGELU    | n:  5
IntSoftmax | n:  3
IntGELU    | n:  5
IntSoftmax | n:  3
IntGELU    | n:  5
IntSoftmax | n:  3
IntGELU    | n:  5
IntSoftmax | n:  3
IntGELU    | n:  5
IntSoftmax | n:  3
IntGELU    | n:  5
IntSoftmax | n:  3
IntGELU    | n:  5
IntSoftmax | n:  3
IntGELU    | n:  5
IntSoftmax | n:  3
IntGELU    | n:  5
IntSoftmax | n:  3
IntGELU    | n:  5
IntSoftmax | n:  3
IntGELU    | n:  5
.calib done
Test: [  0/391]	Time  4.048 ( 4.048)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.18
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=3, intgelu_exp_n=6
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=3, intgelu_exp_n=6, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  3
IntGELU    | n:  6
IntSoftmax | n:  3
IntGELU    | n:  6
IntSoftmax | n:  3
IntGELU    | n:  6
IntSoftmax | n:  3
IntGELU    | n:  6
IntSoftmax | n:  3
IntGELU    | n:  6
IntSoftmax | n:  3
IntGELU    | n:  6
IntSoftmax | n:  3
IntGELU    | n:  6
IntSoftmax | n:  3
IntGELU    | n:  6
IntSoftmax | n:  3
IntGELU    | n:  6
IntSoftmax | n:  3
IntGELU    | n:  6
IntSoftmax | n:  3
IntGELU    | n:  6
IntSoftmax | n:  3
IntGELU    | n:  6
.calib done
Test: [  0/391]	Time  3.970 ( 3.970)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.17
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=3, intgelu_exp_n=7
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=3, intgelu_exp_n=7, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  3
IntGELU    | n:  7
IntSoftmax | n:  3
IntGELU    | n:  7
IntSoftmax | n:  3
IntGELU    | n:  7
IntSoftmax | n:  3
IntGELU    | n:  7
IntSoftmax | n:  3
IntGELU    | n:  7
IntSoftmax | n:  3
IntGELU    | n:  7
IntSoftmax | n:  3
IntGELU    | n:  7
IntSoftmax | n:  3
IntGELU    | n:  7
IntSoftmax | n:  3
IntGELU    | n:  7
IntSoftmax | n:  3
IntGELU    | n:  7
IntSoftmax | n:  3
IntGELU    | n:  7
IntSoftmax | n:  3
IntGELU    | n:  7
.calib done
Test: [  0/391]	Time  3.792 ( 3.792)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.89
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=3, intgelu_exp_n=8
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=3, intgelu_exp_n=8, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  3
IntGELU    | n:  8
IntSoftmax | n:  3
IntGELU    | n:  8
IntSoftmax | n:  3
IntGELU    | n:  8
IntSoftmax | n:  3
IntGELU    | n:  8
IntSoftmax | n:  3
IntGELU    | n:  8
IntSoftmax | n:  3
IntGELU    | n:  8
IntSoftmax | n:  3
IntGELU    | n:  8
IntSoftmax | n:  3
IntGELU    | n:  8
IntSoftmax | n:  3
IntGELU    | n:  8
IntSoftmax | n:  3
IntGELU    | n:  8
IntSoftmax | n:  3
IntGELU    | n:  8
IntSoftmax | n:  3
IntGELU    | n:  8
.calib done
Test: [  0/391]	Time  3.962 ( 3.962)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.00
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=3, intgelu_exp_n=9
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=3, intgelu_exp_n=9, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  3
IntGELU    | n:  9
IntSoftmax | n:  3
IntGELU    | n:  9
IntSoftmax | n:  3
IntGELU    | n:  9
IntSoftmax | n:  3
IntGELU    | n:  9
IntSoftmax | n:  3
IntGELU    | n:  9
IntSoftmax | n:  3
IntGELU    | n:  9
IntSoftmax | n:  3
IntGELU    | n:  9
IntSoftmax | n:  3
IntGELU    | n:  9
IntSoftmax | n:  3
IntGELU    | n:  9
IntSoftmax | n:  3
IntGELU    | n:  9
IntSoftmax | n:  3
IntGELU    | n:  9
IntSoftmax | n:  3
IntGELU    | n:  9
.calib done
Test: [  0/391]	Time  3.882 ( 3.882)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.98
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=3, intgelu_exp_n=10
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=3, intgelu_exp_n=10, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  3
IntGELU    | n:  10
IntSoftmax | n:  3
IntGELU    | n:  10
IntSoftmax | n:  3
IntGELU    | n:  10
IntSoftmax | n:  3
IntGELU    | n:  10
IntSoftmax | n:  3
IntGELU    | n:  10
IntSoftmax | n:  3
IntGELU    | n:  10
IntSoftmax | n:  3
IntGELU    | n:  10
IntSoftmax | n:  3
IntGELU    | n:  10
IntSoftmax | n:  3
IntGELU    | n:  10
IntSoftmax | n:  3
IntGELU    | n:  10
IntSoftmax | n:  3
IntGELU    | n:  10
IntSoftmax | n:  3
IntGELU    | n:  10
.calib done
Test: [  0/391]	Time  3.918 ( 3.918)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.95
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=3, intgelu_exp_n=11
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=3, intgelu_exp_n=11, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  3
IntGELU    | n:  11
IntSoftmax | n:  3
IntGELU    | n:  11
IntSoftmax | n:  3
IntGELU    | n:  11
IntSoftmax | n:  3
IntGELU    | n:  11
IntSoftmax | n:  3
IntGELU    | n:  11
IntSoftmax | n:  3
IntGELU    | n:  11
IntSoftmax | n:  3
IntGELU    | n:  11
IntSoftmax | n:  3
IntGELU    | n:  11
IntSoftmax | n:  3
IntGELU    | n:  11
IntSoftmax | n:  3
IntGELU    | n:  11
IntSoftmax | n:  3
IntGELU    | n:  11
IntSoftmax | n:  3
IntGELU    | n:  11
.calib done
Test: [  0/391]	Time  3.810 ( 3.810)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.85
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=3, intgelu_exp_n=12
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=3, intgelu_exp_n=12, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  3
IntGELU    | n:  12
IntSoftmax | n:  3
IntGELU    | n:  12
IntSoftmax | n:  3
IntGELU    | n:  12
IntSoftmax | n:  3
IntGELU    | n:  12
IntSoftmax | n:  3
IntGELU    | n:  12
IntSoftmax | n:  3
IntGELU    | n:  12
IntSoftmax | n:  3
IntGELU    | n:  12
IntSoftmax | n:  3
IntGELU    | n:  12
IntSoftmax | n:  3
IntGELU    | n:  12
IntSoftmax | n:  3
IntGELU    | n:  12
IntSoftmax | n:  3
IntGELU    | n:  12
IntSoftmax | n:  3
IntGELU    | n:  12
.calib done
Test: [  0/391]	Time  3.793 ( 3.793)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.90
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=3, intgelu_exp_n=13
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=3, intgelu_exp_n=13, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  3
IntGELU    | n:  13
IntSoftmax | n:  3
IntGELU    | n:  13
IntSoftmax | n:  3
IntGELU    | n:  13
IntSoftmax | n:  3
IntGELU    | n:  13
IntSoftmax | n:  3
IntGELU    | n:  13
IntSoftmax | n:  3
IntGELU    | n:  13
IntSoftmax | n:  3
IntGELU    | n:  13
IntSoftmax | n:  3
IntGELU    | n:  13
IntSoftmax | n:  3
IntGELU    | n:  13
IntSoftmax | n:  3
IntGELU    | n:  13
IntSoftmax | n:  3
IntGELU    | n:  13
IntSoftmax | n:  3
IntGELU    | n:  13
.calib done
Test: [  0/391]	Time  3.924 ( 3.924)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.99
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=3, intgelu_exp_n=14
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=3, intgelu_exp_n=14, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  3
IntGELU    | n:  14
IntSoftmax | n:  3
IntGELU    | n:  14
IntSoftmax | n:  3
IntGELU    | n:  14
IntSoftmax | n:  3
IntGELU    | n:  14
IntSoftmax | n:  3
IntGELU    | n:  14
IntSoftmax | n:  3
IntGELU    | n:  14
IntSoftmax | n:  3
IntGELU    | n:  14
IntSoftmax | n:  3
IntGELU    | n:  14
IntSoftmax | n:  3
IntGELU    | n:  14
IntSoftmax | n:  3
IntGELU    | n:  14
IntSoftmax | n:  3
IntGELU    | n:  14
IntSoftmax | n:  3
IntGELU    | n:  14
.calib done
Test: [  0/391]	Time  3.823 ( 3.823)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.92
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=3, intgelu_exp_n=15
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=3, intgelu_exp_n=15, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  3
IntGELU    | n:  15
IntSoftmax | n:  3
IntGELU    | n:  15
IntSoftmax | n:  3
IntGELU    | n:  15
IntSoftmax | n:  3
IntGELU    | n:  15
IntSoftmax | n:  3
IntGELU    | n:  15
IntSoftmax | n:  3
IntGELU    | n:  15
IntSoftmax | n:  3
IntGELU    | n:  15
IntSoftmax | n:  3
IntGELU    | n:  15
IntSoftmax | n:  3
IntGELU    | n:  15
IntSoftmax | n:  3
IntGELU    | n:  15
IntSoftmax | n:  3
IntGELU    | n:  15
IntSoftmax | n:  3
IntGELU    | n:  15
.calib done
Test: [  0/391]	Time  3.837 ( 3.837)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.87
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=3, intgelu_exp_n=16
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=3, intgelu_exp_n=16, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  3
IntGELU    | n:  16
IntSoftmax | n:  3
IntGELU    | n:  16
IntSoftmax | n:  3
IntGELU    | n:  16
IntSoftmax | n:  3
IntGELU    | n:  16
IntSoftmax | n:  3
IntGELU    | n:  16
IntSoftmax | n:  3
IntGELU    | n:  16
IntSoftmax | n:  3
IntGELU    | n:  16
IntSoftmax | n:  3
IntGELU    | n:  16
IntSoftmax | n:  3
IntGELU    | n:  16
IntSoftmax | n:  3
IntGELU    | n:  16
IntSoftmax | n:  3
IntGELU    | n:  16
IntSoftmax | n:  3
IntGELU    | n:  16
.calib done
Test: [  0/391]	Time  4.032 ( 4.032)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.06
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=3, intgelu_exp_n=17
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=3, intgelu_exp_n=17, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  3
IntGELU    | n:  17
IntSoftmax | n:  3
IntGELU    | n:  17
IntSoftmax | n:  3
IntGELU    | n:  17
IntSoftmax | n:  3
IntGELU    | n:  17
IntSoftmax | n:  3
IntGELU    | n:  17
IntSoftmax | n:  3
IntGELU    | n:  17
IntSoftmax | n:  3
IntGELU    | n:  17
IntSoftmax | n:  3
IntGELU    | n:  17
IntSoftmax | n:  3
IntGELU    | n:  17
IntSoftmax | n:  3
IntGELU    | n:  17
IntSoftmax | n:  3
IntGELU    | n:  17
IntSoftmax | n:  3
IntGELU    | n:  17
.calib done
Test: [  0/391]	Time  3.845 ( 3.845)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.88
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=3, intgelu_exp_n=18
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=3, intgelu_exp_n=18, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  3
IntGELU    | n:  18
IntSoftmax | n:  3
IntGELU    | n:  18
IntSoftmax | n:  3
IntGELU    | n:  18
IntSoftmax | n:  3
IntGELU    | n:  18
IntSoftmax | n:  3
IntGELU    | n:  18
IntSoftmax | n:  3
IntGELU    | n:  18
IntSoftmax | n:  3
IntGELU    | n:  18
IntSoftmax | n:  3
IntGELU    | n:  18
IntSoftmax | n:  3
IntGELU    | n:  18
IntSoftmax | n:  3
IntGELU    | n:  18
IntSoftmax | n:  3
IntGELU    | n:  18
IntSoftmax | n:  3
IntGELU    | n:  18
.calib done
Test: [  0/391]	Time  4.138 ( 4.138)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.21
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=3, intgelu_exp_n=19
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=3, intgelu_exp_n=19, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  3
IntGELU    | n:  19
IntSoftmax | n:  3
IntGELU    | n:  19
IntSoftmax | n:  3
IntGELU    | n:  19
IntSoftmax | n:  3
IntGELU    | n:  19
IntSoftmax | n:  3
IntGELU    | n:  19
IntSoftmax | n:  3
IntGELU    | n:  19
IntSoftmax | n:  3
IntGELU    | n:  19
IntSoftmax | n:  3
IntGELU    | n:  19
IntSoftmax | n:  3
IntGELU    | n:  19
IntSoftmax | n:  3
IntGELU    | n:  19
IntSoftmax | n:  3
IntGELU    | n:  19
IntSoftmax | n:  3
IntGELU    | n:  19
.calib done
Test: [  0/391]	Time  4.023 ( 4.023)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.15
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=3, intgelu_exp_n=20
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=3, intgelu_exp_n=20, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  3
IntGELU    | n:  20
IntSoftmax | n:  3
IntGELU    | n:  20
IntSoftmax | n:  3
IntGELU    | n:  20
IntSoftmax | n:  3
IntGELU    | n:  20
IntSoftmax | n:  3
IntGELU    | n:  20
IntSoftmax | n:  3
IntGELU    | n:  20
IntSoftmax | n:  3
IntGELU    | n:  20
IntSoftmax | n:  3
IntGELU    | n:  20
IntSoftmax | n:  3
IntGELU    | n:  20
IntSoftmax | n:  3
IntGELU    | n:  20
IntSoftmax | n:  3
IntGELU    | n:  20
IntSoftmax | n:  3
IntGELU    | n:  20
.calib done
Test: [  0/391]	Time  4.059 ( 4.059)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.11
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=3, intgelu_exp_n=21
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=3, intgelu_exp_n=21, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  3
IntGELU    | n:  21
IntSoftmax | n:  3
IntGELU    | n:  21
IntSoftmax | n:  3
IntGELU    | n:  21
IntSoftmax | n:  3
IntGELU    | n:  21
IntSoftmax | n:  3
IntGELU    | n:  21
IntSoftmax | n:  3
IntGELU    | n:  21
IntSoftmax | n:  3
IntGELU    | n:  21
IntSoftmax | n:  3
IntGELU    | n:  21
IntSoftmax | n:  3
IntGELU    | n:  21
IntSoftmax | n:  3
IntGELU    | n:  21
IntSoftmax | n:  3
IntGELU    | n:  21
IntSoftmax | n:  3
IntGELU    | n:  21
.calib done
Test: [  0/391]	Time  3.777 ( 3.777)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.84
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=3, intgelu_exp_n=22
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=3, intgelu_exp_n=22, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  3
IntGELU    | n:  22
IntSoftmax | n:  3
IntGELU    | n:  22
IntSoftmax | n:  3
IntGELU    | n:  22
IntSoftmax | n:  3
IntGELU    | n:  22
IntSoftmax | n:  3
IntGELU    | n:  22
IntSoftmax | n:  3
IntGELU    | n:  22
IntSoftmax | n:  3
IntGELU    | n:  22
IntSoftmax | n:  3
IntGELU    | n:  22
IntSoftmax | n:  3
IntGELU    | n:  22
IntSoftmax | n:  3
IntGELU    | n:  22
IntSoftmax | n:  3
IntGELU    | n:  22
IntSoftmax | n:  3
IntGELU    | n:  22
.calib done
Test: [  0/391]	Time  3.823 ( 3.823)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.86
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=3, intgelu_exp_n=23
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=3, intgelu_exp_n=23, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  3
IntGELU    | n:  23
IntSoftmax | n:  3
IntGELU    | n:  23
IntSoftmax | n:  3
IntGELU    | n:  23
IntSoftmax | n:  3
IntGELU    | n:  23
IntSoftmax | n:  3
IntGELU    | n:  23
IntSoftmax | n:  3
IntGELU    | n:  23
IntSoftmax | n:  3
IntGELU    | n:  23
IntSoftmax | n:  3
IntGELU    | n:  23
IntSoftmax | n:  3
IntGELU    | n:  23
IntSoftmax | n:  3
IntGELU    | n:  23
IntSoftmax | n:  3
IntGELU    | n:  23
IntSoftmax | n:  3
IntGELU    | n:  23
.calib done
Test: [  0/391]	Time  3.673 ( 3.673)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.68
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=3, intgelu_exp_n=24
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=3, intgelu_exp_n=24, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  3
IntGELU    | n:  24
IntSoftmax | n:  3
IntGELU    | n:  24
IntSoftmax | n:  3
IntGELU    | n:  24
IntSoftmax | n:  3
IntGELU    | n:  24
IntSoftmax | n:  3
IntGELU    | n:  24
IntSoftmax | n:  3
IntGELU    | n:  24
IntSoftmax | n:  3
IntGELU    | n:  24
IntSoftmax | n:  3
IntGELU    | n:  24
IntSoftmax | n:  3
IntGELU    | n:  24
IntSoftmax | n:  3
IntGELU    | n:  24
IntSoftmax | n:  3
IntGELU    | n:  24
IntSoftmax | n:  3
IntGELU    | n:  24
.calib done
Test: [  0/391]	Time  3.817 ( 3.817)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.89
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=3, intgelu_exp_n=25
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=3, intgelu_exp_n=25, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  3
IntGELU    | n:  25
IntSoftmax | n:  3
IntGELU    | n:  25
IntSoftmax | n:  3
IntGELU    | n:  25
IntSoftmax | n:  3
IntGELU    | n:  25
IntSoftmax | n:  3
IntGELU    | n:  25
IntSoftmax | n:  3
IntGELU    | n:  25
IntSoftmax | n:  3
IntGELU    | n:  25
IntSoftmax | n:  3
IntGELU    | n:  25
IntSoftmax | n:  3
IntGELU    | n:  25
IntSoftmax | n:  3
IntGELU    | n:  25
IntSoftmax | n:  3
IntGELU    | n:  25
IntSoftmax | n:  3
IntGELU    | n:  25
.calib done
Test: [  0/391]	Time  3.816 ( 3.816)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.95
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=3, intgelu_exp_n=26
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=3, intgelu_exp_n=26, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  3
IntGELU    | n:  26
IntSoftmax | n:  3
IntGELU    | n:  26
IntSoftmax | n:  3
IntGELU    | n:  26
IntSoftmax | n:  3
IntGELU    | n:  26
IntSoftmax | n:  3
IntGELU    | n:  26
IntSoftmax | n:  3
IntGELU    | n:  26
IntSoftmax | n:  3
IntGELU    | n:  26
IntSoftmax | n:  3
IntGELU    | n:  26
IntSoftmax | n:  3
IntGELU    | n:  26
IntSoftmax | n:  3
IntGELU    | n:  26
IntSoftmax | n:  3
IntGELU    | n:  26
IntSoftmax | n:  3
IntGELU    | n:  26
.calib done
Test: [  0/391]	Time  3.965 ( 3.965)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.99
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=3, intgelu_exp_n=27
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=3, intgelu_exp_n=27, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  3
IntGELU    | n:  27
IntSoftmax | n:  3
IntGELU    | n:  27
IntSoftmax | n:  3
IntGELU    | n:  27
IntSoftmax | n:  3
IntGELU    | n:  27
IntSoftmax | n:  3
IntGELU    | n:  27
IntSoftmax | n:  3
IntGELU    | n:  27
IntSoftmax | n:  3
IntGELU    | n:  27
IntSoftmax | n:  3
IntGELU    | n:  27
IntSoftmax | n:  3
IntGELU    | n:  27
IntSoftmax | n:  3
IntGELU    | n:  27
IntSoftmax | n:  3
IntGELU    | n:  27
IntSoftmax | n:  3
IntGELU    | n:  27
.calib done
Test: [  0/391]	Time  3.692 ( 3.692)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.78
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=3, intgelu_exp_n=28
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=3, intgelu_exp_n=28, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  3
IntGELU    | n:  28
IntSoftmax | n:  3
IntGELU    | n:  28
IntSoftmax | n:  3
IntGELU    | n:  28
IntSoftmax | n:  3
IntGELU    | n:  28
IntSoftmax | n:  3
IntGELU    | n:  28
IntSoftmax | n:  3
IntGELU    | n:  28
IntSoftmax | n:  3
IntGELU    | n:  28
IntSoftmax | n:  3
IntGELU    | n:  28
IntSoftmax | n:  3
IntGELU    | n:  28
IntSoftmax | n:  3
IntGELU    | n:  28
IntSoftmax | n:  3
IntGELU    | n:  28
IntSoftmax | n:  3
IntGELU    | n:  28
.calib done
Test: [  0/391]	Time  3.833 ( 3.833)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.90
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=3, intgelu_exp_n=29
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=3, intgelu_exp_n=29, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  3
IntGELU    | n:  29
IntSoftmax | n:  3
IntGELU    | n:  29
IntSoftmax | n:  3
IntGELU    | n:  29
IntSoftmax | n:  3
IntGELU    | n:  29
IntSoftmax | n:  3
IntGELU    | n:  29
IntSoftmax | n:  3
IntGELU    | n:  29
IntSoftmax | n:  3
IntGELU    | n:  29
IntSoftmax | n:  3
IntGELU    | n:  29
IntSoftmax | n:  3
IntGELU    | n:  29
IntSoftmax | n:  3
IntGELU    | n:  29
IntSoftmax | n:  3
IntGELU    | n:  29
IntSoftmax | n:  3
IntGELU    | n:  29
.calib done
Test: [  0/391]	Time  3.706 ( 3.706)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.76
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=3, intgelu_exp_n=30
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=3, intgelu_exp_n=30, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  3
IntGELU    | n:  30
IntSoftmax | n:  3
IntGELU    | n:  30
IntSoftmax | n:  3
IntGELU    | n:  30
IntSoftmax | n:  3
IntGELU    | n:  30
IntSoftmax | n:  3
IntGELU    | n:  30
IntSoftmax | n:  3
IntGELU    | n:  30
IntSoftmax | n:  3
IntGELU    | n:  30
IntSoftmax | n:  3
IntGELU    | n:  30
IntSoftmax | n:  3
IntGELU    | n:  30
IntSoftmax | n:  3
IntGELU    | n:  30
IntSoftmax | n:  3
IntGELU    | n:  30
IntSoftmax | n:  3
IntGELU    | n:  30
.calib done
Test: [  0/391]	Time  3.943 ( 3.943)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.98
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=3, intgelu_exp_n=31
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=3, intgelu_exp_n=31, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  3
IntGELU    | n:  31
IntSoftmax | n:  3
IntGELU    | n:  31
IntSoftmax | n:  3
IntGELU    | n:  31
IntSoftmax | n:  3
IntGELU    | n:  31
IntSoftmax | n:  3
IntGELU    | n:  31
IntSoftmax | n:  3
IntGELU    | n:  31
IntSoftmax | n:  3
IntGELU    | n:  31
IntSoftmax | n:  3
IntGELU    | n:  31
IntSoftmax | n:  3
IntGELU    | n:  31
IntSoftmax | n:  3
IntGELU    | n:  31
IntSoftmax | n:  3
IntGELU    | n:  31
IntSoftmax | n:  3
IntGELU    | n:  31
.calib done
Test: [  0/391]	Time  3.681 ( 3.681)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.71
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=4, intgelu_exp_n=0
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=4, intgelu_exp_n=0, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  4
IntGELU    | n:  0
IntSoftmax | n:  4
IntGELU    | n:  0
IntSoftmax | n:  4
IntGELU    | n:  0
IntSoftmax | n:  4
IntGELU    | n:  0
IntSoftmax | n:  4
IntGELU    | n:  0
IntSoftmax | n:  4
IntGELU    | n:  0
IntSoftmax | n:  4
IntGELU    | n:  0
IntSoftmax | n:  4
IntGELU    | n:  0
IntSoftmax | n:  4
IntGELU    | n:  0
IntSoftmax | n:  4
IntGELU    | n:  0
IntSoftmax | n:  4
IntGELU    | n:  0
IntSoftmax | n:  4
IntGELU    | n:  0
.calib done
Test: [  0/391]	Time  3.896 ( 3.896)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.93
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=4, intgelu_exp_n=1
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=4, intgelu_exp_n=1, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  4
IntGELU    | n:  1
IntSoftmax | n:  4
IntGELU    | n:  1
IntSoftmax | n:  4
IntGELU    | n:  1
IntSoftmax | n:  4
IntGELU    | n:  1
IntSoftmax | n:  4
IntGELU    | n:  1
IntSoftmax | n:  4
IntGELU    | n:  1
IntSoftmax | n:  4
IntGELU    | n:  1
IntSoftmax | n:  4
IntGELU    | n:  1
IntSoftmax | n:  4
IntGELU    | n:  1
IntSoftmax | n:  4
IntGELU    | n:  1
IntSoftmax | n:  4
IntGELU    | n:  1
IntSoftmax | n:  4
IntGELU    | n:  1
.calib done
Test: [  0/391]	Time  3.932 ( 3.932)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.95
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=4, intgelu_exp_n=2
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=4, intgelu_exp_n=2, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  4
IntGELU    | n:  2
IntSoftmax | n:  4
IntGELU    | n:  2
IntSoftmax | n:  4
IntGELU    | n:  2
IntSoftmax | n:  4
IntGELU    | n:  2
IntSoftmax | n:  4
IntGELU    | n:  2
IntSoftmax | n:  4
IntGELU    | n:  2
IntSoftmax | n:  4
IntGELU    | n:  2
IntSoftmax | n:  4
IntGELU    | n:  2
IntSoftmax | n:  4
IntGELU    | n:  2
IntSoftmax | n:  4
IntGELU    | n:  2
IntSoftmax | n:  4
IntGELU    | n:  2
IntSoftmax | n:  4
IntGELU    | n:  2
.calib done
Test: [  0/391]	Time  3.932 ( 3.932)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=4, intgelu_exp_n=3
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=4, intgelu_exp_n=3, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  4
IntGELU    | n:  3
IntSoftmax | n:  4
IntGELU    | n:  3
IntSoftmax | n:  4
IntGELU    | n:  3
IntSoftmax | n:  4
IntGELU    | n:  3
IntSoftmax | n:  4
IntGELU    | n:  3
IntSoftmax | n:  4
IntGELU    | n:  3
IntSoftmax | n:  4
IntGELU    | n:  3
IntSoftmax | n:  4
IntGELU    | n:  3
IntSoftmax | n:  4
IntGELU    | n:  3
IntSoftmax | n:  4
IntGELU    | n:  3
IntSoftmax | n:  4
IntGELU    | n:  3
IntSoftmax | n:  4
IntGELU    | n:  3
.calib done
Test: [  0/391]	Time  3.774 ( 3.774)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.78
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=4, intgelu_exp_n=4
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=4, intgelu_exp_n=4, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  4
IntGELU    | n:  4
IntSoftmax | n:  4
IntGELU    | n:  4
IntSoftmax | n:  4
IntGELU    | n:  4
IntSoftmax | n:  4
IntGELU    | n:  4
IntSoftmax | n:  4
IntGELU    | n:  4
IntSoftmax | n:  4
IntGELU    | n:  4
IntSoftmax | n:  4
IntGELU    | n:  4
IntSoftmax | n:  4
IntGELU    | n:  4
IntSoftmax | n:  4
IntGELU    | n:  4
IntSoftmax | n:  4
IntGELU    | n:  4
IntSoftmax | n:  4
IntGELU    | n:  4
IntSoftmax | n:  4
IntGELU    | n:  4
.calib done
Test: [  0/391]	Time  3.722 ( 3.722)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.78
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=4, intgelu_exp_n=5
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=4, intgelu_exp_n=5, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  4
IntGELU    | n:  5
IntSoftmax | n:  4
IntGELU    | n:  5
IntSoftmax | n:  4
IntGELU    | n:  5
IntSoftmax | n:  4
IntGELU    | n:  5
IntSoftmax | n:  4
IntGELU    | n:  5
IntSoftmax | n:  4
IntGELU    | n:  5
IntSoftmax | n:  4
IntGELU    | n:  5
IntSoftmax | n:  4
IntGELU    | n:  5
IntSoftmax | n:  4
IntGELU    | n:  5
IntSoftmax | n:  4
IntGELU    | n:  5
IntSoftmax | n:  4
IntGELU    | n:  5
IntSoftmax | n:  4
IntGELU    | n:  5
.calib done
Test: [  0/391]	Time  3.892 ( 3.892)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=4, intgelu_exp_n=6
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=4, intgelu_exp_n=6, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  4
IntGELU    | n:  6
IntSoftmax | n:  4
IntGELU    | n:  6
IntSoftmax | n:  4
IntGELU    | n:  6
IntSoftmax | n:  4
IntGELU    | n:  6
IntSoftmax | n:  4
IntGELU    | n:  6
IntSoftmax | n:  4
IntGELU    | n:  6
IntSoftmax | n:  4
IntGELU    | n:  6
IntSoftmax | n:  4
IntGELU    | n:  6
IntSoftmax | n:  4
IntGELU    | n:  6
IntSoftmax | n:  4
IntGELU    | n:  6
IntSoftmax | n:  4
IntGELU    | n:  6
IntSoftmax | n:  4
IntGELU    | n:  6
.calib done
Test: [  0/391]	Time  3.916 ( 3.916)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.94
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=4, intgelu_exp_n=7
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=4, intgelu_exp_n=7, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  4
IntGELU    | n:  7
IntSoftmax | n:  4
IntGELU    | n:  7
IntSoftmax | n:  4
IntGELU    | n:  7
IntSoftmax | n:  4
IntGELU    | n:  7
IntSoftmax | n:  4
IntGELU    | n:  7
IntSoftmax | n:  4
IntGELU    | n:  7
IntSoftmax | n:  4
IntGELU    | n:  7
IntSoftmax | n:  4
IntGELU    | n:  7
IntSoftmax | n:  4
IntGELU    | n:  7
IntSoftmax | n:  4
IntGELU    | n:  7
IntSoftmax | n:  4
IntGELU    | n:  7
IntSoftmax | n:  4
IntGELU    | n:  7
.calib done
Test: [  0/391]	Time  3.943 ( 3.943)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.00
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=4, intgelu_exp_n=8
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=4, intgelu_exp_n=8, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  4
IntGELU    | n:  8
IntSoftmax | n:  4
IntGELU    | n:  8
IntSoftmax | n:  4
IntGELU    | n:  8
IntSoftmax | n:  4
IntGELU    | n:  8
IntSoftmax | n:  4
IntGELU    | n:  8
IntSoftmax | n:  4
IntGELU    | n:  8
IntSoftmax | n:  4
IntGELU    | n:  8
IntSoftmax | n:  4
IntGELU    | n:  8
IntSoftmax | n:  4
IntGELU    | n:  8
IntSoftmax | n:  4
IntGELU    | n:  8
IntSoftmax | n:  4
IntGELU    | n:  8
IntSoftmax | n:  4
IntGELU    | n:  8
.calib done
Test: [  0/391]	Time  3.927 ( 3.927)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.22
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=4, intgelu_exp_n=9
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=4, intgelu_exp_n=9, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  4
IntGELU    | n:  9
IntSoftmax | n:  4
IntGELU    | n:  9
IntSoftmax | n:  4
IntGELU    | n:  9
IntSoftmax | n:  4
IntGELU    | n:  9
IntSoftmax | n:  4
IntGELU    | n:  9
IntSoftmax | n:  4
IntGELU    | n:  9
IntSoftmax | n:  4
IntGELU    | n:  9
IntSoftmax | n:  4
IntGELU    | n:  9
IntSoftmax | n:  4
IntGELU    | n:  9
IntSoftmax | n:  4
IntGELU    | n:  9
IntSoftmax | n:  4
IntGELU    | n:  9
IntSoftmax | n:  4
IntGELU    | n:  9
.calib done
Test: [  0/391]	Time  3.942 ( 3.942)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=4, intgelu_exp_n=10
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=4, intgelu_exp_n=10, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  4
IntGELU    | n:  10
IntSoftmax | n:  4
IntGELU    | n:  10
IntSoftmax | n:  4
IntGELU    | n:  10
IntSoftmax | n:  4
IntGELU    | n:  10
IntSoftmax | n:  4
IntGELU    | n:  10
IntSoftmax | n:  4
IntGELU    | n:  10
IntSoftmax | n:  4
IntGELU    | n:  10
IntSoftmax | n:  4
IntGELU    | n:  10
IntSoftmax | n:  4
IntGELU    | n:  10
IntSoftmax | n:  4
IntGELU    | n:  10
IntSoftmax | n:  4
IntGELU    | n:  10
IntSoftmax | n:  4
IntGELU    | n:  10
.calib done
Test: [  0/391]	Time  3.890 ( 3.890)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.93
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=4, intgelu_exp_n=11
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=4, intgelu_exp_n=11, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  4
IntGELU    | n:  11
IntSoftmax | n:  4
IntGELU    | n:  11
IntSoftmax | n:  4
IntGELU    | n:  11
IntSoftmax | n:  4
IntGELU    | n:  11
IntSoftmax | n:  4
IntGELU    | n:  11
IntSoftmax | n:  4
IntGELU    | n:  11
IntSoftmax | n:  4
IntGELU    | n:  11
IntSoftmax | n:  4
IntGELU    | n:  11
IntSoftmax | n:  4
IntGELU    | n:  11
IntSoftmax | n:  4
IntGELU    | n:  11
IntSoftmax | n:  4
IntGELU    | n:  11
IntSoftmax | n:  4
IntGELU    | n:  11
.calib done
Test: [  0/391]	Time  3.919 ( 3.919)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=4, intgelu_exp_n=12
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=4, intgelu_exp_n=12, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  4
IntGELU    | n:  12
IntSoftmax | n:  4
IntGELU    | n:  12
IntSoftmax | n:  4
IntGELU    | n:  12
IntSoftmax | n:  4
IntGELU    | n:  12
IntSoftmax | n:  4
IntGELU    | n:  12
IntSoftmax | n:  4
IntGELU    | n:  12
IntSoftmax | n:  4
IntGELU    | n:  12
IntSoftmax | n:  4
IntGELU    | n:  12
IntSoftmax | n:  4
IntGELU    | n:  12
IntSoftmax | n:  4
IntGELU    | n:  12
IntSoftmax | n:  4
IntGELU    | n:  12
IntSoftmax | n:  4
IntGELU    | n:  12
.calib done
Test: [  0/391]	Time  3.651 ( 3.651)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.69
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=4, intgelu_exp_n=13
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=4, intgelu_exp_n=13, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  4
IntGELU    | n:  13
IntSoftmax | n:  4
IntGELU    | n:  13
IntSoftmax | n:  4
IntGELU    | n:  13
IntSoftmax | n:  4
IntGELU    | n:  13
IntSoftmax | n:  4
IntGELU    | n:  13
IntSoftmax | n:  4
IntGELU    | n:  13
IntSoftmax | n:  4
IntGELU    | n:  13
IntSoftmax | n:  4
IntGELU    | n:  13
IntSoftmax | n:  4
IntGELU    | n:  13
IntSoftmax | n:  4
IntGELU    | n:  13
IntSoftmax | n:  4
IntGELU    | n:  13
IntSoftmax | n:  4
IntGELU    | n:  13
.calib done
Test: [  0/391]	Time  3.905 ( 3.905)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=4, intgelu_exp_n=14
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=4, intgelu_exp_n=14, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  4
IntGELU    | n:  14
IntSoftmax | n:  4
IntGELU    | n:  14
IntSoftmax | n:  4
IntGELU    | n:  14
IntSoftmax | n:  4
IntGELU    | n:  14
IntSoftmax | n:  4
IntGELU    | n:  14
IntSoftmax | n:  4
IntGELU    | n:  14
IntSoftmax | n:  4
IntGELU    | n:  14
IntSoftmax | n:  4
IntGELU    | n:  14
IntSoftmax | n:  4
IntGELU    | n:  14
IntSoftmax | n:  4
IntGELU    | n:  14
IntSoftmax | n:  4
IntGELU    | n:  14
IntSoftmax | n:  4
IntGELU    | n:  14
.calib done
Test: [  0/391]	Time  3.933 ( 3.933)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.98
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=4, intgelu_exp_n=15
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=4, intgelu_exp_n=15, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  4
IntGELU    | n:  15
IntSoftmax | n:  4
IntGELU    | n:  15
IntSoftmax | n:  4
IntGELU    | n:  15
IntSoftmax | n:  4
IntGELU    | n:  15
IntSoftmax | n:  4
IntGELU    | n:  15
IntSoftmax | n:  4
IntGELU    | n:  15
IntSoftmax | n:  4
IntGELU    | n:  15
IntSoftmax | n:  4
IntGELU    | n:  15
IntSoftmax | n:  4
IntGELU    | n:  15
IntSoftmax | n:  4
IntGELU    | n:  15
IntSoftmax | n:  4
IntGELU    | n:  15
IntSoftmax | n:  4
IntGELU    | n:  15
.calib done
Test: [  0/391]	Time  3.882 ( 3.882)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.95
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=4, intgelu_exp_n=16
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=4, intgelu_exp_n=16, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  4
IntGELU    | n:  16
IntSoftmax | n:  4
IntGELU    | n:  16
IntSoftmax | n:  4
IntGELU    | n:  16
IntSoftmax | n:  4
IntGELU    | n:  16
IntSoftmax | n:  4
IntGELU    | n:  16
IntSoftmax | n:  4
IntGELU    | n:  16
IntSoftmax | n:  4
IntGELU    | n:  16
IntSoftmax | n:  4
IntGELU    | n:  16
IntSoftmax | n:  4
IntGELU    | n:  16
IntSoftmax | n:  4
IntGELU    | n:  16
IntSoftmax | n:  4
IntGELU    | n:  16
IntSoftmax | n:  4
IntGELU    | n:  16
.calib done
Test: [  0/391]	Time  3.724 ( 3.724)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.78
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=4, intgelu_exp_n=17
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=4, intgelu_exp_n=17, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  4
IntGELU    | n:  17
IntSoftmax | n:  4
IntGELU    | n:  17
IntSoftmax | n:  4
IntGELU    | n:  17
IntSoftmax | n:  4
IntGELU    | n:  17
IntSoftmax | n:  4
IntGELU    | n:  17
IntSoftmax | n:  4
IntGELU    | n:  17
IntSoftmax | n:  4
IntGELU    | n:  17
IntSoftmax | n:  4
IntGELU    | n:  17
IntSoftmax | n:  4
IntGELU    | n:  17
IntSoftmax | n:  4
IntGELU    | n:  17
IntSoftmax | n:  4
IntGELU    | n:  17
IntSoftmax | n:  4
IntGELU    | n:  17
.calib done
Test: [  0/391]	Time  3.901 ( 3.901)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.93
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=4, intgelu_exp_n=18
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=4, intgelu_exp_n=18, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  4
IntGELU    | n:  18
IntSoftmax | n:  4
IntGELU    | n:  18
IntSoftmax | n:  4
IntGELU    | n:  18
IntSoftmax | n:  4
IntGELU    | n:  18
IntSoftmax | n:  4
IntGELU    | n:  18
IntSoftmax | n:  4
IntGELU    | n:  18
IntSoftmax | n:  4
IntGELU    | n:  18
IntSoftmax | n:  4
IntGELU    | n:  18
IntSoftmax | n:  4
IntGELU    | n:  18
IntSoftmax | n:  4
IntGELU    | n:  18
IntSoftmax | n:  4
IntGELU    | n:  18
IntSoftmax | n:  4
IntGELU    | n:  18
.calib done
Test: [  0/391]	Time  3.995 ( 3.995)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.03
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=4, intgelu_exp_n=19
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=4, intgelu_exp_n=19, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  4
IntGELU    | n:  19
IntSoftmax | n:  4
IntGELU    | n:  19
IntSoftmax | n:  4
IntGELU    | n:  19
IntSoftmax | n:  4
IntGELU    | n:  19
IntSoftmax | n:  4
IntGELU    | n:  19
IntSoftmax | n:  4
IntGELU    | n:  19
IntSoftmax | n:  4
IntGELU    | n:  19
IntSoftmax | n:  4
IntGELU    | n:  19
IntSoftmax | n:  4
IntGELU    | n:  19
IntSoftmax | n:  4
IntGELU    | n:  19
IntSoftmax | n:  4
IntGELU    | n:  19
IntSoftmax | n:  4
IntGELU    | n:  19
.calib done
Test: [  0/391]	Time  3.951 ( 3.951)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.04
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=4, intgelu_exp_n=20
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=4, intgelu_exp_n=20, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  4
IntGELU    | n:  20
IntSoftmax | n:  4
IntGELU    | n:  20
IntSoftmax | n:  4
IntGELU    | n:  20
IntSoftmax | n:  4
IntGELU    | n:  20
IntSoftmax | n:  4
IntGELU    | n:  20
IntSoftmax | n:  4
IntGELU    | n:  20
IntSoftmax | n:  4
IntGELU    | n:  20
IntSoftmax | n:  4
IntGELU    | n:  20
IntSoftmax | n:  4
IntGELU    | n:  20
IntSoftmax | n:  4
IntGELU    | n:  20
IntSoftmax | n:  4
IntGELU    | n:  20
IntSoftmax | n:  4
IntGELU    | n:  20
.calib done
Test: [  0/391]	Time  3.806 ( 3.806)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.84
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=4, intgelu_exp_n=21
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=4, intgelu_exp_n=21, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  4
IntGELU    | n:  21
IntSoftmax | n:  4
IntGELU    | n:  21
IntSoftmax | n:  4
IntGELU    | n:  21
IntSoftmax | n:  4
IntGELU    | n:  21
IntSoftmax | n:  4
IntGELU    | n:  21
IntSoftmax | n:  4
IntGELU    | n:  21
IntSoftmax | n:  4
IntGELU    | n:  21
IntSoftmax | n:  4
IntGELU    | n:  21
IntSoftmax | n:  4
IntGELU    | n:  21
IntSoftmax | n:  4
IntGELU    | n:  21
IntSoftmax | n:  4
IntGELU    | n:  21
IntSoftmax | n:  4
IntGELU    | n:  21
.calib done
Test: [  0/391]	Time  3.785 ( 3.785)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.88
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=4, intgelu_exp_n=22
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=4, intgelu_exp_n=22, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  4
IntGELU    | n:  22
IntSoftmax | n:  4
IntGELU    | n:  22
IntSoftmax | n:  4
IntGELU    | n:  22
IntSoftmax | n:  4
IntGELU    | n:  22
IntSoftmax | n:  4
IntGELU    | n:  22
IntSoftmax | n:  4
IntGELU    | n:  22
IntSoftmax | n:  4
IntGELU    | n:  22
IntSoftmax | n:  4
IntGELU    | n:  22
IntSoftmax | n:  4
IntGELU    | n:  22
IntSoftmax | n:  4
IntGELU    | n:  22
IntSoftmax | n:  4
IntGELU    | n:  22
IntSoftmax | n:  4
IntGELU    | n:  22
.calib done
Test: [  0/391]	Time  3.745 ( 3.745)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.72
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=4, intgelu_exp_n=23
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=4, intgelu_exp_n=23, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  4
IntGELU    | n:  23
IntSoftmax | n:  4
IntGELU    | n:  23
IntSoftmax | n:  4
IntGELU    | n:  23
IntSoftmax | n:  4
IntGELU    | n:  23
IntSoftmax | n:  4
IntGELU    | n:  23
IntSoftmax | n:  4
IntGELU    | n:  23
IntSoftmax | n:  4
IntGELU    | n:  23
IntSoftmax | n:  4
IntGELU    | n:  23
IntSoftmax | n:  4
IntGELU    | n:  23
IntSoftmax | n:  4
IntGELU    | n:  23
IntSoftmax | n:  4
IntGELU    | n:  23
IntSoftmax | n:  4
IntGELU    | n:  23
.calib done
Test: [  0/391]	Time  3.662 ( 3.662)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.76
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=4, intgelu_exp_n=24
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=4, intgelu_exp_n=24, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  4
IntGELU    | n:  24
IntSoftmax | n:  4
IntGELU    | n:  24
IntSoftmax | n:  4
IntGELU    | n:  24
IntSoftmax | n:  4
IntGELU    | n:  24
IntSoftmax | n:  4
IntGELU    | n:  24
IntSoftmax | n:  4
IntGELU    | n:  24
IntSoftmax | n:  4
IntGELU    | n:  24
IntSoftmax | n:  4
IntGELU    | n:  24
IntSoftmax | n:  4
IntGELU    | n:  24
IntSoftmax | n:  4
IntGELU    | n:  24
IntSoftmax | n:  4
IntGELU    | n:  24
IntSoftmax | n:  4
IntGELU    | n:  24
.calib done
Test: [  0/391]	Time  3.876 ( 3.876)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.90
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=4, intgelu_exp_n=25
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=4, intgelu_exp_n=25, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  4
IntGELU    | n:  25
IntSoftmax | n:  4
IntGELU    | n:  25
IntSoftmax | n:  4
IntGELU    | n:  25
IntSoftmax | n:  4
IntGELU    | n:  25
IntSoftmax | n:  4
IntGELU    | n:  25
IntSoftmax | n:  4
IntGELU    | n:  25
IntSoftmax | n:  4
IntGELU    | n:  25
IntSoftmax | n:  4
IntGELU    | n:  25
IntSoftmax | n:  4
IntGELU    | n:  25
IntSoftmax | n:  4
IntGELU    | n:  25
IntSoftmax | n:  4
IntGELU    | n:  25
IntSoftmax | n:  4
IntGELU    | n:  25
.calib done
Test: [  0/391]	Time  3.870 ( 3.870)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.93
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=4, intgelu_exp_n=26
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=4, intgelu_exp_n=26, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  4
IntGELU    | n:  26
IntSoftmax | n:  4
IntGELU    | n:  26
IntSoftmax | n:  4
IntGELU    | n:  26
IntSoftmax | n:  4
IntGELU    | n:  26
IntSoftmax | n:  4
IntGELU    | n:  26
IntSoftmax | n:  4
IntGELU    | n:  26
IntSoftmax | n:  4
IntGELU    | n:  26
IntSoftmax | n:  4
IntGELU    | n:  26
IntSoftmax | n:  4
IntGELU    | n:  26
IntSoftmax | n:  4
IntGELU    | n:  26
IntSoftmax | n:  4
IntGELU    | n:  26
IntSoftmax | n:  4
IntGELU    | n:  26
.calib done
Test: [  0/391]	Time  3.910 ( 3.910)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.93
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=4, intgelu_exp_n=27
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=4, intgelu_exp_n=27, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  4
IntGELU    | n:  27
IntSoftmax | n:  4
IntGELU    | n:  27
IntSoftmax | n:  4
IntGELU    | n:  27
IntSoftmax | n:  4
IntGELU    | n:  27
IntSoftmax | n:  4
IntGELU    | n:  27
IntSoftmax | n:  4
IntGELU    | n:  27
IntSoftmax | n:  4
IntGELU    | n:  27
IntSoftmax | n:  4
IntGELU    | n:  27
IntSoftmax | n:  4
IntGELU    | n:  27
IntSoftmax | n:  4
IntGELU    | n:  27
IntSoftmax | n:  4
IntGELU    | n:  27
IntSoftmax | n:  4
IntGELU    | n:  27
.calib done
Test: [  0/391]	Time  3.995 ( 3.995)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.06
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=4, intgelu_exp_n=28
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=4, intgelu_exp_n=28, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  4
IntGELU    | n:  28
IntSoftmax | n:  4
IntGELU    | n:  28
IntSoftmax | n:  4
IntGELU    | n:  28
IntSoftmax | n:  4
IntGELU    | n:  28
IntSoftmax | n:  4
IntGELU    | n:  28
IntSoftmax | n:  4
IntGELU    | n:  28
IntSoftmax | n:  4
IntGELU    | n:  28
IntSoftmax | n:  4
IntGELU    | n:  28
IntSoftmax | n:  4
IntGELU    | n:  28
IntSoftmax | n:  4
IntGELU    | n:  28
IntSoftmax | n:  4
IntGELU    | n:  28
IntSoftmax | n:  4
IntGELU    | n:  28
.calib done
Test: [  0/391]	Time  3.685 ( 3.685)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.71
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=4, intgelu_exp_n=29
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=4, intgelu_exp_n=29, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  4
IntGELU    | n:  29
IntSoftmax | n:  4
IntGELU    | n:  29
IntSoftmax | n:  4
IntGELU    | n:  29
IntSoftmax | n:  4
IntGELU    | n:  29
IntSoftmax | n:  4
IntGELU    | n:  29
IntSoftmax | n:  4
IntGELU    | n:  29
IntSoftmax | n:  4
IntGELU    | n:  29
IntSoftmax | n:  4
IntGELU    | n:  29
IntSoftmax | n:  4
IntGELU    | n:  29
IntSoftmax | n:  4
IntGELU    | n:  29
IntSoftmax | n:  4
IntGELU    | n:  29
IntSoftmax | n:  4
IntGELU    | n:  29
.calib done
Test: [  0/391]	Time  3.844 ( 3.844)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.95
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=4, intgelu_exp_n=30
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=4, intgelu_exp_n=30, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  4
IntGELU    | n:  30
IntSoftmax | n:  4
IntGELU    | n:  30
IntSoftmax | n:  4
IntGELU    | n:  30
IntSoftmax | n:  4
IntGELU    | n:  30
IntSoftmax | n:  4
IntGELU    | n:  30
IntSoftmax | n:  4
IntGELU    | n:  30
IntSoftmax | n:  4
IntGELU    | n:  30
IntSoftmax | n:  4
IntGELU    | n:  30
IntSoftmax | n:  4
IntGELU    | n:  30
IntSoftmax | n:  4
IntGELU    | n:  30
IntSoftmax | n:  4
IntGELU    | n:  30
IntSoftmax | n:  4
IntGELU    | n:  30
.calib done
Test: [  0/391]	Time  3.844 ( 3.844)	Acc@1   0.00 (  0.00)	Acc@5   0.78 (  0.78)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.781
Time: 8.86
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=4, intgelu_exp_n=31
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=4, intgelu_exp_n=31, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  4
IntGELU    | n:  31
IntSoftmax | n:  4
IntGELU    | n:  31
IntSoftmax | n:  4
IntGELU    | n:  31
IntSoftmax | n:  4
IntGELU    | n:  31
IntSoftmax | n:  4
IntGELU    | n:  31
IntSoftmax | n:  4
IntGELU    | n:  31
IntSoftmax | n:  4
IntGELU    | n:  31
IntSoftmax | n:  4
IntGELU    | n:  31
IntSoftmax | n:  4
IntGELU    | n:  31
IntSoftmax | n:  4
IntGELU    | n:  31
IntSoftmax | n:  4
IntGELU    | n:  31
IntSoftmax | n:  4
IntGELU    | n:  31
.calib done
Test: [  0/391]	Time  3.760 ( 3.760)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.81
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=5, intgelu_exp_n=0
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=5, intgelu_exp_n=0, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  5
IntGELU    | n:  0
IntSoftmax | n:  5
IntGELU    | n:  0
IntSoftmax | n:  5
IntGELU    | n:  0
IntSoftmax | n:  5
IntGELU    | n:  0
IntSoftmax | n:  5
IntGELU    | n:  0
IntSoftmax | n:  5
IntGELU    | n:  0
IntSoftmax | n:  5
IntGELU    | n:  0
IntSoftmax | n:  5
IntGELU    | n:  0
IntSoftmax | n:  5
IntGELU    | n:  0
IntSoftmax | n:  5
IntGELU    | n:  0
IntSoftmax | n:  5
IntGELU    | n:  0
IntSoftmax | n:  5
IntGELU    | n:  0
.calib done
Test: [  0/391]	Time  3.993 ( 3.993)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.11
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=5, intgelu_exp_n=1
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=5, intgelu_exp_n=1, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  5
IntGELU    | n:  1
IntSoftmax | n:  5
IntGELU    | n:  1
IntSoftmax | n:  5
IntGELU    | n:  1
IntSoftmax | n:  5
IntGELU    | n:  1
IntSoftmax | n:  5
IntGELU    | n:  1
IntSoftmax | n:  5
IntGELU    | n:  1
IntSoftmax | n:  5
IntGELU    | n:  1
IntSoftmax | n:  5
IntGELU    | n:  1
IntSoftmax | n:  5
IntGELU    | n:  1
IntSoftmax | n:  5
IntGELU    | n:  1
IntSoftmax | n:  5
IntGELU    | n:  1
IntSoftmax | n:  5
IntGELU    | n:  1
.calib done
Test: [  0/391]	Time  3.701 ( 3.701)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.74
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=5, intgelu_exp_n=2
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=5, intgelu_exp_n=2, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  5
IntGELU    | n:  2
IntSoftmax | n:  5
IntGELU    | n:  2
IntSoftmax | n:  5
IntGELU    | n:  2
IntSoftmax | n:  5
IntGELU    | n:  2
IntSoftmax | n:  5
IntGELU    | n:  2
IntSoftmax | n:  5
IntGELU    | n:  2
IntSoftmax | n:  5
IntGELU    | n:  2
IntSoftmax | n:  5
IntGELU    | n:  2
IntSoftmax | n:  5
IntGELU    | n:  2
IntSoftmax | n:  5
IntGELU    | n:  2
IntSoftmax | n:  5
IntGELU    | n:  2
IntSoftmax | n:  5
IntGELU    | n:  2
.calib done
Test: [  0/391]	Time  3.844 ( 3.844)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.92
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=5, intgelu_exp_n=3
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=5, intgelu_exp_n=3, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  5
IntGELU    | n:  3
IntSoftmax | n:  5
IntGELU    | n:  3
IntSoftmax | n:  5
IntGELU    | n:  3
IntSoftmax | n:  5
IntGELU    | n:  3
IntSoftmax | n:  5
IntGELU    | n:  3
IntSoftmax | n:  5
IntGELU    | n:  3
IntSoftmax | n:  5
IntGELU    | n:  3
IntSoftmax | n:  5
IntGELU    | n:  3
IntSoftmax | n:  5
IntGELU    | n:  3
IntSoftmax | n:  5
IntGELU    | n:  3
IntSoftmax | n:  5
IntGELU    | n:  3
IntSoftmax | n:  5
IntGELU    | n:  3
.calib done
Test: [  0/391]	Time  3.970 ( 3.970)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.02
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=5, intgelu_exp_n=4
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=5, intgelu_exp_n=4, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  5
IntGELU    | n:  4
IntSoftmax | n:  5
IntGELU    | n:  4
IntSoftmax | n:  5
IntGELU    | n:  4
IntSoftmax | n:  5
IntGELU    | n:  4
IntSoftmax | n:  5
IntGELU    | n:  4
IntSoftmax | n:  5
IntGELU    | n:  4
IntSoftmax | n:  5
IntGELU    | n:  4
IntSoftmax | n:  5
IntGELU    | n:  4
IntSoftmax | n:  5
IntGELU    | n:  4
IntSoftmax | n:  5
IntGELU    | n:  4
IntSoftmax | n:  5
IntGELU    | n:  4
IntSoftmax | n:  5
IntGELU    | n:  4
.calib done
Test: [  0/391]	Time  4.026 ( 4.026)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.08
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=5, intgelu_exp_n=5
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=5, intgelu_exp_n=5, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  5
IntGELU    | n:  5
IntSoftmax | n:  5
IntGELU    | n:  5
IntSoftmax | n:  5
IntGELU    | n:  5
IntSoftmax | n:  5
IntGELU    | n:  5
IntSoftmax | n:  5
IntGELU    | n:  5
IntSoftmax | n:  5
IntGELU    | n:  5
IntSoftmax | n:  5
IntGELU    | n:  5
IntSoftmax | n:  5
IntGELU    | n:  5
IntSoftmax | n:  5
IntGELU    | n:  5
IntSoftmax | n:  5
IntGELU    | n:  5
IntSoftmax | n:  5
IntGELU    | n:  5
IntSoftmax | n:  5
IntGELU    | n:  5
.calib done
Test: [  0/391]	Time  3.826 ( 3.826)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.80
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=5, intgelu_exp_n=6
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=5, intgelu_exp_n=6, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  5
IntGELU    | n:  6
IntSoftmax | n:  5
IntGELU    | n:  6
IntSoftmax | n:  5
IntGELU    | n:  6
IntSoftmax | n:  5
IntGELU    | n:  6
IntSoftmax | n:  5
IntGELU    | n:  6
IntSoftmax | n:  5
IntGELU    | n:  6
IntSoftmax | n:  5
IntGELU    | n:  6
IntSoftmax | n:  5
IntGELU    | n:  6
IntSoftmax | n:  5
IntGELU    | n:  6
IntSoftmax | n:  5
IntGELU    | n:  6
IntSoftmax | n:  5
IntGELU    | n:  6
IntSoftmax | n:  5
IntGELU    | n:  6
.calib done
Test: [  0/391]	Time  3.841 ( 3.841)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.84
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=5, intgelu_exp_n=7
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=5, intgelu_exp_n=7, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  5
IntGELU    | n:  7
IntSoftmax | n:  5
IntGELU    | n:  7
IntSoftmax | n:  5
IntGELU    | n:  7
IntSoftmax | n:  5
IntGELU    | n:  7
IntSoftmax | n:  5
IntGELU    | n:  7
IntSoftmax | n:  5
IntGELU    | n:  7
IntSoftmax | n:  5
IntGELU    | n:  7
IntSoftmax | n:  5
IntGELU    | n:  7
IntSoftmax | n:  5
IntGELU    | n:  7
IntSoftmax | n:  5
IntGELU    | n:  7
IntSoftmax | n:  5
IntGELU    | n:  7
IntSoftmax | n:  5
IntGELU    | n:  7
.calib done
Test: [  0/391]	Time  3.982 ( 3.982)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.05
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=5, intgelu_exp_n=8
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=5, intgelu_exp_n=8, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  5
IntGELU    | n:  8
IntSoftmax | n:  5
IntGELU    | n:  8
IntSoftmax | n:  5
IntGELU    | n:  8
IntSoftmax | n:  5
IntGELU    | n:  8
IntSoftmax | n:  5
IntGELU    | n:  8
IntSoftmax | n:  5
IntGELU    | n:  8
IntSoftmax | n:  5
IntGELU    | n:  8
IntSoftmax | n:  5
IntGELU    | n:  8
IntSoftmax | n:  5
IntGELU    | n:  8
IntSoftmax | n:  5
IntGELU    | n:  8
IntSoftmax | n:  5
IntGELU    | n:  8
IntSoftmax | n:  5
IntGELU    | n:  8
.calib done
Test: [  0/391]	Time  3.965 ( 3.965)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.05
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=5, intgelu_exp_n=9
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=5, intgelu_exp_n=9, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  5
IntGELU    | n:  9
IntSoftmax | n:  5
IntGELU    | n:  9
IntSoftmax | n:  5
IntGELU    | n:  9
IntSoftmax | n:  5
IntGELU    | n:  9
IntSoftmax | n:  5
IntGELU    | n:  9
IntSoftmax | n:  5
IntGELU    | n:  9
IntSoftmax | n:  5
IntGELU    | n:  9
IntSoftmax | n:  5
IntGELU    | n:  9
IntSoftmax | n:  5
IntGELU    | n:  9
IntSoftmax | n:  5
IntGELU    | n:  9
IntSoftmax | n:  5
IntGELU    | n:  9
IntSoftmax | n:  5
IntGELU    | n:  9
.calib done
Test: [  0/391]	Time  3.861 ( 3.861)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.98
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=5, intgelu_exp_n=10
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=5, intgelu_exp_n=10, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  5
IntGELU    | n:  10
IntSoftmax | n:  5
IntGELU    | n:  10
IntSoftmax | n:  5
IntGELU    | n:  10
IntSoftmax | n:  5
IntGELU    | n:  10
IntSoftmax | n:  5
IntGELU    | n:  10
IntSoftmax | n:  5
IntGELU    | n:  10
IntSoftmax | n:  5
IntGELU    | n:  10
IntSoftmax | n:  5
IntGELU    | n:  10
IntSoftmax | n:  5
IntGELU    | n:  10
IntSoftmax | n:  5
IntGELU    | n:  10
IntSoftmax | n:  5
IntGELU    | n:  10
IntSoftmax | n:  5
IntGELU    | n:  10
.calib done
Test: [  0/391]	Time  3.823 ( 3.823)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.86
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=5, intgelu_exp_n=11
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=5, intgelu_exp_n=11, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  5
IntGELU    | n:  11
IntSoftmax | n:  5
IntGELU    | n:  11
IntSoftmax | n:  5
IntGELU    | n:  11
IntSoftmax | n:  5
IntGELU    | n:  11
IntSoftmax | n:  5
IntGELU    | n:  11
IntSoftmax | n:  5
IntGELU    | n:  11
IntSoftmax | n:  5
IntGELU    | n:  11
IntSoftmax | n:  5
IntGELU    | n:  11
IntSoftmax | n:  5
IntGELU    | n:  11
IntSoftmax | n:  5
IntGELU    | n:  11
IntSoftmax | n:  5
IntGELU    | n:  11
IntSoftmax | n:  5
IntGELU    | n:  11
.calib done
Test: [  0/391]	Time  3.883 ( 3.883)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=5, intgelu_exp_n=12
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=5, intgelu_exp_n=12, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  5
IntGELU    | n:  12
IntSoftmax | n:  5
IntGELU    | n:  12
IntSoftmax | n:  5
IntGELU    | n:  12
IntSoftmax | n:  5
IntGELU    | n:  12
IntSoftmax | n:  5
IntGELU    | n:  12
IntSoftmax | n:  5
IntGELU    | n:  12
IntSoftmax | n:  5
IntGELU    | n:  12
IntSoftmax | n:  5
IntGELU    | n:  12
IntSoftmax | n:  5
IntGELU    | n:  12
IntSoftmax | n:  5
IntGELU    | n:  12
IntSoftmax | n:  5
IntGELU    | n:  12
IntSoftmax | n:  5
IntGELU    | n:  12
.calib done
Test: [  0/391]	Time  3.894 ( 3.894)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.88
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=5, intgelu_exp_n=13
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=5, intgelu_exp_n=13, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  5
IntGELU    | n:  13
IntSoftmax | n:  5
IntGELU    | n:  13
IntSoftmax | n:  5
IntGELU    | n:  13
IntSoftmax | n:  5
IntGELU    | n:  13
IntSoftmax | n:  5
IntGELU    | n:  13
IntSoftmax | n:  5
IntGELU    | n:  13
IntSoftmax | n:  5
IntGELU    | n:  13
IntSoftmax | n:  5
IntGELU    | n:  13
IntSoftmax | n:  5
IntGELU    | n:  13
IntSoftmax | n:  5
IntGELU    | n:  13
IntSoftmax | n:  5
IntGELU    | n:  13
IntSoftmax | n:  5
IntGELU    | n:  13
.calib done
Test: [  0/391]	Time  4.025 ( 4.025)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.05
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=5, intgelu_exp_n=14
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=5, intgelu_exp_n=14, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  5
IntGELU    | n:  14
IntSoftmax | n:  5
IntGELU    | n:  14
IntSoftmax | n:  5
IntGELU    | n:  14
IntSoftmax | n:  5
IntGELU    | n:  14
IntSoftmax | n:  5
IntGELU    | n:  14
IntSoftmax | n:  5
IntGELU    | n:  14
IntSoftmax | n:  5
IntGELU    | n:  14
IntSoftmax | n:  5
IntGELU    | n:  14
IntSoftmax | n:  5
IntGELU    | n:  14
IntSoftmax | n:  5
IntGELU    | n:  14
IntSoftmax | n:  5
IntGELU    | n:  14
IntSoftmax | n:  5
IntGELU    | n:  14
.calib done
Test: [  0/391]	Time  3.932 ( 3.932)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=5, intgelu_exp_n=15
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=5, intgelu_exp_n=15, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  5
IntGELU    | n:  15
IntSoftmax | n:  5
IntGELU    | n:  15
IntSoftmax | n:  5
IntGELU    | n:  15
IntSoftmax | n:  5
IntGELU    | n:  15
IntSoftmax | n:  5
IntGELU    | n:  15
IntSoftmax | n:  5
IntGELU    | n:  15
IntSoftmax | n:  5
IntGELU    | n:  15
IntSoftmax | n:  5
IntGELU    | n:  15
IntSoftmax | n:  5
IntGELU    | n:  15
IntSoftmax | n:  5
IntGELU    | n:  15
IntSoftmax | n:  5
IntGELU    | n:  15
IntSoftmax | n:  5
IntGELU    | n:  15
.calib done
Test: [  0/391]	Time  3.971 ( 3.971)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.08
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=5, intgelu_exp_n=16
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=5, intgelu_exp_n=16, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  5
IntGELU    | n:  16
IntSoftmax | n:  5
IntGELU    | n:  16
IntSoftmax | n:  5
IntGELU    | n:  16
IntSoftmax | n:  5
IntGELU    | n:  16
IntSoftmax | n:  5
IntGELU    | n:  16
IntSoftmax | n:  5
IntGELU    | n:  16
IntSoftmax | n:  5
IntGELU    | n:  16
IntSoftmax | n:  5
IntGELU    | n:  16
IntSoftmax | n:  5
IntGELU    | n:  16
IntSoftmax | n:  5
IntGELU    | n:  16
IntSoftmax | n:  5
IntGELU    | n:  16
IntSoftmax | n:  5
IntGELU    | n:  16
.calib done
Test: [  0/391]	Time  3.809 ( 3.809)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.87
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=5, intgelu_exp_n=17
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=5, intgelu_exp_n=17, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  5
IntGELU    | n:  17
IntSoftmax | n:  5
IntGELU    | n:  17
IntSoftmax | n:  5
IntGELU    | n:  17
IntSoftmax | n:  5
IntGELU    | n:  17
IntSoftmax | n:  5
IntGELU    | n:  17
IntSoftmax | n:  5
IntGELU    | n:  17
IntSoftmax | n:  5
IntGELU    | n:  17
IntSoftmax | n:  5
IntGELU    | n:  17
IntSoftmax | n:  5
IntGELU    | n:  17
IntSoftmax | n:  5
IntGELU    | n:  17
IntSoftmax | n:  5
IntGELU    | n:  17
IntSoftmax | n:  5
IntGELU    | n:  17
.calib done
Test: [  0/391]	Time  4.046 ( 4.046)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.07
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=5, intgelu_exp_n=18
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=5, intgelu_exp_n=18, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  5
IntGELU    | n:  18
IntSoftmax | n:  5
IntGELU    | n:  18
IntSoftmax | n:  5
IntGELU    | n:  18
IntSoftmax | n:  5
IntGELU    | n:  18
IntSoftmax | n:  5
IntGELU    | n:  18
IntSoftmax | n:  5
IntGELU    | n:  18
IntSoftmax | n:  5
IntGELU    | n:  18
IntSoftmax | n:  5
IntGELU    | n:  18
IntSoftmax | n:  5
IntGELU    | n:  18
IntSoftmax | n:  5
IntGELU    | n:  18
IntSoftmax | n:  5
IntGELU    | n:  18
IntSoftmax | n:  5
IntGELU    | n:  18
.calib done
Test: [  0/391]	Time  4.044 ( 4.044)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.09
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=5, intgelu_exp_n=19
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=5, intgelu_exp_n=19, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  5
IntGELU    | n:  19
IntSoftmax | n:  5
IntGELU    | n:  19
IntSoftmax | n:  5
IntGELU    | n:  19
IntSoftmax | n:  5
IntGELU    | n:  19
IntSoftmax | n:  5
IntGELU    | n:  19
IntSoftmax | n:  5
IntGELU    | n:  19
IntSoftmax | n:  5
IntGELU    | n:  19
IntSoftmax | n:  5
IntGELU    | n:  19
IntSoftmax | n:  5
IntGELU    | n:  19
IntSoftmax | n:  5
IntGELU    | n:  19
IntSoftmax | n:  5
IntGELU    | n:  19
IntSoftmax | n:  5
IntGELU    | n:  19
.calib done
Test: [  0/391]	Time  3.836 ( 3.836)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.82
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=5, intgelu_exp_n=20
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=5, intgelu_exp_n=20, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  5
IntGELU    | n:  20
IntSoftmax | n:  5
IntGELU    | n:  20
IntSoftmax | n:  5
IntGELU    | n:  20
IntSoftmax | n:  5
IntGELU    | n:  20
IntSoftmax | n:  5
IntGELU    | n:  20
IntSoftmax | n:  5
IntGELU    | n:  20
IntSoftmax | n:  5
IntGELU    | n:  20
IntSoftmax | n:  5
IntGELU    | n:  20
IntSoftmax | n:  5
IntGELU    | n:  20
IntSoftmax | n:  5
IntGELU    | n:  20
IntSoftmax | n:  5
IntGELU    | n:  20
IntSoftmax | n:  5
IntGELU    | n:  20
.calib done
Test: [  0/391]	Time  3.879 ( 3.879)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.91
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=5, intgelu_exp_n=21
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=5, intgelu_exp_n=21, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  5
IntGELU    | n:  21
IntSoftmax | n:  5
IntGELU    | n:  21
IntSoftmax | n:  5
IntGELU    | n:  21
IntSoftmax | n:  5
IntGELU    | n:  21
IntSoftmax | n:  5
IntGELU    | n:  21
IntSoftmax | n:  5
IntGELU    | n:  21
IntSoftmax | n:  5
IntGELU    | n:  21
IntSoftmax | n:  5
IntGELU    | n:  21
IntSoftmax | n:  5
IntGELU    | n:  21
IntSoftmax | n:  5
IntGELU    | n:  21
IntSoftmax | n:  5
IntGELU    | n:  21
IntSoftmax | n:  5
IntGELU    | n:  21
.calib done
Test: [  0/391]	Time  3.970 ( 3.970)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.01
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=5, intgelu_exp_n=22
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=5, intgelu_exp_n=22, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  5
IntGELU    | n:  22
IntSoftmax | n:  5
IntGELU    | n:  22
IntSoftmax | n:  5
IntGELU    | n:  22
IntSoftmax | n:  5
IntGELU    | n:  22
IntSoftmax | n:  5
IntGELU    | n:  22
IntSoftmax | n:  5
IntGELU    | n:  22
IntSoftmax | n:  5
IntGELU    | n:  22
IntSoftmax | n:  5
IntGELU    | n:  22
IntSoftmax | n:  5
IntGELU    | n:  22
IntSoftmax | n:  5
IntGELU    | n:  22
IntSoftmax | n:  5
IntGELU    | n:  22
IntSoftmax | n:  5
IntGELU    | n:  22
.calib done
Test: [  0/391]	Time  3.806 ( 3.806)	Acc@1   0.00 (  0.00)	Acc@5   0.78 (  0.78)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.781
Time: 8.88
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=5, intgelu_exp_n=23
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=5, intgelu_exp_n=23, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  5
IntGELU    | n:  23
IntSoftmax | n:  5
IntGELU    | n:  23
IntSoftmax | n:  5
IntGELU    | n:  23
IntSoftmax | n:  5
IntGELU    | n:  23
IntSoftmax | n:  5
IntGELU    | n:  23
IntSoftmax | n:  5
IntGELU    | n:  23
IntSoftmax | n:  5
IntGELU    | n:  23
IntSoftmax | n:  5
IntGELU    | n:  23
IntSoftmax | n:  5
IntGELU    | n:  23
IntSoftmax | n:  5
IntGELU    | n:  23
IntSoftmax | n:  5
IntGELU    | n:  23
IntSoftmax | n:  5
IntGELU    | n:  23
.calib done
Test: [  0/391]	Time  3.696 ( 3.696)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.78
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=5, intgelu_exp_n=24
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=5, intgelu_exp_n=24, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  5
IntGELU    | n:  24
IntSoftmax | n:  5
IntGELU    | n:  24
IntSoftmax | n:  5
IntGELU    | n:  24
IntSoftmax | n:  5
IntGELU    | n:  24
IntSoftmax | n:  5
IntGELU    | n:  24
IntSoftmax | n:  5
IntGELU    | n:  24
IntSoftmax | n:  5
IntGELU    | n:  24
IntSoftmax | n:  5
IntGELU    | n:  24
IntSoftmax | n:  5
IntGELU    | n:  24
IntSoftmax | n:  5
IntGELU    | n:  24
IntSoftmax | n:  5
IntGELU    | n:  24
IntSoftmax | n:  5
IntGELU    | n:  24
.calib done
Test: [  0/391]	Time  3.838 ( 3.838)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.89
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=5, intgelu_exp_n=25
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=5, intgelu_exp_n=25, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  5
IntGELU    | n:  25
IntSoftmax | n:  5
IntGELU    | n:  25
IntSoftmax | n:  5
IntGELU    | n:  25
IntSoftmax | n:  5
IntGELU    | n:  25
IntSoftmax | n:  5
IntGELU    | n:  25
IntSoftmax | n:  5
IntGELU    | n:  25
IntSoftmax | n:  5
IntGELU    | n:  25
IntSoftmax | n:  5
IntGELU    | n:  25
IntSoftmax | n:  5
IntGELU    | n:  25
IntSoftmax | n:  5
IntGELU    | n:  25
IntSoftmax | n:  5
IntGELU    | n:  25
IntSoftmax | n:  5
IntGELU    | n:  25
.calib done
Test: [  0/391]	Time  3.747 ( 3.747)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.84
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=5, intgelu_exp_n=26
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=5, intgelu_exp_n=26, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  5
IntGELU    | n:  26
IntSoftmax | n:  5
IntGELU    | n:  26
IntSoftmax | n:  5
IntGELU    | n:  26
IntSoftmax | n:  5
IntGELU    | n:  26
IntSoftmax | n:  5
IntGELU    | n:  26
IntSoftmax | n:  5
IntGELU    | n:  26
IntSoftmax | n:  5
IntGELU    | n:  26
IntSoftmax | n:  5
IntGELU    | n:  26
IntSoftmax | n:  5
IntGELU    | n:  26
IntSoftmax | n:  5
IntGELU    | n:  26
IntSoftmax | n:  5
IntGELU    | n:  26
IntSoftmax | n:  5
IntGELU    | n:  26
.calib done
Test: [  0/391]	Time  3.909 ( 3.909)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.94
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=5, intgelu_exp_n=27
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=5, intgelu_exp_n=27, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  5
IntGELU    | n:  27
IntSoftmax | n:  5
IntGELU    | n:  27
IntSoftmax | n:  5
IntGELU    | n:  27
IntSoftmax | n:  5
IntGELU    | n:  27
IntSoftmax | n:  5
IntGELU    | n:  27
IntSoftmax | n:  5
IntGELU    | n:  27
IntSoftmax | n:  5
IntGELU    | n:  27
IntSoftmax | n:  5
IntGELU    | n:  27
IntSoftmax | n:  5
IntGELU    | n:  27
IntSoftmax | n:  5
IntGELU    | n:  27
IntSoftmax | n:  5
IntGELU    | n:  27
IntSoftmax | n:  5
IntGELU    | n:  27
.calib done
Test: [  0/391]	Time  3.898 ( 3.898)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.89
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=5, intgelu_exp_n=28
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=5, intgelu_exp_n=28, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  5
IntGELU    | n:  28
IntSoftmax | n:  5
IntGELU    | n:  28
IntSoftmax | n:  5
IntGELU    | n:  28
IntSoftmax | n:  5
IntGELU    | n:  28
IntSoftmax | n:  5
IntGELU    | n:  28
IntSoftmax | n:  5
IntGELU    | n:  28
IntSoftmax | n:  5
IntGELU    | n:  28
IntSoftmax | n:  5
IntGELU    | n:  28
IntSoftmax | n:  5
IntGELU    | n:  28
IntSoftmax | n:  5
IntGELU    | n:  28
IntSoftmax | n:  5
IntGELU    | n:  28
IntSoftmax | n:  5
IntGELU    | n:  28
.calib done
Test: [  0/391]	Time  3.969 ( 3.969)	Acc@1   0.00 (  0.00)	Acc@5   0.78 (  0.78)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.781
Time: 9.03
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=5, intgelu_exp_n=29
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=5, intgelu_exp_n=29, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  5
IntGELU    | n:  29
IntSoftmax | n:  5
IntGELU    | n:  29
IntSoftmax | n:  5
IntGELU    | n:  29
IntSoftmax | n:  5
IntGELU    | n:  29
IntSoftmax | n:  5
IntGELU    | n:  29
IntSoftmax | n:  5
IntGELU    | n:  29
IntSoftmax | n:  5
IntGELU    | n:  29
IntSoftmax | n:  5
IntGELU    | n:  29
IntSoftmax | n:  5
IntGELU    | n:  29
IntSoftmax | n:  5
IntGELU    | n:  29
IntSoftmax | n:  5
IntGELU    | n:  29
IntSoftmax | n:  5
IntGELU    | n:  29
.calib done
Test: [  0/391]	Time  3.974 ( 3.974)	Acc@1   0.78 (  0.78)	Acc@5   1.56 (  1.56)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.781 Prec@5 1.562
Time: 9.03
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=5, intgelu_exp_n=30
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=5, intgelu_exp_n=30, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  5
IntGELU    | n:  30
IntSoftmax | n:  5
IntGELU    | n:  30
IntSoftmax | n:  5
IntGELU    | n:  30
IntSoftmax | n:  5
IntGELU    | n:  30
IntSoftmax | n:  5
IntGELU    | n:  30
IntSoftmax | n:  5
IntGELU    | n:  30
IntSoftmax | n:  5
IntGELU    | n:  30
IntSoftmax | n:  5
IntGELU    | n:  30
IntSoftmax | n:  5
IntGELU    | n:  30
IntSoftmax | n:  5
IntGELU    | n:  30
IntSoftmax | n:  5
IntGELU    | n:  30
IntSoftmax | n:  5
IntGELU    | n:  30
.calib done
Test: [  0/391]	Time  3.817 ( 3.817)	Acc@1   2.34 (  2.34)	Acc@5   6.25 (  6.25)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 2.344 Prec@5 6.250
Time: 8.86
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=5, intgelu_exp_n=31
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=5, intgelu_exp_n=31, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  5
IntGELU    | n:  31
IntSoftmax | n:  5
IntGELU    | n:  31
IntSoftmax | n:  5
IntGELU    | n:  31
IntSoftmax | n:  5
IntGELU    | n:  31
IntSoftmax | n:  5
IntGELU    | n:  31
IntSoftmax | n:  5
IntGELU    | n:  31
IntSoftmax | n:  5
IntGELU    | n:  31
IntSoftmax | n:  5
IntGELU    | n:  31
IntSoftmax | n:  5
IntGELU    | n:  31
IntSoftmax | n:  5
IntGELU    | n:  31
IntSoftmax | n:  5
IntGELU    | n:  31
IntSoftmax | n:  5
IntGELU    | n:  31
.calib done
Test: [  0/391]	Time  3.962 ( 3.962)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.04
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=6, intgelu_exp_n=0
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=6, intgelu_exp_n=0, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  6
IntGELU    | n:  0
IntSoftmax | n:  6
IntGELU    | n:  0
IntSoftmax | n:  6
IntGELU    | n:  0
IntSoftmax | n:  6
IntGELU    | n:  0
IntSoftmax | n:  6
IntGELU    | n:  0
IntSoftmax | n:  6
IntGELU    | n:  0
IntSoftmax | n:  6
IntGELU    | n:  0
IntSoftmax | n:  6
IntGELU    | n:  0
IntSoftmax | n:  6
IntGELU    | n:  0
IntSoftmax | n:  6
IntGELU    | n:  0
IntSoftmax | n:  6
IntGELU    | n:  0
IntSoftmax | n:  6
IntGELU    | n:  0
.calib done
Test: [  0/391]	Time  3.963 ( 3.963)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.01
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=6, intgelu_exp_n=1
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=6, intgelu_exp_n=1, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  6
IntGELU    | n:  1
IntSoftmax | n:  6
IntGELU    | n:  1
IntSoftmax | n:  6
IntGELU    | n:  1
IntSoftmax | n:  6
IntGELU    | n:  1
IntSoftmax | n:  6
IntGELU    | n:  1
IntSoftmax | n:  6
IntGELU    | n:  1
IntSoftmax | n:  6
IntGELU    | n:  1
IntSoftmax | n:  6
IntGELU    | n:  1
IntSoftmax | n:  6
IntGELU    | n:  1
IntSoftmax | n:  6
IntGELU    | n:  1
IntSoftmax | n:  6
IntGELU    | n:  1
IntSoftmax | n:  6
IntGELU    | n:  1
.calib done
Test: [  0/391]	Time  3.869 ( 3.869)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.95
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=6, intgelu_exp_n=2
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=6, intgelu_exp_n=2, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  6
IntGELU    | n:  2
IntSoftmax | n:  6
IntGELU    | n:  2
IntSoftmax | n:  6
IntGELU    | n:  2
IntSoftmax | n:  6
IntGELU    | n:  2
IntSoftmax | n:  6
IntGELU    | n:  2
IntSoftmax | n:  6
IntGELU    | n:  2
IntSoftmax | n:  6
IntGELU    | n:  2
IntSoftmax | n:  6
IntGELU    | n:  2
IntSoftmax | n:  6
IntGELU    | n:  2
IntSoftmax | n:  6
IntGELU    | n:  2
IntSoftmax | n:  6
IntGELU    | n:  2
IntSoftmax | n:  6
IntGELU    | n:  2
.calib done
Test: [  0/391]	Time  3.807 ( 3.807)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.84
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=6, intgelu_exp_n=3
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=6, intgelu_exp_n=3, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  6
IntGELU    | n:  3
IntSoftmax | n:  6
IntGELU    | n:  3
IntSoftmax | n:  6
IntGELU    | n:  3
IntSoftmax | n:  6
IntGELU    | n:  3
IntSoftmax | n:  6
IntGELU    | n:  3
IntSoftmax | n:  6
IntGELU    | n:  3
IntSoftmax | n:  6
IntGELU    | n:  3
IntSoftmax | n:  6
IntGELU    | n:  3
IntSoftmax | n:  6
IntGELU    | n:  3
IntSoftmax | n:  6
IntGELU    | n:  3
IntSoftmax | n:  6
IntGELU    | n:  3
IntSoftmax | n:  6
IntGELU    | n:  3
.calib done
Test: [  0/391]	Time  3.841 ( 3.841)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.86
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=6, intgelu_exp_n=4
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=6, intgelu_exp_n=4, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  6
IntGELU    | n:  4
IntSoftmax | n:  6
IntGELU    | n:  4
IntSoftmax | n:  6
IntGELU    | n:  4
IntSoftmax | n:  6
IntGELU    | n:  4
IntSoftmax | n:  6
IntGELU    | n:  4
IntSoftmax | n:  6
IntGELU    | n:  4
IntSoftmax | n:  6
IntGELU    | n:  4
IntSoftmax | n:  6
IntGELU    | n:  4
IntSoftmax | n:  6
IntGELU    | n:  4
IntSoftmax | n:  6
IntGELU    | n:  4
IntSoftmax | n:  6
IntGELU    | n:  4
IntSoftmax | n:  6
IntGELU    | n:  4
.calib done
Test: [  0/391]	Time  3.871 ( 3.871)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.93
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=6, intgelu_exp_n=5
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=6, intgelu_exp_n=5, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  6
IntGELU    | n:  5
IntSoftmax | n:  6
IntGELU    | n:  5
IntSoftmax | n:  6
IntGELU    | n:  5
IntSoftmax | n:  6
IntGELU    | n:  5
IntSoftmax | n:  6
IntGELU    | n:  5
IntSoftmax | n:  6
IntGELU    | n:  5
IntSoftmax | n:  6
IntGELU    | n:  5
IntSoftmax | n:  6
IntGELU    | n:  5
IntSoftmax | n:  6
IntGELU    | n:  5
IntSoftmax | n:  6
IntGELU    | n:  5
IntSoftmax | n:  6
IntGELU    | n:  5
IntSoftmax | n:  6
IntGELU    | n:  5
.calib done
Test: [  0/391]	Time  3.837 ( 3.837)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.91
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=6, intgelu_exp_n=6
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=6, intgelu_exp_n=6, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  6
IntGELU    | n:  6
IntSoftmax | n:  6
IntGELU    | n:  6
IntSoftmax | n:  6
IntGELU    | n:  6
IntSoftmax | n:  6
IntGELU    | n:  6
IntSoftmax | n:  6
IntGELU    | n:  6
IntSoftmax | n:  6
IntGELU    | n:  6
IntSoftmax | n:  6
IntGELU    | n:  6
IntSoftmax | n:  6
IntGELU    | n:  6
IntSoftmax | n:  6
IntGELU    | n:  6
IntSoftmax | n:  6
IntGELU    | n:  6
IntSoftmax | n:  6
IntGELU    | n:  6
IntSoftmax | n:  6
IntGELU    | n:  6
.calib done
Test: [  0/391]	Time  3.832 ( 3.832)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.93
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=6, intgelu_exp_n=7
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=6, intgelu_exp_n=7, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  6
IntGELU    | n:  7
IntSoftmax | n:  6
IntGELU    | n:  7
IntSoftmax | n:  6
IntGELU    | n:  7
IntSoftmax | n:  6
IntGELU    | n:  7
IntSoftmax | n:  6
IntGELU    | n:  7
IntSoftmax | n:  6
IntGELU    | n:  7
IntSoftmax | n:  6
IntGELU    | n:  7
IntSoftmax | n:  6
IntGELU    | n:  7
IntSoftmax | n:  6
IntGELU    | n:  7
IntSoftmax | n:  6
IntGELU    | n:  7
IntSoftmax | n:  6
IntGELU    | n:  7
IntSoftmax | n:  6
IntGELU    | n:  7
.calib done
Test: [  0/391]	Time  3.868 ( 3.868)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.90
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=6, intgelu_exp_n=8
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=6, intgelu_exp_n=8, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  6
IntGELU    | n:  8
IntSoftmax | n:  6
IntGELU    | n:  8
IntSoftmax | n:  6
IntGELU    | n:  8
IntSoftmax | n:  6
IntGELU    | n:  8
IntSoftmax | n:  6
IntGELU    | n:  8
IntSoftmax | n:  6
IntGELU    | n:  8
IntSoftmax | n:  6
IntGELU    | n:  8
IntSoftmax | n:  6
IntGELU    | n:  8
IntSoftmax | n:  6
IntGELU    | n:  8
IntSoftmax | n:  6
IntGELU    | n:  8
IntSoftmax | n:  6
IntGELU    | n:  8
IntSoftmax | n:  6
IntGELU    | n:  8
.calib done
Test: [  0/391]	Time  3.892 ( 3.892)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=6, intgelu_exp_n=9
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=6, intgelu_exp_n=9, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  6
IntGELU    | n:  9
IntSoftmax | n:  6
IntGELU    | n:  9
IntSoftmax | n:  6
IntGELU    | n:  9
IntSoftmax | n:  6
IntGELU    | n:  9
IntSoftmax | n:  6
IntGELU    | n:  9
IntSoftmax | n:  6
IntGELU    | n:  9
IntSoftmax | n:  6
IntGELU    | n:  9
IntSoftmax | n:  6
IntGELU    | n:  9
IntSoftmax | n:  6
IntGELU    | n:  9
IntSoftmax | n:  6
IntGELU    | n:  9
IntSoftmax | n:  6
IntGELU    | n:  9
IntSoftmax | n:  6
IntGELU    | n:  9
.calib done
Test: [  0/391]	Time  3.925 ( 3.925)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=6, intgelu_exp_n=10
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=6, intgelu_exp_n=10, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  6
IntGELU    | n:  10
IntSoftmax | n:  6
IntGELU    | n:  10
IntSoftmax | n:  6
IntGELU    | n:  10
IntSoftmax | n:  6
IntGELU    | n:  10
IntSoftmax | n:  6
IntGELU    | n:  10
IntSoftmax | n:  6
IntGELU    | n:  10
IntSoftmax | n:  6
IntGELU    | n:  10
IntSoftmax | n:  6
IntGELU    | n:  10
IntSoftmax | n:  6
IntGELU    | n:  10
IntSoftmax | n:  6
IntGELU    | n:  10
IntSoftmax | n:  6
IntGELU    | n:  10
IntSoftmax | n:  6
IntGELU    | n:  10
.calib done
Test: [  0/391]	Time  3.811 ( 3.811)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.82
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=6, intgelu_exp_n=11
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=6, intgelu_exp_n=11, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  6
IntGELU    | n:  11
IntSoftmax | n:  6
IntGELU    | n:  11
IntSoftmax | n:  6
IntGELU    | n:  11
IntSoftmax | n:  6
IntGELU    | n:  11
IntSoftmax | n:  6
IntGELU    | n:  11
IntSoftmax | n:  6
IntGELU    | n:  11
IntSoftmax | n:  6
IntGELU    | n:  11
IntSoftmax | n:  6
IntGELU    | n:  11
IntSoftmax | n:  6
IntGELU    | n:  11
IntSoftmax | n:  6
IntGELU    | n:  11
IntSoftmax | n:  6
IntGELU    | n:  11
IntSoftmax | n:  6
IntGELU    | n:  11
.calib done
Test: [  0/391]	Time  3.916 ( 3.916)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.95
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=6, intgelu_exp_n=12
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=6, intgelu_exp_n=12, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  6
IntGELU    | n:  12
IntSoftmax | n:  6
IntGELU    | n:  12
IntSoftmax | n:  6
IntGELU    | n:  12
IntSoftmax | n:  6
IntGELU    | n:  12
IntSoftmax | n:  6
IntGELU    | n:  12
IntSoftmax | n:  6
IntGELU    | n:  12
IntSoftmax | n:  6
IntGELU    | n:  12
IntSoftmax | n:  6
IntGELU    | n:  12
IntSoftmax | n:  6
IntGELU    | n:  12
IntSoftmax | n:  6
IntGELU    | n:  12
IntSoftmax | n:  6
IntGELU    | n:  12
IntSoftmax | n:  6
IntGELU    | n:  12
.calib done
Test: [  0/391]	Time  3.763 ( 3.763)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.77
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=6, intgelu_exp_n=13
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=6, intgelu_exp_n=13, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  6
IntGELU    | n:  13
IntSoftmax | n:  6
IntGELU    | n:  13
IntSoftmax | n:  6
IntGELU    | n:  13
IntSoftmax | n:  6
IntGELU    | n:  13
IntSoftmax | n:  6
IntGELU    | n:  13
IntSoftmax | n:  6
IntGELU    | n:  13
IntSoftmax | n:  6
IntGELU    | n:  13
IntSoftmax | n:  6
IntGELU    | n:  13
IntSoftmax | n:  6
IntGELU    | n:  13
IntSoftmax | n:  6
IntGELU    | n:  13
IntSoftmax | n:  6
IntGELU    | n:  13
IntSoftmax | n:  6
IntGELU    | n:  13
.calib done
Test: [  0/391]	Time  3.869 ( 3.869)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.95
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=6, intgelu_exp_n=14
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=6, intgelu_exp_n=14, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  6
IntGELU    | n:  14
IntSoftmax | n:  6
IntGELU    | n:  14
IntSoftmax | n:  6
IntGELU    | n:  14
IntSoftmax | n:  6
IntGELU    | n:  14
IntSoftmax | n:  6
IntGELU    | n:  14
IntSoftmax | n:  6
IntGELU    | n:  14
IntSoftmax | n:  6
IntGELU    | n:  14
IntSoftmax | n:  6
IntGELU    | n:  14
IntSoftmax | n:  6
IntGELU    | n:  14
IntSoftmax | n:  6
IntGELU    | n:  14
IntSoftmax | n:  6
IntGELU    | n:  14
IntSoftmax | n:  6
IntGELU    | n:  14
.calib done
Test: [  0/391]	Time  3.950 ( 3.950)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.99
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=6, intgelu_exp_n=15
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=6, intgelu_exp_n=15, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  6
IntGELU    | n:  15
IntSoftmax | n:  6
IntGELU    | n:  15
IntSoftmax | n:  6
IntGELU    | n:  15
IntSoftmax | n:  6
IntGELU    | n:  15
IntSoftmax | n:  6
IntGELU    | n:  15
IntSoftmax | n:  6
IntGELU    | n:  15
IntSoftmax | n:  6
IntGELU    | n:  15
IntSoftmax | n:  6
IntGELU    | n:  15
IntSoftmax | n:  6
IntGELU    | n:  15
IntSoftmax | n:  6
IntGELU    | n:  15
IntSoftmax | n:  6
IntGELU    | n:  15
IntSoftmax | n:  6
IntGELU    | n:  15
.calib done
Test: [  0/391]	Time  3.912 ( 3.912)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.94
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=6, intgelu_exp_n=16
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=6, intgelu_exp_n=16, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  6
IntGELU    | n:  16
IntSoftmax | n:  6
IntGELU    | n:  16
IntSoftmax | n:  6
IntGELU    | n:  16
IntSoftmax | n:  6
IntGELU    | n:  16
IntSoftmax | n:  6
IntGELU    | n:  16
IntSoftmax | n:  6
IntGELU    | n:  16
IntSoftmax | n:  6
IntGELU    | n:  16
IntSoftmax | n:  6
IntGELU    | n:  16
IntSoftmax | n:  6
IntGELU    | n:  16
IntSoftmax | n:  6
IntGELU    | n:  16
IntSoftmax | n:  6
IntGELU    | n:  16
IntSoftmax | n:  6
IntGELU    | n:  16
.calib done
Test: [  0/391]	Time  3.798 ( 3.798)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.83
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=6, intgelu_exp_n=17
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=6, intgelu_exp_n=17, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  6
IntGELU    | n:  17
IntSoftmax | n:  6
IntGELU    | n:  17
IntSoftmax | n:  6
IntGELU    | n:  17
IntSoftmax | n:  6
IntGELU    | n:  17
IntSoftmax | n:  6
IntGELU    | n:  17
IntSoftmax | n:  6
IntGELU    | n:  17
IntSoftmax | n:  6
IntGELU    | n:  17
IntSoftmax | n:  6
IntGELU    | n:  17
IntSoftmax | n:  6
IntGELU    | n:  17
IntSoftmax | n:  6
IntGELU    | n:  17
IntSoftmax | n:  6
IntGELU    | n:  17
IntSoftmax | n:  6
IntGELU    | n:  17
.calib done
Test: [  0/391]	Time  3.911 ( 3.911)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.92
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=6, intgelu_exp_n=18
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=6, intgelu_exp_n=18, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  6
IntGELU    | n:  18
IntSoftmax | n:  6
IntGELU    | n:  18
IntSoftmax | n:  6
IntGELU    | n:  18
IntSoftmax | n:  6
IntGELU    | n:  18
IntSoftmax | n:  6
IntGELU    | n:  18
IntSoftmax | n:  6
IntGELU    | n:  18
IntSoftmax | n:  6
IntGELU    | n:  18
IntSoftmax | n:  6
IntGELU    | n:  18
IntSoftmax | n:  6
IntGELU    | n:  18
IntSoftmax | n:  6
IntGELU    | n:  18
IntSoftmax | n:  6
IntGELU    | n:  18
IntSoftmax | n:  6
IntGELU    | n:  18
.calib done
Test: [  0/391]	Time  3.832 ( 3.832)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.86
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=6, intgelu_exp_n=19
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=6, intgelu_exp_n=19, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  6
IntGELU    | n:  19
IntSoftmax | n:  6
IntGELU    | n:  19
IntSoftmax | n:  6
IntGELU    | n:  19
IntSoftmax | n:  6
IntGELU    | n:  19
IntSoftmax | n:  6
IntGELU    | n:  19
IntSoftmax | n:  6
IntGELU    | n:  19
IntSoftmax | n:  6
IntGELU    | n:  19
IntSoftmax | n:  6
IntGELU    | n:  19
IntSoftmax | n:  6
IntGELU    | n:  19
IntSoftmax | n:  6
IntGELU    | n:  19
IntSoftmax | n:  6
IntGELU    | n:  19
IntSoftmax | n:  6
IntGELU    | n:  19
.calib done
Test: [  0/391]	Time  3.701 ( 3.701)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.75
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=6, intgelu_exp_n=20
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=6, intgelu_exp_n=20, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  6
IntGELU    | n:  20
IntSoftmax | n:  6
IntGELU    | n:  20
IntSoftmax | n:  6
IntGELU    | n:  20
IntSoftmax | n:  6
IntGELU    | n:  20
IntSoftmax | n:  6
IntGELU    | n:  20
IntSoftmax | n:  6
IntGELU    | n:  20
IntSoftmax | n:  6
IntGELU    | n:  20
IntSoftmax | n:  6
IntGELU    | n:  20
IntSoftmax | n:  6
IntGELU    | n:  20
IntSoftmax | n:  6
IntGELU    | n:  20
IntSoftmax | n:  6
IntGELU    | n:  20
IntSoftmax | n:  6
IntGELU    | n:  20
.calib done
Test: [  0/391]	Time  3.730 ( 3.730)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.71
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=6, intgelu_exp_n=21
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=6, intgelu_exp_n=21, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  6
IntGELU    | n:  21
IntSoftmax | n:  6
IntGELU    | n:  21
IntSoftmax | n:  6
IntGELU    | n:  21
IntSoftmax | n:  6
IntGELU    | n:  21
IntSoftmax | n:  6
IntGELU    | n:  21
IntSoftmax | n:  6
IntGELU    | n:  21
IntSoftmax | n:  6
IntGELU    | n:  21
IntSoftmax | n:  6
IntGELU    | n:  21
IntSoftmax | n:  6
IntGELU    | n:  21
IntSoftmax | n:  6
IntGELU    | n:  21
IntSoftmax | n:  6
IntGELU    | n:  21
IntSoftmax | n:  6
IntGELU    | n:  21
.calib done
Test: [  0/391]	Time  3.939 ( 3.939)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.02
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=6, intgelu_exp_n=22
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=6, intgelu_exp_n=22, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  6
IntGELU    | n:  22
IntSoftmax | n:  6
IntGELU    | n:  22
IntSoftmax | n:  6
IntGELU    | n:  22
IntSoftmax | n:  6
IntGELU    | n:  22
IntSoftmax | n:  6
IntGELU    | n:  22
IntSoftmax | n:  6
IntGELU    | n:  22
IntSoftmax | n:  6
IntGELU    | n:  22
IntSoftmax | n:  6
IntGELU    | n:  22
IntSoftmax | n:  6
IntGELU    | n:  22
IntSoftmax | n:  6
IntGELU    | n:  22
IntSoftmax | n:  6
IntGELU    | n:  22
IntSoftmax | n:  6
IntGELU    | n:  22
.calib done
Test: [  0/391]	Time  3.886 ( 3.886)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.93
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=6, intgelu_exp_n=23
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=6, intgelu_exp_n=23, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  6
IntGELU    | n:  23
IntSoftmax | n:  6
IntGELU    | n:  23
IntSoftmax | n:  6
IntGELU    | n:  23
IntSoftmax | n:  6
IntGELU    | n:  23
IntSoftmax | n:  6
IntGELU    | n:  23
IntSoftmax | n:  6
IntGELU    | n:  23
IntSoftmax | n:  6
IntGELU    | n:  23
IntSoftmax | n:  6
IntGELU    | n:  23
IntSoftmax | n:  6
IntGELU    | n:  23
IntSoftmax | n:  6
IntGELU    | n:  23
IntSoftmax | n:  6
IntGELU    | n:  23
IntSoftmax | n:  6
IntGELU    | n:  23
.calib done
Test: [  0/391]	Time  3.849 ( 3.849)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.93
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=6, intgelu_exp_n=24
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=6, intgelu_exp_n=24, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  6
IntGELU    | n:  24
IntSoftmax | n:  6
IntGELU    | n:  24
IntSoftmax | n:  6
IntGELU    | n:  24
IntSoftmax | n:  6
IntGELU    | n:  24
IntSoftmax | n:  6
IntGELU    | n:  24
IntSoftmax | n:  6
IntGELU    | n:  24
IntSoftmax | n:  6
IntGELU    | n:  24
IntSoftmax | n:  6
IntGELU    | n:  24
IntSoftmax | n:  6
IntGELU    | n:  24
IntSoftmax | n:  6
IntGELU    | n:  24
IntSoftmax | n:  6
IntGELU    | n:  24
IntSoftmax | n:  6
IntGELU    | n:  24
.calib done
Test: [  0/391]	Time  3.956 ( 3.956)	Acc@1   0.00 (  0.00)	Acc@5   0.78 (  0.78)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.781
Time: 9.02
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=6, intgelu_exp_n=25
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=6, intgelu_exp_n=25, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  6
IntGELU    | n:  25
IntSoftmax | n:  6
IntGELU    | n:  25
IntSoftmax | n:  6
IntGELU    | n:  25
IntSoftmax | n:  6
IntGELU    | n:  25
IntSoftmax | n:  6
IntGELU    | n:  25
IntSoftmax | n:  6
IntGELU    | n:  25
IntSoftmax | n:  6
IntGELU    | n:  25
IntSoftmax | n:  6
IntGELU    | n:  25
IntSoftmax | n:  6
IntGELU    | n:  25
IntSoftmax | n:  6
IntGELU    | n:  25
IntSoftmax | n:  6
IntGELU    | n:  25
IntSoftmax | n:  6
IntGELU    | n:  25
.calib done
Test: [  0/391]	Time  3.995 ( 3.995)	Acc@1   0.00 (  0.00)	Acc@5   0.78 (  0.78)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.781
Time: 9.06
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=6, intgelu_exp_n=26
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=6, intgelu_exp_n=26, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  6
IntGELU    | n:  26
IntSoftmax | n:  6
IntGELU    | n:  26
IntSoftmax | n:  6
IntGELU    | n:  26
IntSoftmax | n:  6
IntGELU    | n:  26
IntSoftmax | n:  6
IntGELU    | n:  26
IntSoftmax | n:  6
IntGELU    | n:  26
IntSoftmax | n:  6
IntGELU    | n:  26
IntSoftmax | n:  6
IntGELU    | n:  26
IntSoftmax | n:  6
IntGELU    | n:  26
IntSoftmax | n:  6
IntGELU    | n:  26
IntSoftmax | n:  6
IntGELU    | n:  26
IntSoftmax | n:  6
IntGELU    | n:  26
.calib done
Test: [  0/391]	Time  3.932 ( 3.932)	Acc@1   0.00 (  0.00)	Acc@5   3.12 (  3.12)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 3.125
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=6, intgelu_exp_n=27
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=6, intgelu_exp_n=27, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  6
IntGELU    | n:  27
IntSoftmax | n:  6
IntGELU    | n:  27
IntSoftmax | n:  6
IntGELU    | n:  27
IntSoftmax | n:  6
IntGELU    | n:  27
IntSoftmax | n:  6
IntGELU    | n:  27
IntSoftmax | n:  6
IntGELU    | n:  27
IntSoftmax | n:  6
IntGELU    | n:  27
IntSoftmax | n:  6
IntGELU    | n:  27
IntSoftmax | n:  6
IntGELU    | n:  27
IntSoftmax | n:  6
IntGELU    | n:  27
IntSoftmax | n:  6
IntGELU    | n:  27
IntSoftmax | n:  6
IntGELU    | n:  27
.calib done
Test: [  0/391]	Time  3.818 ( 3.818)	Acc@1   0.00 (  0.00)	Acc@5   2.34 (  2.34)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 2.344
Time: 8.93
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=6, intgelu_exp_n=28
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=6, intgelu_exp_n=28, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  6
IntGELU    | n:  28
IntSoftmax | n:  6
IntGELU    | n:  28
IntSoftmax | n:  6
IntGELU    | n:  28
IntSoftmax | n:  6
IntGELU    | n:  28
IntSoftmax | n:  6
IntGELU    | n:  28
IntSoftmax | n:  6
IntGELU    | n:  28
IntSoftmax | n:  6
IntGELU    | n:  28
IntSoftmax | n:  6
IntGELU    | n:  28
IntSoftmax | n:  6
IntGELU    | n:  28
IntSoftmax | n:  6
IntGELU    | n:  28
IntSoftmax | n:  6
IntGELU    | n:  28
IntSoftmax | n:  6
IntGELU    | n:  28
.calib done
Test: [  0/391]	Time  3.857 ( 3.857)	Acc@1   0.78 (  0.78)	Acc@5   3.12 (  3.12)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.781 Prec@5 3.125
Time: 8.85
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=6, intgelu_exp_n=29
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=6, intgelu_exp_n=29, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  6
IntGELU    | n:  29
IntSoftmax | n:  6
IntGELU    | n:  29
IntSoftmax | n:  6
IntGELU    | n:  29
IntSoftmax | n:  6
IntGELU    | n:  29
IntSoftmax | n:  6
IntGELU    | n:  29
IntSoftmax | n:  6
IntGELU    | n:  29
IntSoftmax | n:  6
IntGELU    | n:  29
IntSoftmax | n:  6
IntGELU    | n:  29
IntSoftmax | n:  6
IntGELU    | n:  29
IntSoftmax | n:  6
IntGELU    | n:  29
IntSoftmax | n:  6
IntGELU    | n:  29
IntSoftmax | n:  6
IntGELU    | n:  29
.calib done
Test: [  0/391]	Time  3.904 ( 3.904)	Acc@1   4.69 (  4.69)	Acc@5  10.94 ( 10.94)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 4.688 Prec@5 10.938
Time: 9.03
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=6, intgelu_exp_n=30
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=6, intgelu_exp_n=30, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  6
IntGELU    | n:  30
IntSoftmax | n:  6
IntGELU    | n:  30
IntSoftmax | n:  6
IntGELU    | n:  30
IntSoftmax | n:  6
IntGELU    | n:  30
IntSoftmax | n:  6
IntGELU    | n:  30
IntSoftmax | n:  6
IntGELU    | n:  30
IntSoftmax | n:  6
IntGELU    | n:  30
IntSoftmax | n:  6
IntGELU    | n:  30
IntSoftmax | n:  6
IntGELU    | n:  30
IntSoftmax | n:  6
IntGELU    | n:  30
IntSoftmax | n:  6
IntGELU    | n:  30
IntSoftmax | n:  6
IntGELU    | n:  30
.calib done
Test: [  0/391]	Time  3.948 ( 3.948)	Acc@1   3.12 (  3.12)	Acc@5  12.50 ( 12.50)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 3.125 Prec@5 12.500
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=6, intgelu_exp_n=31
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=6, intgelu_exp_n=31, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  6
IntGELU    | n:  31
IntSoftmax | n:  6
IntGELU    | n:  31
IntSoftmax | n:  6
IntGELU    | n:  31
IntSoftmax | n:  6
IntGELU    | n:  31
IntSoftmax | n:  6
IntGELU    | n:  31
IntSoftmax | n:  6
IntGELU    | n:  31
IntSoftmax | n:  6
IntGELU    | n:  31
IntSoftmax | n:  6
IntGELU    | n:  31
IntSoftmax | n:  6
IntGELU    | n:  31
IntSoftmax | n:  6
IntGELU    | n:  31
IntSoftmax | n:  6
IntGELU    | n:  31
IntSoftmax | n:  6
IntGELU    | n:  31
.calib done
Test: [  0/391]	Time  3.999 ( 3.999)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.08
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=7, intgelu_exp_n=0
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=7, intgelu_exp_n=0, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  7
IntGELU    | n:  0
IntSoftmax | n:  7
IntGELU    | n:  0
IntSoftmax | n:  7
IntGELU    | n:  0
IntSoftmax | n:  7
IntGELU    | n:  0
IntSoftmax | n:  7
IntGELU    | n:  0
IntSoftmax | n:  7
IntGELU    | n:  0
IntSoftmax | n:  7
IntGELU    | n:  0
IntSoftmax | n:  7
IntGELU    | n:  0
IntSoftmax | n:  7
IntGELU    | n:  0
IntSoftmax | n:  7
IntGELU    | n:  0
IntSoftmax | n:  7
IntGELU    | n:  0
IntSoftmax | n:  7
IntGELU    | n:  0
.calib done
Test: [  0/391]	Time  4.016 ( 4.016)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.11
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=7, intgelu_exp_n=1
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=7, intgelu_exp_n=1, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  7
IntGELU    | n:  1
IntSoftmax | n:  7
IntGELU    | n:  1
IntSoftmax | n:  7
IntGELU    | n:  1
IntSoftmax | n:  7
IntGELU    | n:  1
IntSoftmax | n:  7
IntGELU    | n:  1
IntSoftmax | n:  7
IntGELU    | n:  1
IntSoftmax | n:  7
IntGELU    | n:  1
IntSoftmax | n:  7
IntGELU    | n:  1
IntSoftmax | n:  7
IntGELU    | n:  1
IntSoftmax | n:  7
IntGELU    | n:  1
IntSoftmax | n:  7
IntGELU    | n:  1
IntSoftmax | n:  7
IntGELU    | n:  1
.calib done
Test: [  0/391]	Time  3.977 ( 3.977)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.09
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=7, intgelu_exp_n=2
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=7, intgelu_exp_n=2, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  7
IntGELU    | n:  2
IntSoftmax | n:  7
IntGELU    | n:  2
IntSoftmax | n:  7
IntGELU    | n:  2
IntSoftmax | n:  7
IntGELU    | n:  2
IntSoftmax | n:  7
IntGELU    | n:  2
IntSoftmax | n:  7
IntGELU    | n:  2
IntSoftmax | n:  7
IntGELU    | n:  2
IntSoftmax | n:  7
IntGELU    | n:  2
IntSoftmax | n:  7
IntGELU    | n:  2
IntSoftmax | n:  7
IntGELU    | n:  2
IntSoftmax | n:  7
IntGELU    | n:  2
IntSoftmax | n:  7
IntGELU    | n:  2
.calib done
Test: [  0/391]	Time  3.945 ( 3.945)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=7, intgelu_exp_n=3
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=7, intgelu_exp_n=3, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  7
IntGELU    | n:  3
IntSoftmax | n:  7
IntGELU    | n:  3
IntSoftmax | n:  7
IntGELU    | n:  3
IntSoftmax | n:  7
IntGELU    | n:  3
IntSoftmax | n:  7
IntGELU    | n:  3
IntSoftmax | n:  7
IntGELU    | n:  3
IntSoftmax | n:  7
IntGELU    | n:  3
IntSoftmax | n:  7
IntGELU    | n:  3
IntSoftmax | n:  7
IntGELU    | n:  3
IntSoftmax | n:  7
IntGELU    | n:  3
IntSoftmax | n:  7
IntGELU    | n:  3
IntSoftmax | n:  7
IntGELU    | n:  3
.calib done
Test: [  0/391]	Time  3.876 ( 3.876)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.92
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=7, intgelu_exp_n=4
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=7, intgelu_exp_n=4, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  7
IntGELU    | n:  4
IntSoftmax | n:  7
IntGELU    | n:  4
IntSoftmax | n:  7
IntGELU    | n:  4
IntSoftmax | n:  7
IntGELU    | n:  4
IntSoftmax | n:  7
IntGELU    | n:  4
IntSoftmax | n:  7
IntGELU    | n:  4
IntSoftmax | n:  7
IntGELU    | n:  4
IntSoftmax | n:  7
IntGELU    | n:  4
IntSoftmax | n:  7
IntGELU    | n:  4
IntSoftmax | n:  7
IntGELU    | n:  4
IntSoftmax | n:  7
IntGELU    | n:  4
IntSoftmax | n:  7
IntGELU    | n:  4
.calib done
Test: [  0/391]	Time  3.920 ( 3.920)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.94
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=7, intgelu_exp_n=5
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=7, intgelu_exp_n=5, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  7
IntGELU    | n:  5
IntSoftmax | n:  7
IntGELU    | n:  5
IntSoftmax | n:  7
IntGELU    | n:  5
IntSoftmax | n:  7
IntGELU    | n:  5
IntSoftmax | n:  7
IntGELU    | n:  5
IntSoftmax | n:  7
IntGELU    | n:  5
IntSoftmax | n:  7
IntGELU    | n:  5
IntSoftmax | n:  7
IntGELU    | n:  5
IntSoftmax | n:  7
IntGELU    | n:  5
IntSoftmax | n:  7
IntGELU    | n:  5
IntSoftmax | n:  7
IntGELU    | n:  5
IntSoftmax | n:  7
IntGELU    | n:  5
.calib done
Test: [  0/391]	Time  3.956 ( 3.956)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.98
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=7, intgelu_exp_n=6
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=7, intgelu_exp_n=6, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  7
IntGELU    | n:  6
IntSoftmax | n:  7
IntGELU    | n:  6
IntSoftmax | n:  7
IntGELU    | n:  6
IntSoftmax | n:  7
IntGELU    | n:  6
IntSoftmax | n:  7
IntGELU    | n:  6
IntSoftmax | n:  7
IntGELU    | n:  6
IntSoftmax | n:  7
IntGELU    | n:  6
IntSoftmax | n:  7
IntGELU    | n:  6
IntSoftmax | n:  7
IntGELU    | n:  6
IntSoftmax | n:  7
IntGELU    | n:  6
IntSoftmax | n:  7
IntGELU    | n:  6
IntSoftmax | n:  7
IntGELU    | n:  6
.calib done
Test: [  0/391]	Time  3.914 ( 3.914)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.92
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=7, intgelu_exp_n=7
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=7, intgelu_exp_n=7, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  7
IntGELU    | n:  7
IntSoftmax | n:  7
IntGELU    | n:  7
IntSoftmax | n:  7
IntGELU    | n:  7
IntSoftmax | n:  7
IntGELU    | n:  7
IntSoftmax | n:  7
IntGELU    | n:  7
IntSoftmax | n:  7
IntGELU    | n:  7
IntSoftmax | n:  7
IntGELU    | n:  7
IntSoftmax | n:  7
IntGELU    | n:  7
IntSoftmax | n:  7
IntGELU    | n:  7
IntSoftmax | n:  7
IntGELU    | n:  7
IntSoftmax | n:  7
IntGELU    | n:  7
IntSoftmax | n:  7
IntGELU    | n:  7
.calib done
Test: [  0/391]	Time  3.613 ( 3.613)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.70
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=7, intgelu_exp_n=8
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=7, intgelu_exp_n=8, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  7
IntGELU    | n:  8
IntSoftmax | n:  7
IntGELU    | n:  8
IntSoftmax | n:  7
IntGELU    | n:  8
IntSoftmax | n:  7
IntGELU    | n:  8
IntSoftmax | n:  7
IntGELU    | n:  8
IntSoftmax | n:  7
IntGELU    | n:  8
IntSoftmax | n:  7
IntGELU    | n:  8
IntSoftmax | n:  7
IntGELU    | n:  8
IntSoftmax | n:  7
IntGELU    | n:  8
IntSoftmax | n:  7
IntGELU    | n:  8
IntSoftmax | n:  7
IntGELU    | n:  8
IntSoftmax | n:  7
IntGELU    | n:  8
.calib done
Test: [  0/391]	Time  3.840 ( 3.840)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.88
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=7, intgelu_exp_n=9
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=7, intgelu_exp_n=9, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  7
IntGELU    | n:  9
IntSoftmax | n:  7
IntGELU    | n:  9
IntSoftmax | n:  7
IntGELU    | n:  9
IntSoftmax | n:  7
IntGELU    | n:  9
IntSoftmax | n:  7
IntGELU    | n:  9
IntSoftmax | n:  7
IntGELU    | n:  9
IntSoftmax | n:  7
IntGELU    | n:  9
IntSoftmax | n:  7
IntGELU    | n:  9
IntSoftmax | n:  7
IntGELU    | n:  9
IntSoftmax | n:  7
IntGELU    | n:  9
IntSoftmax | n:  7
IntGELU    | n:  9
IntSoftmax | n:  7
IntGELU    | n:  9
.calib done
Test: [  0/391]	Time  3.821 ( 3.821)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.85
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=7, intgelu_exp_n=10
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=7, intgelu_exp_n=10, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  7
IntGELU    | n:  10
IntSoftmax | n:  7
IntGELU    | n:  10
IntSoftmax | n:  7
IntGELU    | n:  10
IntSoftmax | n:  7
IntGELU    | n:  10
IntSoftmax | n:  7
IntGELU    | n:  10
IntSoftmax | n:  7
IntGELU    | n:  10
IntSoftmax | n:  7
IntGELU    | n:  10
IntSoftmax | n:  7
IntGELU    | n:  10
IntSoftmax | n:  7
IntGELU    | n:  10
IntSoftmax | n:  7
IntGELU    | n:  10
IntSoftmax | n:  7
IntGELU    | n:  10
IntSoftmax | n:  7
IntGELU    | n:  10
.calib done
Test: [  0/391]	Time  3.635 ( 3.635)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.66
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=7, intgelu_exp_n=11
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=7, intgelu_exp_n=11, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  7
IntGELU    | n:  11
IntSoftmax | n:  7
IntGELU    | n:  11
IntSoftmax | n:  7
IntGELU    | n:  11
IntSoftmax | n:  7
IntGELU    | n:  11
IntSoftmax | n:  7
IntGELU    | n:  11
IntSoftmax | n:  7
IntGELU    | n:  11
IntSoftmax | n:  7
IntGELU    | n:  11
IntSoftmax | n:  7
IntGELU    | n:  11
IntSoftmax | n:  7
IntGELU    | n:  11
IntSoftmax | n:  7
IntGELU    | n:  11
IntSoftmax | n:  7
IntGELU    | n:  11
IntSoftmax | n:  7
IntGELU    | n:  11
.calib done
Test: [  0/391]	Time  3.670 ( 3.670)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.73
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=7, intgelu_exp_n=12
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=7, intgelu_exp_n=12, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  7
IntGELU    | n:  12
IntSoftmax | n:  7
IntGELU    | n:  12
IntSoftmax | n:  7
IntGELU    | n:  12
IntSoftmax | n:  7
IntGELU    | n:  12
IntSoftmax | n:  7
IntGELU    | n:  12
IntSoftmax | n:  7
IntGELU    | n:  12
IntSoftmax | n:  7
IntGELU    | n:  12
IntSoftmax | n:  7
IntGELU    | n:  12
IntSoftmax | n:  7
IntGELU    | n:  12
IntSoftmax | n:  7
IntGELU    | n:  12
IntSoftmax | n:  7
IntGELU    | n:  12
IntSoftmax | n:  7
IntGELU    | n:  12
.calib done
Test: [  0/391]	Time  3.961 ( 3.961)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.99
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=7, intgelu_exp_n=13
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=7, intgelu_exp_n=13, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  7
IntGELU    | n:  13
IntSoftmax | n:  7
IntGELU    | n:  13
IntSoftmax | n:  7
IntGELU    | n:  13
IntSoftmax | n:  7
IntGELU    | n:  13
IntSoftmax | n:  7
IntGELU    | n:  13
IntSoftmax | n:  7
IntGELU    | n:  13
IntSoftmax | n:  7
IntGELU    | n:  13
IntSoftmax | n:  7
IntGELU    | n:  13
IntSoftmax | n:  7
IntGELU    | n:  13
IntSoftmax | n:  7
IntGELU    | n:  13
IntSoftmax | n:  7
IntGELU    | n:  13
IntSoftmax | n:  7
IntGELU    | n:  13
.calib done
Test: [  0/391]	Time  3.820 ( 3.820)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.88
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=7, intgelu_exp_n=14
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=7, intgelu_exp_n=14, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  7
IntGELU    | n:  14
IntSoftmax | n:  7
IntGELU    | n:  14
IntSoftmax | n:  7
IntGELU    | n:  14
IntSoftmax | n:  7
IntGELU    | n:  14
IntSoftmax | n:  7
IntGELU    | n:  14
IntSoftmax | n:  7
IntGELU    | n:  14
IntSoftmax | n:  7
IntGELU    | n:  14
IntSoftmax | n:  7
IntGELU    | n:  14
IntSoftmax | n:  7
IntGELU    | n:  14
IntSoftmax | n:  7
IntGELU    | n:  14
IntSoftmax | n:  7
IntGELU    | n:  14
IntSoftmax | n:  7
IntGELU    | n:  14
.calib done
Test: [  0/391]	Time  3.690 ( 3.690)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.75
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=7, intgelu_exp_n=15
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=7, intgelu_exp_n=15, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  7
IntGELU    | n:  15
IntSoftmax | n:  7
IntGELU    | n:  15
IntSoftmax | n:  7
IntGELU    | n:  15
IntSoftmax | n:  7
IntGELU    | n:  15
IntSoftmax | n:  7
IntGELU    | n:  15
IntSoftmax | n:  7
IntGELU    | n:  15
IntSoftmax | n:  7
IntGELU    | n:  15
IntSoftmax | n:  7
IntGELU    | n:  15
IntSoftmax | n:  7
IntGELU    | n:  15
IntSoftmax | n:  7
IntGELU    | n:  15
IntSoftmax | n:  7
IntGELU    | n:  15
IntSoftmax | n:  7
IntGELU    | n:  15
.calib done
Test: [  0/391]	Time  4.026 ( 4.026)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.03
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=7, intgelu_exp_n=16
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=7, intgelu_exp_n=16, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  7
IntGELU    | n:  16
IntSoftmax | n:  7
IntGELU    | n:  16
IntSoftmax | n:  7
IntGELU    | n:  16
IntSoftmax | n:  7
IntGELU    | n:  16
IntSoftmax | n:  7
IntGELU    | n:  16
IntSoftmax | n:  7
IntGELU    | n:  16
IntSoftmax | n:  7
IntGELU    | n:  16
IntSoftmax | n:  7
IntGELU    | n:  16
IntSoftmax | n:  7
IntGELU    | n:  16
IntSoftmax | n:  7
IntGELU    | n:  16
IntSoftmax | n:  7
IntGELU    | n:  16
IntSoftmax | n:  7
IntGELU    | n:  16
.calib done
Test: [  0/391]	Time  3.714 ( 3.714)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.81
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=7, intgelu_exp_n=17
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=7, intgelu_exp_n=17, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  7
IntGELU    | n:  17
IntSoftmax | n:  7
IntGELU    | n:  17
IntSoftmax | n:  7
IntGELU    | n:  17
IntSoftmax | n:  7
IntGELU    | n:  17
IntSoftmax | n:  7
IntGELU    | n:  17
IntSoftmax | n:  7
IntGELU    | n:  17
IntSoftmax | n:  7
IntGELU    | n:  17
IntSoftmax | n:  7
IntGELU    | n:  17
IntSoftmax | n:  7
IntGELU    | n:  17
IntSoftmax | n:  7
IntGELU    | n:  17
IntSoftmax | n:  7
IntGELU    | n:  17
IntSoftmax | n:  7
IntGELU    | n:  17
.calib done
Test: [  0/391]	Time  3.685 ( 3.685)	Acc@1   0.00 (  0.00)	Acc@5   0.78 (  0.78)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.781
Time: 8.71
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=7, intgelu_exp_n=18
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=7, intgelu_exp_n=18, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  7
IntGELU    | n:  18
IntSoftmax | n:  7
IntGELU    | n:  18
IntSoftmax | n:  7
IntGELU    | n:  18
IntSoftmax | n:  7
IntGELU    | n:  18
IntSoftmax | n:  7
IntGELU    | n:  18
IntSoftmax | n:  7
IntGELU    | n:  18
IntSoftmax | n:  7
IntGELU    | n:  18
IntSoftmax | n:  7
IntGELU    | n:  18
IntSoftmax | n:  7
IntGELU    | n:  18
IntSoftmax | n:  7
IntGELU    | n:  18
IntSoftmax | n:  7
IntGELU    | n:  18
IntSoftmax | n:  7
IntGELU    | n:  18
.calib done
Test: [  0/391]	Time  3.937 ( 3.937)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.00
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=7, intgelu_exp_n=19
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=7, intgelu_exp_n=19, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  7
IntGELU    | n:  19
IntSoftmax | n:  7
IntGELU    | n:  19
IntSoftmax | n:  7
IntGELU    | n:  19
IntSoftmax | n:  7
IntGELU    | n:  19
IntSoftmax | n:  7
IntGELU    | n:  19
IntSoftmax | n:  7
IntGELU    | n:  19
IntSoftmax | n:  7
IntGELU    | n:  19
IntSoftmax | n:  7
IntGELU    | n:  19
IntSoftmax | n:  7
IntGELU    | n:  19
IntSoftmax | n:  7
IntGELU    | n:  19
IntSoftmax | n:  7
IntGELU    | n:  19
IntSoftmax | n:  7
IntGELU    | n:  19
.calib done
Test: [  0/391]	Time  4.006 ( 4.006)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.05
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=7, intgelu_exp_n=20
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=7, intgelu_exp_n=20, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  7
IntGELU    | n:  20
IntSoftmax | n:  7
IntGELU    | n:  20
IntSoftmax | n:  7
IntGELU    | n:  20
IntSoftmax | n:  7
IntGELU    | n:  20
IntSoftmax | n:  7
IntGELU    | n:  20
IntSoftmax | n:  7
IntGELU    | n:  20
IntSoftmax | n:  7
IntGELU    | n:  20
IntSoftmax | n:  7
IntGELU    | n:  20
IntSoftmax | n:  7
IntGELU    | n:  20
IntSoftmax | n:  7
IntGELU    | n:  20
IntSoftmax | n:  7
IntGELU    | n:  20
IntSoftmax | n:  7
IntGELU    | n:  20
.calib done
Test: [  0/391]	Time  3.734 ( 3.734)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.81
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=7, intgelu_exp_n=21
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=7, intgelu_exp_n=21, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  7
IntGELU    | n:  21
IntSoftmax | n:  7
IntGELU    | n:  21
IntSoftmax | n:  7
IntGELU    | n:  21
IntSoftmax | n:  7
IntGELU    | n:  21
IntSoftmax | n:  7
IntGELU    | n:  21
IntSoftmax | n:  7
IntGELU    | n:  21
IntSoftmax | n:  7
IntGELU    | n:  21
IntSoftmax | n:  7
IntGELU    | n:  21
IntSoftmax | n:  7
IntGELU    | n:  21
IntSoftmax | n:  7
IntGELU    | n:  21
IntSoftmax | n:  7
IntGELU    | n:  21
IntSoftmax | n:  7
IntGELU    | n:  21
.calib done
Test: [  0/391]	Time  3.920 ( 3.920)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.95
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=7, intgelu_exp_n=22
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=7, intgelu_exp_n=22, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  7
IntGELU    | n:  22
IntSoftmax | n:  7
IntGELU    | n:  22
IntSoftmax | n:  7
IntGELU    | n:  22
IntSoftmax | n:  7
IntGELU    | n:  22
IntSoftmax | n:  7
IntGELU    | n:  22
IntSoftmax | n:  7
IntGELU    | n:  22
IntSoftmax | n:  7
IntGELU    | n:  22
IntSoftmax | n:  7
IntGELU    | n:  22
IntSoftmax | n:  7
IntGELU    | n:  22
IntSoftmax | n:  7
IntGELU    | n:  22
IntSoftmax | n:  7
IntGELU    | n:  22
IntSoftmax | n:  7
IntGELU    | n:  22
.calib done
Test: [  0/391]	Time  3.978 ( 3.978)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.09
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=7, intgelu_exp_n=23
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=7, intgelu_exp_n=23, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  7
IntGELU    | n:  23
IntSoftmax | n:  7
IntGELU    | n:  23
IntSoftmax | n:  7
IntGELU    | n:  23
IntSoftmax | n:  7
IntGELU    | n:  23
IntSoftmax | n:  7
IntGELU    | n:  23
IntSoftmax | n:  7
IntGELU    | n:  23
IntSoftmax | n:  7
IntGELU    | n:  23
IntSoftmax | n:  7
IntGELU    | n:  23
IntSoftmax | n:  7
IntGELU    | n:  23
IntSoftmax | n:  7
IntGELU    | n:  23
IntSoftmax | n:  7
IntGELU    | n:  23
IntSoftmax | n:  7
IntGELU    | n:  23
.calib done
Test: [  0/391]	Time  3.812 ( 3.812)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.82
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=7, intgelu_exp_n=24
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=7, intgelu_exp_n=24, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  7
IntGELU    | n:  24
IntSoftmax | n:  7
IntGELU    | n:  24
IntSoftmax | n:  7
IntGELU    | n:  24
IntSoftmax | n:  7
IntGELU    | n:  24
IntSoftmax | n:  7
IntGELU    | n:  24
IntSoftmax | n:  7
IntGELU    | n:  24
IntSoftmax | n:  7
IntGELU    | n:  24
IntSoftmax | n:  7
IntGELU    | n:  24
IntSoftmax | n:  7
IntGELU    | n:  24
IntSoftmax | n:  7
IntGELU    | n:  24
IntSoftmax | n:  7
IntGELU    | n:  24
IntSoftmax | n:  7
IntGELU    | n:  24
.calib done
Test: [  0/391]	Time  3.691 ( 3.691)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.70
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=7, intgelu_exp_n=25
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=7, intgelu_exp_n=25, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  7
IntGELU    | n:  25
IntSoftmax | n:  7
IntGELU    | n:  25
IntSoftmax | n:  7
IntGELU    | n:  25
IntSoftmax | n:  7
IntGELU    | n:  25
IntSoftmax | n:  7
IntGELU    | n:  25
IntSoftmax | n:  7
IntGELU    | n:  25
IntSoftmax | n:  7
IntGELU    | n:  25
IntSoftmax | n:  7
IntGELU    | n:  25
IntSoftmax | n:  7
IntGELU    | n:  25
IntSoftmax | n:  7
IntGELU    | n:  25
IntSoftmax | n:  7
IntGELU    | n:  25
IntSoftmax | n:  7
IntGELU    | n:  25
.calib done
Test: [  0/391]	Time  4.004 ( 4.004)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.04
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=7, intgelu_exp_n=26
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=7, intgelu_exp_n=26, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  7
IntGELU    | n:  26
IntSoftmax | n:  7
IntGELU    | n:  26
IntSoftmax | n:  7
IntGELU    | n:  26
IntSoftmax | n:  7
IntGELU    | n:  26
IntSoftmax | n:  7
IntGELU    | n:  26
IntSoftmax | n:  7
IntGELU    | n:  26
IntSoftmax | n:  7
IntGELU    | n:  26
IntSoftmax | n:  7
IntGELU    | n:  26
IntSoftmax | n:  7
IntGELU    | n:  26
IntSoftmax | n:  7
IntGELU    | n:  26
IntSoftmax | n:  7
IntGELU    | n:  26
IntSoftmax | n:  7
IntGELU    | n:  26
.calib done
Test: [  0/391]	Time  3.955 ( 3.955)	Acc@1   0.78 (  0.78)	Acc@5   1.56 (  1.56)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.781 Prec@5 1.562
Time: 9.01
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=7, intgelu_exp_n=27
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=7, intgelu_exp_n=27, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  7
IntGELU    | n:  27
IntSoftmax | n:  7
IntGELU    | n:  27
IntSoftmax | n:  7
IntGELU    | n:  27
IntSoftmax | n:  7
IntGELU    | n:  27
IntSoftmax | n:  7
IntGELU    | n:  27
IntSoftmax | n:  7
IntGELU    | n:  27
IntSoftmax | n:  7
IntGELU    | n:  27
IntSoftmax | n:  7
IntGELU    | n:  27
IntSoftmax | n:  7
IntGELU    | n:  27
IntSoftmax | n:  7
IntGELU    | n:  27
IntSoftmax | n:  7
IntGELU    | n:  27
IntSoftmax | n:  7
IntGELU    | n:  27
.calib done
Test: [  0/391]	Time  3.852 ( 3.852)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.94
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=7, intgelu_exp_n=28
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=7, intgelu_exp_n=28, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  7
IntGELU    | n:  28
IntSoftmax | n:  7
IntGELU    | n:  28
IntSoftmax | n:  7
IntGELU    | n:  28
IntSoftmax | n:  7
IntGELU    | n:  28
IntSoftmax | n:  7
IntGELU    | n:  28
IntSoftmax | n:  7
IntGELU    | n:  28
IntSoftmax | n:  7
IntGELU    | n:  28
IntSoftmax | n:  7
IntGELU    | n:  28
IntSoftmax | n:  7
IntGELU    | n:  28
IntSoftmax | n:  7
IntGELU    | n:  28
IntSoftmax | n:  7
IntGELU    | n:  28
IntSoftmax | n:  7
IntGELU    | n:  28
.calib done
Test: [  0/391]	Time  3.870 ( 3.870)	Acc@1   1.56 (  1.56)	Acc@5   3.12 (  3.12)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 1.562 Prec@5 3.125
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=7, intgelu_exp_n=29
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=7, intgelu_exp_n=29, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  7
IntGELU    | n:  29
IntSoftmax | n:  7
IntGELU    | n:  29
IntSoftmax | n:  7
IntGELU    | n:  29
IntSoftmax | n:  7
IntGELU    | n:  29
IntSoftmax | n:  7
IntGELU    | n:  29
IntSoftmax | n:  7
IntGELU    | n:  29
IntSoftmax | n:  7
IntGELU    | n:  29
IntSoftmax | n:  7
IntGELU    | n:  29
IntSoftmax | n:  7
IntGELU    | n:  29
IntSoftmax | n:  7
IntGELU    | n:  29
IntSoftmax | n:  7
IntGELU    | n:  29
IntSoftmax | n:  7
IntGELU    | n:  29
.calib done
Test: [  0/391]	Time  4.015 ( 4.015)	Acc@1   4.69 (  4.69)	Acc@5  12.50 ( 12.50)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 4.688 Prec@5 12.500
Time: 9.09
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=7, intgelu_exp_n=30
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=7, intgelu_exp_n=30, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  7
IntGELU    | n:  30
IntSoftmax | n:  7
IntGELU    | n:  30
IntSoftmax | n:  7
IntGELU    | n:  30
IntSoftmax | n:  7
IntGELU    | n:  30
IntSoftmax | n:  7
IntGELU    | n:  30
IntSoftmax | n:  7
IntGELU    | n:  30
IntSoftmax | n:  7
IntGELU    | n:  30
IntSoftmax | n:  7
IntGELU    | n:  30
IntSoftmax | n:  7
IntGELU    | n:  30
IntSoftmax | n:  7
IntGELU    | n:  30
IntSoftmax | n:  7
IntGELU    | n:  30
IntSoftmax | n:  7
IntGELU    | n:  30
.calib done
Test: [  0/391]	Time  3.701 ( 3.701)	Acc@1   3.91 (  3.91)	Acc@5  12.50 ( 12.50)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 3.906 Prec@5 12.500
Time: 8.78
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=7, intgelu_exp_n=31
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=7, intgelu_exp_n=31, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  7
IntGELU    | n:  31
IntSoftmax | n:  7
IntGELU    | n:  31
IntSoftmax | n:  7
IntGELU    | n:  31
IntSoftmax | n:  7
IntGELU    | n:  31
IntSoftmax | n:  7
IntGELU    | n:  31
IntSoftmax | n:  7
IntGELU    | n:  31
IntSoftmax | n:  7
IntGELU    | n:  31
IntSoftmax | n:  7
IntGELU    | n:  31
IntSoftmax | n:  7
IntGELU    | n:  31
IntSoftmax | n:  7
IntGELU    | n:  31
IntSoftmax | n:  7
IntGELU    | n:  31
IntSoftmax | n:  7
IntGELU    | n:  31
.calib done
Test: [  0/391]	Time  3.980 ( 3.980)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.07
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=8, intgelu_exp_n=0
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=8, intgelu_exp_n=0, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  8
IntGELU    | n:  0
IntSoftmax | n:  8
IntGELU    | n:  0
IntSoftmax | n:  8
IntGELU    | n:  0
IntSoftmax | n:  8
IntGELU    | n:  0
IntSoftmax | n:  8
IntGELU    | n:  0
IntSoftmax | n:  8
IntGELU    | n:  0
IntSoftmax | n:  8
IntGELU    | n:  0
IntSoftmax | n:  8
IntGELU    | n:  0
IntSoftmax | n:  8
IntGELU    | n:  0
IntSoftmax | n:  8
IntGELU    | n:  0
IntSoftmax | n:  8
IntGELU    | n:  0
IntSoftmax | n:  8
IntGELU    | n:  0
.calib done
Test: [  0/391]	Time  3.968 ( 3.968)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.05
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=8, intgelu_exp_n=1
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=8, intgelu_exp_n=1, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  8
IntGELU    | n:  1
IntSoftmax | n:  8
IntGELU    | n:  1
IntSoftmax | n:  8
IntGELU    | n:  1
IntSoftmax | n:  8
IntGELU    | n:  1
IntSoftmax | n:  8
IntGELU    | n:  1
IntSoftmax | n:  8
IntGELU    | n:  1
IntSoftmax | n:  8
IntGELU    | n:  1
IntSoftmax | n:  8
IntGELU    | n:  1
IntSoftmax | n:  8
IntGELU    | n:  1
IntSoftmax | n:  8
IntGELU    | n:  1
IntSoftmax | n:  8
IntGELU    | n:  1
IntSoftmax | n:  8
IntGELU    | n:  1
.calib done
Test: [  0/391]	Time  3.579 ( 3.579)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.61
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=8, intgelu_exp_n=2
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=8, intgelu_exp_n=2, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  8
IntGELU    | n:  2
IntSoftmax | n:  8
IntGELU    | n:  2
IntSoftmax | n:  8
IntGELU    | n:  2
IntSoftmax | n:  8
IntGELU    | n:  2
IntSoftmax | n:  8
IntGELU    | n:  2
IntSoftmax | n:  8
IntGELU    | n:  2
IntSoftmax | n:  8
IntGELU    | n:  2
IntSoftmax | n:  8
IntGELU    | n:  2
IntSoftmax | n:  8
IntGELU    | n:  2
IntSoftmax | n:  8
IntGELU    | n:  2
IntSoftmax | n:  8
IntGELU    | n:  2
IntSoftmax | n:  8
IntGELU    | n:  2
.calib done
Test: [  0/391]	Time  3.879 ( 3.879)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.91
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=8, intgelu_exp_n=3
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=8, intgelu_exp_n=3, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  8
IntGELU    | n:  3
IntSoftmax | n:  8
IntGELU    | n:  3
IntSoftmax | n:  8
IntGELU    | n:  3
IntSoftmax | n:  8
IntGELU    | n:  3
IntSoftmax | n:  8
IntGELU    | n:  3
IntSoftmax | n:  8
IntGELU    | n:  3
IntSoftmax | n:  8
IntGELU    | n:  3
IntSoftmax | n:  8
IntGELU    | n:  3
IntSoftmax | n:  8
IntGELU    | n:  3
IntSoftmax | n:  8
IntGELU    | n:  3
IntSoftmax | n:  8
IntGELU    | n:  3
IntSoftmax | n:  8
IntGELU    | n:  3
.calib done
Test: [  0/391]	Time  3.941 ( 3.941)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.95
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=8, intgelu_exp_n=4
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=8, intgelu_exp_n=4, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  8
IntGELU    | n:  4
IntSoftmax | n:  8
IntGELU    | n:  4
IntSoftmax | n:  8
IntGELU    | n:  4
IntSoftmax | n:  8
IntGELU    | n:  4
IntSoftmax | n:  8
IntGELU    | n:  4
IntSoftmax | n:  8
IntGELU    | n:  4
IntSoftmax | n:  8
IntGELU    | n:  4
IntSoftmax | n:  8
IntGELU    | n:  4
IntSoftmax | n:  8
IntGELU    | n:  4
IntSoftmax | n:  8
IntGELU    | n:  4
IntSoftmax | n:  8
IntGELU    | n:  4
IntSoftmax | n:  8
IntGELU    | n:  4
.calib done
Test: [  0/391]	Time  3.998 ( 3.998)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.02
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=8, intgelu_exp_n=5
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=8, intgelu_exp_n=5, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  8
IntGELU    | n:  5
IntSoftmax | n:  8
IntGELU    | n:  5
IntSoftmax | n:  8
IntGELU    | n:  5
IntSoftmax | n:  8
IntGELU    | n:  5
IntSoftmax | n:  8
IntGELU    | n:  5
IntSoftmax | n:  8
IntGELU    | n:  5
IntSoftmax | n:  8
IntGELU    | n:  5
IntSoftmax | n:  8
IntGELU    | n:  5
IntSoftmax | n:  8
IntGELU    | n:  5
IntSoftmax | n:  8
IntGELU    | n:  5
IntSoftmax | n:  8
IntGELU    | n:  5
IntSoftmax | n:  8
IntGELU    | n:  5
.calib done
Test: [  0/391]	Time  3.817 ( 3.817)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.88
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=8, intgelu_exp_n=6
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=8, intgelu_exp_n=6, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  8
IntGELU    | n:  6
IntSoftmax | n:  8
IntGELU    | n:  6
IntSoftmax | n:  8
IntGELU    | n:  6
IntSoftmax | n:  8
IntGELU    | n:  6
IntSoftmax | n:  8
IntGELU    | n:  6
IntSoftmax | n:  8
IntGELU    | n:  6
IntSoftmax | n:  8
IntGELU    | n:  6
IntSoftmax | n:  8
IntGELU    | n:  6
IntSoftmax | n:  8
IntGELU    | n:  6
IntSoftmax | n:  8
IntGELU    | n:  6
IntSoftmax | n:  8
IntGELU    | n:  6
IntSoftmax | n:  8
IntGELU    | n:  6
.calib done
Test: [  0/391]	Time  3.917 ( 3.917)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.07
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=8, intgelu_exp_n=7
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=8, intgelu_exp_n=7, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  8
IntGELU    | n:  7
IntSoftmax | n:  8
IntGELU    | n:  7
IntSoftmax | n:  8
IntGELU    | n:  7
IntSoftmax | n:  8
IntGELU    | n:  7
IntSoftmax | n:  8
IntGELU    | n:  7
IntSoftmax | n:  8
IntGELU    | n:  7
IntSoftmax | n:  8
IntGELU    | n:  7
IntSoftmax | n:  8
IntGELU    | n:  7
IntSoftmax | n:  8
IntGELU    | n:  7
IntSoftmax | n:  8
IntGELU    | n:  7
IntSoftmax | n:  8
IntGELU    | n:  7
IntSoftmax | n:  8
IntGELU    | n:  7
.calib done
Test: [  0/391]	Time  4.017 ( 4.017)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.07
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=8, intgelu_exp_n=8
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=8, intgelu_exp_n=8, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  8
IntGELU    | n:  8
IntSoftmax | n:  8
IntGELU    | n:  8
IntSoftmax | n:  8
IntGELU    | n:  8
IntSoftmax | n:  8
IntGELU    | n:  8
IntSoftmax | n:  8
IntGELU    | n:  8
IntSoftmax | n:  8
IntGELU    | n:  8
IntSoftmax | n:  8
IntGELU    | n:  8
IntSoftmax | n:  8
IntGELU    | n:  8
IntSoftmax | n:  8
IntGELU    | n:  8
IntSoftmax | n:  8
IntGELU    | n:  8
IntSoftmax | n:  8
IntGELU    | n:  8
IntSoftmax | n:  8
IntGELU    | n:  8
.calib done
Test: [  0/391]	Time  3.850 ( 3.850)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.88
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=8, intgelu_exp_n=9
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=8, intgelu_exp_n=9, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  8
IntGELU    | n:  9
IntSoftmax | n:  8
IntGELU    | n:  9
IntSoftmax | n:  8
IntGELU    | n:  9
IntSoftmax | n:  8
IntGELU    | n:  9
IntSoftmax | n:  8
IntGELU    | n:  9
IntSoftmax | n:  8
IntGELU    | n:  9
IntSoftmax | n:  8
IntGELU    | n:  9
IntSoftmax | n:  8
IntGELU    | n:  9
IntSoftmax | n:  8
IntGELU    | n:  9
IntSoftmax | n:  8
IntGELU    | n:  9
IntSoftmax | n:  8
IntGELU    | n:  9
IntSoftmax | n:  8
IntGELU    | n:  9
.calib done
Test: [  0/391]	Time  3.901 ( 3.901)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.98
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=8, intgelu_exp_n=10
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=8, intgelu_exp_n=10, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  8
IntGELU    | n:  10
IntSoftmax | n:  8
IntGELU    | n:  10
IntSoftmax | n:  8
IntGELU    | n:  10
IntSoftmax | n:  8
IntGELU    | n:  10
IntSoftmax | n:  8
IntGELU    | n:  10
IntSoftmax | n:  8
IntGELU    | n:  10
IntSoftmax | n:  8
IntGELU    | n:  10
IntSoftmax | n:  8
IntGELU    | n:  10
IntSoftmax | n:  8
IntGELU    | n:  10
IntSoftmax | n:  8
IntGELU    | n:  10
IntSoftmax | n:  8
IntGELU    | n:  10
IntSoftmax | n:  8
IntGELU    | n:  10
.calib done
Test: [  0/391]	Time  3.750 ( 3.750)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.82
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=8, intgelu_exp_n=11
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=8, intgelu_exp_n=11, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  8
IntGELU    | n:  11
IntSoftmax | n:  8
IntGELU    | n:  11
IntSoftmax | n:  8
IntGELU    | n:  11
IntSoftmax | n:  8
IntGELU    | n:  11
IntSoftmax | n:  8
IntGELU    | n:  11
IntSoftmax | n:  8
IntGELU    | n:  11
IntSoftmax | n:  8
IntGELU    | n:  11
IntSoftmax | n:  8
IntGELU    | n:  11
IntSoftmax | n:  8
IntGELU    | n:  11
IntSoftmax | n:  8
IntGELU    | n:  11
IntSoftmax | n:  8
IntGELU    | n:  11
IntSoftmax | n:  8
IntGELU    | n:  11
.calib done
Test: [  0/391]	Time  3.999 ( 3.999)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.09
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=8, intgelu_exp_n=12
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=8, intgelu_exp_n=12, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  8
IntGELU    | n:  12
IntSoftmax | n:  8
IntGELU    | n:  12
IntSoftmax | n:  8
IntGELU    | n:  12
IntSoftmax | n:  8
IntGELU    | n:  12
IntSoftmax | n:  8
IntGELU    | n:  12
IntSoftmax | n:  8
IntGELU    | n:  12
IntSoftmax | n:  8
IntGELU    | n:  12
IntSoftmax | n:  8
IntGELU    | n:  12
IntSoftmax | n:  8
IntGELU    | n:  12
IntSoftmax | n:  8
IntGELU    | n:  12
IntSoftmax | n:  8
IntGELU    | n:  12
IntSoftmax | n:  8
IntGELU    | n:  12
.calib done
Test: [  0/391]	Time  4.102 ( 4.102)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.16
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=8, intgelu_exp_n=13
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=8, intgelu_exp_n=13, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  8
IntGELU    | n:  13
IntSoftmax | n:  8
IntGELU    | n:  13
IntSoftmax | n:  8
IntGELU    | n:  13
IntSoftmax | n:  8
IntGELU    | n:  13
IntSoftmax | n:  8
IntGELU    | n:  13
IntSoftmax | n:  8
IntGELU    | n:  13
IntSoftmax | n:  8
IntGELU    | n:  13
IntSoftmax | n:  8
IntGELU    | n:  13
IntSoftmax | n:  8
IntGELU    | n:  13
IntSoftmax | n:  8
IntGELU    | n:  13
IntSoftmax | n:  8
IntGELU    | n:  13
IntSoftmax | n:  8
IntGELU    | n:  13
.calib done
Test: [  0/391]	Time  4.033 ( 4.033)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.08
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=8, intgelu_exp_n=14
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=8, intgelu_exp_n=14, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  8
IntGELU    | n:  14
IntSoftmax | n:  8
IntGELU    | n:  14
IntSoftmax | n:  8
IntGELU    | n:  14
IntSoftmax | n:  8
IntGELU    | n:  14
IntSoftmax | n:  8
IntGELU    | n:  14
IntSoftmax | n:  8
IntGELU    | n:  14
IntSoftmax | n:  8
IntGELU    | n:  14
IntSoftmax | n:  8
IntGELU    | n:  14
IntSoftmax | n:  8
IntGELU    | n:  14
IntSoftmax | n:  8
IntGELU    | n:  14
IntSoftmax | n:  8
IntGELU    | n:  14
IntSoftmax | n:  8
IntGELU    | n:  14
.calib done
Test: [  0/391]	Time  3.673 ( 3.673)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.70
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=8, intgelu_exp_n=15
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=8, intgelu_exp_n=15, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  8
IntGELU    | n:  15
IntSoftmax | n:  8
IntGELU    | n:  15
IntSoftmax | n:  8
IntGELU    | n:  15
IntSoftmax | n:  8
IntGELU    | n:  15
IntSoftmax | n:  8
IntGELU    | n:  15
IntSoftmax | n:  8
IntGELU    | n:  15
IntSoftmax | n:  8
IntGELU    | n:  15
IntSoftmax | n:  8
IntGELU    | n:  15
IntSoftmax | n:  8
IntGELU    | n:  15
IntSoftmax | n:  8
IntGELU    | n:  15
IntSoftmax | n:  8
IntGELU    | n:  15
IntSoftmax | n:  8
IntGELU    | n:  15
.calib done
Test: [  0/391]	Time  3.856 ( 3.856)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.91
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=8, intgelu_exp_n=16
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=8, intgelu_exp_n=16, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  8
IntGELU    | n:  16
IntSoftmax | n:  8
IntGELU    | n:  16
IntSoftmax | n:  8
IntGELU    | n:  16
IntSoftmax | n:  8
IntGELU    | n:  16
IntSoftmax | n:  8
IntGELU    | n:  16
IntSoftmax | n:  8
IntGELU    | n:  16
IntSoftmax | n:  8
IntGELU    | n:  16
IntSoftmax | n:  8
IntGELU    | n:  16
IntSoftmax | n:  8
IntGELU    | n:  16
IntSoftmax | n:  8
IntGELU    | n:  16
IntSoftmax | n:  8
IntGELU    | n:  16
IntSoftmax | n:  8
IntGELU    | n:  16
.calib done
Test: [  0/391]	Time  4.003 ( 4.003)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.06
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=8, intgelu_exp_n=17
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=8, intgelu_exp_n=17, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  8
IntGELU    | n:  17
IntSoftmax | n:  8
IntGELU    | n:  17
IntSoftmax | n:  8
IntGELU    | n:  17
IntSoftmax | n:  8
IntGELU    | n:  17
IntSoftmax | n:  8
IntGELU    | n:  17
IntSoftmax | n:  8
IntGELU    | n:  17
IntSoftmax | n:  8
IntGELU    | n:  17
IntSoftmax | n:  8
IntGELU    | n:  17
IntSoftmax | n:  8
IntGELU    | n:  17
IntSoftmax | n:  8
IntGELU    | n:  17
IntSoftmax | n:  8
IntGELU    | n:  17
IntSoftmax | n:  8
IntGELU    | n:  17
.calib done
Test: [  0/391]	Time  3.769 ( 3.769)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.84
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=8, intgelu_exp_n=18
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=8, intgelu_exp_n=18, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  8
IntGELU    | n:  18
IntSoftmax | n:  8
IntGELU    | n:  18
IntSoftmax | n:  8
IntGELU    | n:  18
IntSoftmax | n:  8
IntGELU    | n:  18
IntSoftmax | n:  8
IntGELU    | n:  18
IntSoftmax | n:  8
IntGELU    | n:  18
IntSoftmax | n:  8
IntGELU    | n:  18
IntSoftmax | n:  8
IntGELU    | n:  18
IntSoftmax | n:  8
IntGELU    | n:  18
IntSoftmax | n:  8
IntGELU    | n:  18
IntSoftmax | n:  8
IntGELU    | n:  18
IntSoftmax | n:  8
IntGELU    | n:  18
.calib done
Test: [  0/391]	Time  3.804 ( 3.804)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.81
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=8, intgelu_exp_n=19
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=8, intgelu_exp_n=19, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  8
IntGELU    | n:  19
IntSoftmax | n:  8
IntGELU    | n:  19
IntSoftmax | n:  8
IntGELU    | n:  19
IntSoftmax | n:  8
IntGELU    | n:  19
IntSoftmax | n:  8
IntGELU    | n:  19
IntSoftmax | n:  8
IntGELU    | n:  19
IntSoftmax | n:  8
IntGELU    | n:  19
IntSoftmax | n:  8
IntGELU    | n:  19
IntSoftmax | n:  8
IntGELU    | n:  19
IntSoftmax | n:  8
IntGELU    | n:  19
IntSoftmax | n:  8
IntGELU    | n:  19
IntSoftmax | n:  8
IntGELU    | n:  19
.calib done
Test: [  0/391]	Time  3.904 ( 3.904)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.94
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=8, intgelu_exp_n=20
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=8, intgelu_exp_n=20, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  8
IntGELU    | n:  20
IntSoftmax | n:  8
IntGELU    | n:  20
IntSoftmax | n:  8
IntGELU    | n:  20
IntSoftmax | n:  8
IntGELU    | n:  20
IntSoftmax | n:  8
IntGELU    | n:  20
IntSoftmax | n:  8
IntGELU    | n:  20
IntSoftmax | n:  8
IntGELU    | n:  20
IntSoftmax | n:  8
IntGELU    | n:  20
IntSoftmax | n:  8
IntGELU    | n:  20
IntSoftmax | n:  8
IntGELU    | n:  20
IntSoftmax | n:  8
IntGELU    | n:  20
IntSoftmax | n:  8
IntGELU    | n:  20
.calib done
Test: [  0/391]	Time  3.829 ( 3.829)	Acc@1   0.00 (  0.00)	Acc@5   1.56 (  1.56)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 1.562
Time: 8.91
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=8, intgelu_exp_n=21
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=8, intgelu_exp_n=21, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  8
IntGELU    | n:  21
IntSoftmax | n:  8
IntGELU    | n:  21
IntSoftmax | n:  8
IntGELU    | n:  21
IntSoftmax | n:  8
IntGELU    | n:  21
IntSoftmax | n:  8
IntGELU    | n:  21
IntSoftmax | n:  8
IntGELU    | n:  21
IntSoftmax | n:  8
IntGELU    | n:  21
IntSoftmax | n:  8
IntGELU    | n:  21
IntSoftmax | n:  8
IntGELU    | n:  21
IntSoftmax | n:  8
IntGELU    | n:  21
IntSoftmax | n:  8
IntGELU    | n:  21
IntSoftmax | n:  8
IntGELU    | n:  21
.calib done
Test: [  0/391]	Time  3.964 ( 3.964)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.00
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=8, intgelu_exp_n=22
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=8, intgelu_exp_n=22, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  8
IntGELU    | n:  22
IntSoftmax | n:  8
IntGELU    | n:  22
IntSoftmax | n:  8
IntGELU    | n:  22
IntSoftmax | n:  8
IntGELU    | n:  22
IntSoftmax | n:  8
IntGELU    | n:  22
IntSoftmax | n:  8
IntGELU    | n:  22
IntSoftmax | n:  8
IntGELU    | n:  22
IntSoftmax | n:  8
IntGELU    | n:  22
IntSoftmax | n:  8
IntGELU    | n:  22
IntSoftmax | n:  8
IntGELU    | n:  22
IntSoftmax | n:  8
IntGELU    | n:  22
IntSoftmax | n:  8
IntGELU    | n:  22
.calib done
Test: [  0/391]	Time  3.929 ( 3.929)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=8, intgelu_exp_n=23
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=8, intgelu_exp_n=23, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  8
IntGELU    | n:  23
IntSoftmax | n:  8
IntGELU    | n:  23
IntSoftmax | n:  8
IntGELU    | n:  23
IntSoftmax | n:  8
IntGELU    | n:  23
IntSoftmax | n:  8
IntGELU    | n:  23
IntSoftmax | n:  8
IntGELU    | n:  23
IntSoftmax | n:  8
IntGELU    | n:  23
IntSoftmax | n:  8
IntGELU    | n:  23
IntSoftmax | n:  8
IntGELU    | n:  23
IntSoftmax | n:  8
IntGELU    | n:  23
IntSoftmax | n:  8
IntGELU    | n:  23
IntSoftmax | n:  8
IntGELU    | n:  23
.calib done
Test: [  0/391]	Time  3.976 ( 3.976)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.03
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=8, intgelu_exp_n=24
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=8, intgelu_exp_n=24, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  8
IntGELU    | n:  24
IntSoftmax | n:  8
IntGELU    | n:  24
IntSoftmax | n:  8
IntGELU    | n:  24
IntSoftmax | n:  8
IntGELU    | n:  24
IntSoftmax | n:  8
IntGELU    | n:  24
IntSoftmax | n:  8
IntGELU    | n:  24
IntSoftmax | n:  8
IntGELU    | n:  24
IntSoftmax | n:  8
IntGELU    | n:  24
IntSoftmax | n:  8
IntGELU    | n:  24
IntSoftmax | n:  8
IntGELU    | n:  24
IntSoftmax | n:  8
IntGELU    | n:  24
IntSoftmax | n:  8
IntGELU    | n:  24
.calib done
Test: [  0/391]	Time  3.862 ( 3.862)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.88
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=8, intgelu_exp_n=25
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=8, intgelu_exp_n=25, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  8
IntGELU    | n:  25
IntSoftmax | n:  8
IntGELU    | n:  25
IntSoftmax | n:  8
IntGELU    | n:  25
IntSoftmax | n:  8
IntGELU    | n:  25
IntSoftmax | n:  8
IntGELU    | n:  25
IntSoftmax | n:  8
IntGELU    | n:  25
IntSoftmax | n:  8
IntGELU    | n:  25
IntSoftmax | n:  8
IntGELU    | n:  25
IntSoftmax | n:  8
IntGELU    | n:  25
IntSoftmax | n:  8
IntGELU    | n:  25
IntSoftmax | n:  8
IntGELU    | n:  25
IntSoftmax | n:  8
IntGELU    | n:  25
.calib done
Test: [  0/391]	Time  3.823 ( 3.823)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.88
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=8, intgelu_exp_n=26
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=8, intgelu_exp_n=26, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  8
IntGELU    | n:  26
IntSoftmax | n:  8
IntGELU    | n:  26
IntSoftmax | n:  8
IntGELU    | n:  26
IntSoftmax | n:  8
IntGELU    | n:  26
IntSoftmax | n:  8
IntGELU    | n:  26
IntSoftmax | n:  8
IntGELU    | n:  26
IntSoftmax | n:  8
IntGELU    | n:  26
IntSoftmax | n:  8
IntGELU    | n:  26
IntSoftmax | n:  8
IntGELU    | n:  26
IntSoftmax | n:  8
IntGELU    | n:  26
IntSoftmax | n:  8
IntGELU    | n:  26
IntSoftmax | n:  8
IntGELU    | n:  26
.calib done
Test: [  0/391]	Time  3.869 ( 3.869)	Acc@1   0.00 (  0.00)	Acc@5   0.78 (  0.78)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.781
Time: 8.85
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=8, intgelu_exp_n=27
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=8, intgelu_exp_n=27, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  8
IntGELU    | n:  27
IntSoftmax | n:  8
IntGELU    | n:  27
IntSoftmax | n:  8
IntGELU    | n:  27
IntSoftmax | n:  8
IntGELU    | n:  27
IntSoftmax | n:  8
IntGELU    | n:  27
IntSoftmax | n:  8
IntGELU    | n:  27
IntSoftmax | n:  8
IntGELU    | n:  27
IntSoftmax | n:  8
IntGELU    | n:  27
IntSoftmax | n:  8
IntGELU    | n:  27
IntSoftmax | n:  8
IntGELU    | n:  27
IntSoftmax | n:  8
IntGELU    | n:  27
IntSoftmax | n:  8
IntGELU    | n:  27
.calib done
Test: [  0/391]	Time  3.792 ( 3.792)	Acc@1   0.00 (  0.00)	Acc@5   1.56 (  1.56)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 1.562
Time: 8.87
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=8, intgelu_exp_n=28
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=8, intgelu_exp_n=28, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  8
IntGELU    | n:  28
IntSoftmax | n:  8
IntGELU    | n:  28
IntSoftmax | n:  8
IntGELU    | n:  28
IntSoftmax | n:  8
IntGELU    | n:  28
IntSoftmax | n:  8
IntGELU    | n:  28
IntSoftmax | n:  8
IntGELU    | n:  28
IntSoftmax | n:  8
IntGELU    | n:  28
IntSoftmax | n:  8
IntGELU    | n:  28
IntSoftmax | n:  8
IntGELU    | n:  28
IntSoftmax | n:  8
IntGELU    | n:  28
IntSoftmax | n:  8
IntGELU    | n:  28
IntSoftmax | n:  8
IntGELU    | n:  28
.calib done
Test: [  0/391]	Time  3.972 ( 3.972)	Acc@1   0.00 (  0.00)	Acc@5   1.56 (  1.56)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 1.562
Time: 9.07
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=8, intgelu_exp_n=29
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=8, intgelu_exp_n=29, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  8
IntGELU    | n:  29
IntSoftmax | n:  8
IntGELU    | n:  29
IntSoftmax | n:  8
IntGELU    | n:  29
IntSoftmax | n:  8
IntGELU    | n:  29
IntSoftmax | n:  8
IntGELU    | n:  29
IntSoftmax | n:  8
IntGELU    | n:  29
IntSoftmax | n:  8
IntGELU    | n:  29
IntSoftmax | n:  8
IntGELU    | n:  29
IntSoftmax | n:  8
IntGELU    | n:  29
IntSoftmax | n:  8
IntGELU    | n:  29
IntSoftmax | n:  8
IntGELU    | n:  29
IntSoftmax | n:  8
IntGELU    | n:  29
.calib done
Test: [  0/391]	Time  4.080 ( 4.080)	Acc@1   0.78 (  0.78)	Acc@5   3.91 (  3.91)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.781 Prec@5 3.906
Time: 9.17
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=8, intgelu_exp_n=30
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=8, intgelu_exp_n=30, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  8
IntGELU    | n:  30
IntSoftmax | n:  8
IntGELU    | n:  30
IntSoftmax | n:  8
IntGELU    | n:  30
IntSoftmax | n:  8
IntGELU    | n:  30
IntSoftmax | n:  8
IntGELU    | n:  30
IntSoftmax | n:  8
IntGELU    | n:  30
IntSoftmax | n:  8
IntGELU    | n:  30
IntSoftmax | n:  8
IntGELU    | n:  30
IntSoftmax | n:  8
IntGELU    | n:  30
IntSoftmax | n:  8
IntGELU    | n:  30
IntSoftmax | n:  8
IntGELU    | n:  30
IntSoftmax | n:  8
IntGELU    | n:  30
.calib done
Test: [  0/391]	Time  3.692 ( 3.692)	Acc@1   7.03 (  7.03)	Acc@5  21.88 ( 21.88)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 7.031 Prec@5 21.875
Time: 8.69
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=8, intgelu_exp_n=31
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=8, intgelu_exp_n=31, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  8
IntGELU    | n:  31
IntSoftmax | n:  8
IntGELU    | n:  31
IntSoftmax | n:  8
IntGELU    | n:  31
IntSoftmax | n:  8
IntGELU    | n:  31
IntSoftmax | n:  8
IntGELU    | n:  31
IntSoftmax | n:  8
IntGELU    | n:  31
IntSoftmax | n:  8
IntGELU    | n:  31
IntSoftmax | n:  8
IntGELU    | n:  31
IntSoftmax | n:  8
IntGELU    | n:  31
IntSoftmax | n:  8
IntGELU    | n:  31
IntSoftmax | n:  8
IntGELU    | n:  31
IntSoftmax | n:  8
IntGELU    | n:  31
.calib done
Test: [  0/391]	Time  3.912 ( 3.912)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.00
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=9, intgelu_exp_n=0
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=9, intgelu_exp_n=0, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  9
IntGELU    | n:  0
IntSoftmax | n:  9
IntGELU    | n:  0
IntSoftmax | n:  9
IntGELU    | n:  0
IntSoftmax | n:  9
IntGELU    | n:  0
IntSoftmax | n:  9
IntGELU    | n:  0
IntSoftmax | n:  9
IntGELU    | n:  0
IntSoftmax | n:  9
IntGELU    | n:  0
IntSoftmax | n:  9
IntGELU    | n:  0
IntSoftmax | n:  9
IntGELU    | n:  0
IntSoftmax | n:  9
IntGELU    | n:  0
IntSoftmax | n:  9
IntGELU    | n:  0
IntSoftmax | n:  9
IntGELU    | n:  0
.calib done
Test: [  0/391]	Time  3.766 ( 3.766)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.77
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=9, intgelu_exp_n=1
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=9, intgelu_exp_n=1, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  9
IntGELU    | n:  1
IntSoftmax | n:  9
IntGELU    | n:  1
IntSoftmax | n:  9
IntGELU    | n:  1
IntSoftmax | n:  9
IntGELU    | n:  1
IntSoftmax | n:  9
IntGELU    | n:  1
IntSoftmax | n:  9
IntGELU    | n:  1
IntSoftmax | n:  9
IntGELU    | n:  1
IntSoftmax | n:  9
IntGELU    | n:  1
IntSoftmax | n:  9
IntGELU    | n:  1
IntSoftmax | n:  9
IntGELU    | n:  1
IntSoftmax | n:  9
IntGELU    | n:  1
IntSoftmax | n:  9
IntGELU    | n:  1
.calib done
Test: [  0/391]	Time  3.726 ( 3.726)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.71
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=9, intgelu_exp_n=2
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=9, intgelu_exp_n=2, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  9
IntGELU    | n:  2
IntSoftmax | n:  9
IntGELU    | n:  2
IntSoftmax | n:  9
IntGELU    | n:  2
IntSoftmax | n:  9
IntGELU    | n:  2
IntSoftmax | n:  9
IntGELU    | n:  2
IntSoftmax | n:  9
IntGELU    | n:  2
IntSoftmax | n:  9
IntGELU    | n:  2
IntSoftmax | n:  9
IntGELU    | n:  2
IntSoftmax | n:  9
IntGELU    | n:  2
IntSoftmax | n:  9
IntGELU    | n:  2
IntSoftmax | n:  9
IntGELU    | n:  2
IntSoftmax | n:  9
IntGELU    | n:  2
.calib done
Test: [  0/391]	Time  3.984 ( 3.984)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.04
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=9, intgelu_exp_n=3
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=9, intgelu_exp_n=3, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  9
IntGELU    | n:  3
IntSoftmax | n:  9
IntGELU    | n:  3
IntSoftmax | n:  9
IntGELU    | n:  3
IntSoftmax | n:  9
IntGELU    | n:  3
IntSoftmax | n:  9
IntGELU    | n:  3
IntSoftmax | n:  9
IntGELU    | n:  3
IntSoftmax | n:  9
IntGELU    | n:  3
IntSoftmax | n:  9
IntGELU    | n:  3
IntSoftmax | n:  9
IntGELU    | n:  3
IntSoftmax | n:  9
IntGELU    | n:  3
IntSoftmax | n:  9
IntGELU    | n:  3
IntSoftmax | n:  9
IntGELU    | n:  3
.calib done
Test: [  0/391]	Time  3.733 ( 3.733)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.81
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=9, intgelu_exp_n=4
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=9, intgelu_exp_n=4, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  9
IntGELU    | n:  4
IntSoftmax | n:  9
IntGELU    | n:  4
IntSoftmax | n:  9
IntGELU    | n:  4
IntSoftmax | n:  9
IntGELU    | n:  4
IntSoftmax | n:  9
IntGELU    | n:  4
IntSoftmax | n:  9
IntGELU    | n:  4
IntSoftmax | n:  9
IntGELU    | n:  4
IntSoftmax | n:  9
IntGELU    | n:  4
IntSoftmax | n:  9
IntGELU    | n:  4
IntSoftmax | n:  9
IntGELU    | n:  4
IntSoftmax | n:  9
IntGELU    | n:  4
IntSoftmax | n:  9
IntGELU    | n:  4
.calib done
Test: [  0/391]	Time  3.823 ( 3.823)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.83
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=9, intgelu_exp_n=5
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=9, intgelu_exp_n=5, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  9
IntGELU    | n:  5
IntSoftmax | n:  9
IntGELU    | n:  5
IntSoftmax | n:  9
IntGELU    | n:  5
IntSoftmax | n:  9
IntGELU    | n:  5
IntSoftmax | n:  9
IntGELU    | n:  5
IntSoftmax | n:  9
IntGELU    | n:  5
IntSoftmax | n:  9
IntGELU    | n:  5
IntSoftmax | n:  9
IntGELU    | n:  5
IntSoftmax | n:  9
IntGELU    | n:  5
IntSoftmax | n:  9
IntGELU    | n:  5
IntSoftmax | n:  9
IntGELU    | n:  5
IntSoftmax | n:  9
IntGELU    | n:  5
.calib done
Test: [  0/391]	Time  3.909 ( 3.909)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.90
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=9, intgelu_exp_n=6
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=9, intgelu_exp_n=6, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  9
IntGELU    | n:  6
IntSoftmax | n:  9
IntGELU    | n:  6
IntSoftmax | n:  9
IntGELU    | n:  6
IntSoftmax | n:  9
IntGELU    | n:  6
IntSoftmax | n:  9
IntGELU    | n:  6
IntSoftmax | n:  9
IntGELU    | n:  6
IntSoftmax | n:  9
IntGELU    | n:  6
IntSoftmax | n:  9
IntGELU    | n:  6
IntSoftmax | n:  9
IntGELU    | n:  6
IntSoftmax | n:  9
IntGELU    | n:  6
IntSoftmax | n:  9
IntGELU    | n:  6
IntSoftmax | n:  9
IntGELU    | n:  6
.calib done
Test: [  0/391]	Time  3.851 ( 3.851)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.87
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=9, intgelu_exp_n=7
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=9, intgelu_exp_n=7, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  9
IntGELU    | n:  7
IntSoftmax | n:  9
IntGELU    | n:  7
IntSoftmax | n:  9
IntGELU    | n:  7
IntSoftmax | n:  9
IntGELU    | n:  7
IntSoftmax | n:  9
IntGELU    | n:  7
IntSoftmax | n:  9
IntGELU    | n:  7
IntSoftmax | n:  9
IntGELU    | n:  7
IntSoftmax | n:  9
IntGELU    | n:  7
IntSoftmax | n:  9
IntGELU    | n:  7
IntSoftmax | n:  9
IntGELU    | n:  7
IntSoftmax | n:  9
IntGELU    | n:  7
IntSoftmax | n:  9
IntGELU    | n:  7
.calib done
Test: [  0/391]	Time  3.964 ( 3.964)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.00
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=9, intgelu_exp_n=8
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=9, intgelu_exp_n=8, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  9
IntGELU    | n:  8
IntSoftmax | n:  9
IntGELU    | n:  8
IntSoftmax | n:  9
IntGELU    | n:  8
IntSoftmax | n:  9
IntGELU    | n:  8
IntSoftmax | n:  9
IntGELU    | n:  8
IntSoftmax | n:  9
IntGELU    | n:  8
IntSoftmax | n:  9
IntGELU    | n:  8
IntSoftmax | n:  9
IntGELU    | n:  8
IntSoftmax | n:  9
IntGELU    | n:  8
IntSoftmax | n:  9
IntGELU    | n:  8
IntSoftmax | n:  9
IntGELU    | n:  8
IntSoftmax | n:  9
IntGELU    | n:  8
.calib done
Test: [  0/391]	Time  3.887 ( 3.887)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.90
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=9, intgelu_exp_n=9
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=9, intgelu_exp_n=9, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  9
IntGELU    | n:  9
IntSoftmax | n:  9
IntGELU    | n:  9
IntSoftmax | n:  9
IntGELU    | n:  9
IntSoftmax | n:  9
IntGELU    | n:  9
IntSoftmax | n:  9
IntGELU    | n:  9
IntSoftmax | n:  9
IntGELU    | n:  9
IntSoftmax | n:  9
IntGELU    | n:  9
IntSoftmax | n:  9
IntGELU    | n:  9
IntSoftmax | n:  9
IntGELU    | n:  9
IntSoftmax | n:  9
IntGELU    | n:  9
IntSoftmax | n:  9
IntGELU    | n:  9
IntSoftmax | n:  9
IntGELU    | n:  9
.calib done
Test: [  0/391]	Time  3.687 ( 3.687)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.73
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=9, intgelu_exp_n=10
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=9, intgelu_exp_n=10, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  9
IntGELU    | n:  10
IntSoftmax | n:  9
IntGELU    | n:  10
IntSoftmax | n:  9
IntGELU    | n:  10
IntSoftmax | n:  9
IntGELU    | n:  10
IntSoftmax | n:  9
IntGELU    | n:  10
IntSoftmax | n:  9
IntGELU    | n:  10
IntSoftmax | n:  9
IntGELU    | n:  10
IntSoftmax | n:  9
IntGELU    | n:  10
IntSoftmax | n:  9
IntGELU    | n:  10
IntSoftmax | n:  9
IntGELU    | n:  10
IntSoftmax | n:  9
IntGELU    | n:  10
IntSoftmax | n:  9
IntGELU    | n:  10
.calib done
Test: [  0/391]	Time  3.719 ( 3.719)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.75
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=9, intgelu_exp_n=11
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=9, intgelu_exp_n=11, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  9
IntGELU    | n:  11
IntSoftmax | n:  9
IntGELU    | n:  11
IntSoftmax | n:  9
IntGELU    | n:  11
IntSoftmax | n:  9
IntGELU    | n:  11
IntSoftmax | n:  9
IntGELU    | n:  11
IntSoftmax | n:  9
IntGELU    | n:  11
IntSoftmax | n:  9
IntGELU    | n:  11
IntSoftmax | n:  9
IntGELU    | n:  11
IntSoftmax | n:  9
IntGELU    | n:  11
IntSoftmax | n:  9
IntGELU    | n:  11
IntSoftmax | n:  9
IntGELU    | n:  11
IntSoftmax | n:  9
IntGELU    | n:  11
.calib done
Test: [  0/391]	Time  3.824 ( 3.824)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.90
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=9, intgelu_exp_n=12
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=9, intgelu_exp_n=12, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  9
IntGELU    | n:  12
IntSoftmax | n:  9
IntGELU    | n:  12
IntSoftmax | n:  9
IntGELU    | n:  12
IntSoftmax | n:  9
IntGELU    | n:  12
IntSoftmax | n:  9
IntGELU    | n:  12
IntSoftmax | n:  9
IntGELU    | n:  12
IntSoftmax | n:  9
IntGELU    | n:  12
IntSoftmax | n:  9
IntGELU    | n:  12
IntSoftmax | n:  9
IntGELU    | n:  12
IntSoftmax | n:  9
IntGELU    | n:  12
IntSoftmax | n:  9
IntGELU    | n:  12
IntSoftmax | n:  9
IntGELU    | n:  12
.calib done
Test: [  0/391]	Time  3.775 ( 3.775)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.83
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=9, intgelu_exp_n=13
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=9, intgelu_exp_n=13, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  9
IntGELU    | n:  13
IntSoftmax | n:  9
IntGELU    | n:  13
IntSoftmax | n:  9
IntGELU    | n:  13
IntSoftmax | n:  9
IntGELU    | n:  13
IntSoftmax | n:  9
IntGELU    | n:  13
IntSoftmax | n:  9
IntGELU    | n:  13
IntSoftmax | n:  9
IntGELU    | n:  13
IntSoftmax | n:  9
IntGELU    | n:  13
IntSoftmax | n:  9
IntGELU    | n:  13
IntSoftmax | n:  9
IntGELU    | n:  13
IntSoftmax | n:  9
IntGELU    | n:  13
IntSoftmax | n:  9
IntGELU    | n:  13
.calib done
Test: [  0/391]	Time  3.976 ( 3.976)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.00
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=9, intgelu_exp_n=14
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=9, intgelu_exp_n=14, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  9
IntGELU    | n:  14
IntSoftmax | n:  9
IntGELU    | n:  14
IntSoftmax | n:  9
IntGELU    | n:  14
IntSoftmax | n:  9
IntGELU    | n:  14
IntSoftmax | n:  9
IntGELU    | n:  14
IntSoftmax | n:  9
IntGELU    | n:  14
IntSoftmax | n:  9
IntGELU    | n:  14
IntSoftmax | n:  9
IntGELU    | n:  14
IntSoftmax | n:  9
IntGELU    | n:  14
IntSoftmax | n:  9
IntGELU    | n:  14
IntSoftmax | n:  9
IntGELU    | n:  14
IntSoftmax | n:  9
IntGELU    | n:  14
.calib done
Test: [  0/391]	Time  3.843 ( 3.843)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=9, intgelu_exp_n=15
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=9, intgelu_exp_n=15, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  9
IntGELU    | n:  15
IntSoftmax | n:  9
IntGELU    | n:  15
IntSoftmax | n:  9
IntGELU    | n:  15
IntSoftmax | n:  9
IntGELU    | n:  15
IntSoftmax | n:  9
IntGELU    | n:  15
IntSoftmax | n:  9
IntGELU    | n:  15
IntSoftmax | n:  9
IntGELU    | n:  15
IntSoftmax | n:  9
IntGELU    | n:  15
IntSoftmax | n:  9
IntGELU    | n:  15
IntSoftmax | n:  9
IntGELU    | n:  15
IntSoftmax | n:  9
IntGELU    | n:  15
IntSoftmax | n:  9
IntGELU    | n:  15
.calib done
Test: [  0/391]	Time  3.936 ( 3.936)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=9, intgelu_exp_n=16
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=9, intgelu_exp_n=16, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  9
IntGELU    | n:  16
IntSoftmax | n:  9
IntGELU    | n:  16
IntSoftmax | n:  9
IntGELU    | n:  16
IntSoftmax | n:  9
IntGELU    | n:  16
IntSoftmax | n:  9
IntGELU    | n:  16
IntSoftmax | n:  9
IntGELU    | n:  16
IntSoftmax | n:  9
IntGELU    | n:  16
IntSoftmax | n:  9
IntGELU    | n:  16
IntSoftmax | n:  9
IntGELU    | n:  16
IntSoftmax | n:  9
IntGELU    | n:  16
IntSoftmax | n:  9
IntGELU    | n:  16
IntSoftmax | n:  9
IntGELU    | n:  16
.calib done
Test: [  0/391]	Time  3.734 ( 3.734)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.80
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=9, intgelu_exp_n=17
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=9, intgelu_exp_n=17, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  9
IntGELU    | n:  17
IntSoftmax | n:  9
IntGELU    | n:  17
IntSoftmax | n:  9
IntGELU    | n:  17
IntSoftmax | n:  9
IntGELU    | n:  17
IntSoftmax | n:  9
IntGELU    | n:  17
IntSoftmax | n:  9
IntGELU    | n:  17
IntSoftmax | n:  9
IntGELU    | n:  17
IntSoftmax | n:  9
IntGELU    | n:  17
IntSoftmax | n:  9
IntGELU    | n:  17
IntSoftmax | n:  9
IntGELU    | n:  17
IntSoftmax | n:  9
IntGELU    | n:  17
IntSoftmax | n:  9
IntGELU    | n:  17
.calib done
Test: [  0/391]	Time  3.956 ( 3.956)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.04
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=9, intgelu_exp_n=18
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=9, intgelu_exp_n=18, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  9
IntGELU    | n:  18
IntSoftmax | n:  9
IntGELU    | n:  18
IntSoftmax | n:  9
IntGELU    | n:  18
IntSoftmax | n:  9
IntGELU    | n:  18
IntSoftmax | n:  9
IntGELU    | n:  18
IntSoftmax | n:  9
IntGELU    | n:  18
IntSoftmax | n:  9
IntGELU    | n:  18
IntSoftmax | n:  9
IntGELU    | n:  18
IntSoftmax | n:  9
IntGELU    | n:  18
IntSoftmax | n:  9
IntGELU    | n:  18
IntSoftmax | n:  9
IntGELU    | n:  18
IntSoftmax | n:  9
IntGELU    | n:  18
.calib done
Test: [  0/391]	Time  3.939 ( 3.939)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=9, intgelu_exp_n=19
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=9, intgelu_exp_n=19, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  9
IntGELU    | n:  19
IntSoftmax | n:  9
IntGELU    | n:  19
IntSoftmax | n:  9
IntGELU    | n:  19
IntSoftmax | n:  9
IntGELU    | n:  19
IntSoftmax | n:  9
IntGELU    | n:  19
IntSoftmax | n:  9
IntGELU    | n:  19
IntSoftmax | n:  9
IntGELU    | n:  19
IntSoftmax | n:  9
IntGELU    | n:  19
IntSoftmax | n:  9
IntGELU    | n:  19
IntSoftmax | n:  9
IntGELU    | n:  19
IntSoftmax | n:  9
IntGELU    | n:  19
IntSoftmax | n:  9
IntGELU    | n:  19
.calib done
Test: [  0/391]	Time  3.972 ( 3.972)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.00
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=9, intgelu_exp_n=20
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=9, intgelu_exp_n=20, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  9
IntGELU    | n:  20
IntSoftmax | n:  9
IntGELU    | n:  20
IntSoftmax | n:  9
IntGELU    | n:  20
IntSoftmax | n:  9
IntGELU    | n:  20
IntSoftmax | n:  9
IntGELU    | n:  20
IntSoftmax | n:  9
IntGELU    | n:  20
IntSoftmax | n:  9
IntGELU    | n:  20
IntSoftmax | n:  9
IntGELU    | n:  20
IntSoftmax | n:  9
IntGELU    | n:  20
IntSoftmax | n:  9
IntGELU    | n:  20
IntSoftmax | n:  9
IntGELU    | n:  20
IntSoftmax | n:  9
IntGELU    | n:  20
.calib done
Test: [  0/391]	Time  3.977 ( 3.977)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.12
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=9, intgelu_exp_n=21
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=9, intgelu_exp_n=21, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  9
IntGELU    | n:  21
IntSoftmax | n:  9
IntGELU    | n:  21
IntSoftmax | n:  9
IntGELU    | n:  21
IntSoftmax | n:  9
IntGELU    | n:  21
IntSoftmax | n:  9
IntGELU    | n:  21
IntSoftmax | n:  9
IntGELU    | n:  21
IntSoftmax | n:  9
IntGELU    | n:  21
IntSoftmax | n:  9
IntGELU    | n:  21
IntSoftmax | n:  9
IntGELU    | n:  21
IntSoftmax | n:  9
IntGELU    | n:  21
IntSoftmax | n:  9
IntGELU    | n:  21
IntSoftmax | n:  9
IntGELU    | n:  21
.calib done
Test: [  0/391]	Time  3.896 ( 3.896)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.91
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=9, intgelu_exp_n=22
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=9, intgelu_exp_n=22, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  9
IntGELU    | n:  22
IntSoftmax | n:  9
IntGELU    | n:  22
IntSoftmax | n:  9
IntGELU    | n:  22
IntSoftmax | n:  9
IntGELU    | n:  22
IntSoftmax | n:  9
IntGELU    | n:  22
IntSoftmax | n:  9
IntGELU    | n:  22
IntSoftmax | n:  9
IntGELU    | n:  22
IntSoftmax | n:  9
IntGELU    | n:  22
IntSoftmax | n:  9
IntGELU    | n:  22
IntSoftmax | n:  9
IntGELU    | n:  22
IntSoftmax | n:  9
IntGELU    | n:  22
IntSoftmax | n:  9
IntGELU    | n:  22
.calib done
Test: [  0/391]	Time  3.710 ( 3.710)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.81
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=9, intgelu_exp_n=23
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=9, intgelu_exp_n=23, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  9
IntGELU    | n:  23
IntSoftmax | n:  9
IntGELU    | n:  23
IntSoftmax | n:  9
IntGELU    | n:  23
IntSoftmax | n:  9
IntGELU    | n:  23
IntSoftmax | n:  9
IntGELU    | n:  23
IntSoftmax | n:  9
IntGELU    | n:  23
IntSoftmax | n:  9
IntGELU    | n:  23
IntSoftmax | n:  9
IntGELU    | n:  23
IntSoftmax | n:  9
IntGELU    | n:  23
IntSoftmax | n:  9
IntGELU    | n:  23
IntSoftmax | n:  9
IntGELU    | n:  23
IntSoftmax | n:  9
IntGELU    | n:  23
.calib done
Test: [  0/391]	Time  3.895 ( 3.895)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.94
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=9, intgelu_exp_n=24
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=9, intgelu_exp_n=24, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  9
IntGELU    | n:  24
IntSoftmax | n:  9
IntGELU    | n:  24
IntSoftmax | n:  9
IntGELU    | n:  24
IntSoftmax | n:  9
IntGELU    | n:  24
IntSoftmax | n:  9
IntGELU    | n:  24
IntSoftmax | n:  9
IntGELU    | n:  24
IntSoftmax | n:  9
IntGELU    | n:  24
IntSoftmax | n:  9
IntGELU    | n:  24
IntSoftmax | n:  9
IntGELU    | n:  24
IntSoftmax | n:  9
IntGELU    | n:  24
IntSoftmax | n:  9
IntGELU    | n:  24
IntSoftmax | n:  9
IntGELU    | n:  24
.calib done
Test: [  0/391]	Time  3.834 ( 3.834)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.87
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=9, intgelu_exp_n=25
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=9, intgelu_exp_n=25, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  9
IntGELU    | n:  25
IntSoftmax | n:  9
IntGELU    | n:  25
IntSoftmax | n:  9
IntGELU    | n:  25
IntSoftmax | n:  9
IntGELU    | n:  25
IntSoftmax | n:  9
IntGELU    | n:  25
IntSoftmax | n:  9
IntGELU    | n:  25
IntSoftmax | n:  9
IntGELU    | n:  25
IntSoftmax | n:  9
IntGELU    | n:  25
IntSoftmax | n:  9
IntGELU    | n:  25
IntSoftmax | n:  9
IntGELU    | n:  25
IntSoftmax | n:  9
IntGELU    | n:  25
IntSoftmax | n:  9
IntGELU    | n:  25
.calib done
Test: [  0/391]	Time  3.649 ( 3.649)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.78
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=9, intgelu_exp_n=26
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=9, intgelu_exp_n=26, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  9
IntGELU    | n:  26
IntSoftmax | n:  9
IntGELU    | n:  26
IntSoftmax | n:  9
IntGELU    | n:  26
IntSoftmax | n:  9
IntGELU    | n:  26
IntSoftmax | n:  9
IntGELU    | n:  26
IntSoftmax | n:  9
IntGELU    | n:  26
IntSoftmax | n:  9
IntGELU    | n:  26
IntSoftmax | n:  9
IntGELU    | n:  26
IntSoftmax | n:  9
IntGELU    | n:  26
IntSoftmax | n:  9
IntGELU    | n:  26
IntSoftmax | n:  9
IntGELU    | n:  26
IntSoftmax | n:  9
IntGELU    | n:  26
.calib done
Test: [  0/391]	Time  3.913 ( 3.913)	Acc@1   0.00 (  0.00)	Acc@5   1.56 (  1.56)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 1.562
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=9, intgelu_exp_n=27
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=9, intgelu_exp_n=27, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  9
IntGELU    | n:  27
IntSoftmax | n:  9
IntGELU    | n:  27
IntSoftmax | n:  9
IntGELU    | n:  27
IntSoftmax | n:  9
IntGELU    | n:  27
IntSoftmax | n:  9
IntGELU    | n:  27
IntSoftmax | n:  9
IntGELU    | n:  27
IntSoftmax | n:  9
IntGELU    | n:  27
IntSoftmax | n:  9
IntGELU    | n:  27
IntSoftmax | n:  9
IntGELU    | n:  27
IntSoftmax | n:  9
IntGELU    | n:  27
IntSoftmax | n:  9
IntGELU    | n:  27
IntSoftmax | n:  9
IntGELU    | n:  27
.calib done
Test: [  0/391]	Time  3.885 ( 3.885)	Acc@1   0.00 (  0.00)	Acc@5   0.78 (  0.78)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.781
Time: 8.95
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=9, intgelu_exp_n=28
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=9, intgelu_exp_n=28, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  9
IntGELU    | n:  28
IntSoftmax | n:  9
IntGELU    | n:  28
IntSoftmax | n:  9
IntGELU    | n:  28
IntSoftmax | n:  9
IntGELU    | n:  28
IntSoftmax | n:  9
IntGELU    | n:  28
IntSoftmax | n:  9
IntGELU    | n:  28
IntSoftmax | n:  9
IntGELU    | n:  28
IntSoftmax | n:  9
IntGELU    | n:  28
IntSoftmax | n:  9
IntGELU    | n:  28
IntSoftmax | n:  9
IntGELU    | n:  28
IntSoftmax | n:  9
IntGELU    | n:  28
IntSoftmax | n:  9
IntGELU    | n:  28
.calib done
Test: [  0/391]	Time  3.950 ( 3.950)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.01
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=9, intgelu_exp_n=29
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=9, intgelu_exp_n=29, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  9
IntGELU    | n:  29
IntSoftmax | n:  9
IntGELU    | n:  29
IntSoftmax | n:  9
IntGELU    | n:  29
IntSoftmax | n:  9
IntGELU    | n:  29
IntSoftmax | n:  9
IntGELU    | n:  29
IntSoftmax | n:  9
IntGELU    | n:  29
IntSoftmax | n:  9
IntGELU    | n:  29
IntSoftmax | n:  9
IntGELU    | n:  29
IntSoftmax | n:  9
IntGELU    | n:  29
IntSoftmax | n:  9
IntGELU    | n:  29
IntSoftmax | n:  9
IntGELU    | n:  29
IntSoftmax | n:  9
IntGELU    | n:  29
.calib done
Test: [  0/391]	Time  3.940 ( 3.940)	Acc@1   3.91 (  3.91)	Acc@5   9.38 (  9.38)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 3.906 Prec@5 9.375
Time: 9.01
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=9, intgelu_exp_n=30
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=9, intgelu_exp_n=30, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  9
IntGELU    | n:  30
IntSoftmax | n:  9
IntGELU    | n:  30
IntSoftmax | n:  9
IntGELU    | n:  30
IntSoftmax | n:  9
IntGELU    | n:  30
IntSoftmax | n:  9
IntGELU    | n:  30
IntSoftmax | n:  9
IntGELU    | n:  30
IntSoftmax | n:  9
IntGELU    | n:  30
IntSoftmax | n:  9
IntGELU    | n:  30
IntSoftmax | n:  9
IntGELU    | n:  30
IntSoftmax | n:  9
IntGELU    | n:  30
IntSoftmax | n:  9
IntGELU    | n:  30
IntSoftmax | n:  9
IntGELU    | n:  30
.calib done
Test: [  0/391]	Time  3.963 ( 3.963)	Acc@1   1.56 (  1.56)	Acc@5   5.47 (  5.47)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 1.562 Prec@5 5.469
Time: 9.02
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=9, intgelu_exp_n=31
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=9, intgelu_exp_n=31, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  9
IntGELU    | n:  31
IntSoftmax | n:  9
IntGELU    | n:  31
IntSoftmax | n:  9
IntGELU    | n:  31
IntSoftmax | n:  9
IntGELU    | n:  31
IntSoftmax | n:  9
IntGELU    | n:  31
IntSoftmax | n:  9
IntGELU    | n:  31
IntSoftmax | n:  9
IntGELU    | n:  31
IntSoftmax | n:  9
IntGELU    | n:  31
IntSoftmax | n:  9
IntGELU    | n:  31
IntSoftmax | n:  9
IntGELU    | n:  31
IntSoftmax | n:  9
IntGELU    | n:  31
IntSoftmax | n:  9
IntGELU    | n:  31
.calib done
Test: [  0/391]	Time  3.791 ( 3.791)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.82
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=10, intgelu_exp_n=0
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=10, intgelu_exp_n=0, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  10
IntGELU    | n:  0
IntSoftmax | n:  10
IntGELU    | n:  0
IntSoftmax | n:  10
IntGELU    | n:  0
IntSoftmax | n:  10
IntGELU    | n:  0
IntSoftmax | n:  10
IntGELU    | n:  0
IntSoftmax | n:  10
IntGELU    | n:  0
IntSoftmax | n:  10
IntGELU    | n:  0
IntSoftmax | n:  10
IntGELU    | n:  0
IntSoftmax | n:  10
IntGELU    | n:  0
IntSoftmax | n:  10
IntGELU    | n:  0
IntSoftmax | n:  10
IntGELU    | n:  0
IntSoftmax | n:  10
IntGELU    | n:  0
.calib done
Test: [  0/391]	Time  3.970 ( 3.970)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.06
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=10, intgelu_exp_n=1
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=10, intgelu_exp_n=1, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  10
IntGELU    | n:  1
IntSoftmax | n:  10
IntGELU    | n:  1
IntSoftmax | n:  10
IntGELU    | n:  1
IntSoftmax | n:  10
IntGELU    | n:  1
IntSoftmax | n:  10
IntGELU    | n:  1
IntSoftmax | n:  10
IntGELU    | n:  1
IntSoftmax | n:  10
IntGELU    | n:  1
IntSoftmax | n:  10
IntGELU    | n:  1
IntSoftmax | n:  10
IntGELU    | n:  1
IntSoftmax | n:  10
IntGELU    | n:  1
IntSoftmax | n:  10
IntGELU    | n:  1
IntSoftmax | n:  10
IntGELU    | n:  1
.calib done
Test: [  0/391]	Time  3.835 ( 3.835)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.84
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=10, intgelu_exp_n=2
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=10, intgelu_exp_n=2, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  10
IntGELU    | n:  2
IntSoftmax | n:  10
IntGELU    | n:  2
IntSoftmax | n:  10
IntGELU    | n:  2
IntSoftmax | n:  10
IntGELU    | n:  2
IntSoftmax | n:  10
IntGELU    | n:  2
IntSoftmax | n:  10
IntGELU    | n:  2
IntSoftmax | n:  10
IntGELU    | n:  2
IntSoftmax | n:  10
IntGELU    | n:  2
IntSoftmax | n:  10
IntGELU    | n:  2
IntSoftmax | n:  10
IntGELU    | n:  2
IntSoftmax | n:  10
IntGELU    | n:  2
IntSoftmax | n:  10
IntGELU    | n:  2
.calib done
Test: [  0/391]	Time  3.921 ( 3.921)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.01
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=10, intgelu_exp_n=3
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=10, intgelu_exp_n=3, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  10
IntGELU    | n:  3
IntSoftmax | n:  10
IntGELU    | n:  3
IntSoftmax | n:  10
IntGELU    | n:  3
IntSoftmax | n:  10
IntGELU    | n:  3
IntSoftmax | n:  10
IntGELU    | n:  3
IntSoftmax | n:  10
IntGELU    | n:  3
IntSoftmax | n:  10
IntGELU    | n:  3
IntSoftmax | n:  10
IntGELU    | n:  3
IntSoftmax | n:  10
IntGELU    | n:  3
IntSoftmax | n:  10
IntGELU    | n:  3
IntSoftmax | n:  10
IntGELU    | n:  3
IntSoftmax | n:  10
IntGELU    | n:  3
.calib done
Test: [  0/391]	Time  3.936 ( 3.936)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.94
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=10, intgelu_exp_n=4
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=10, intgelu_exp_n=4, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  10
IntGELU    | n:  4
IntSoftmax | n:  10
IntGELU    | n:  4
IntSoftmax | n:  10
IntGELU    | n:  4
IntSoftmax | n:  10
IntGELU    | n:  4
IntSoftmax | n:  10
IntGELU    | n:  4
IntSoftmax | n:  10
IntGELU    | n:  4
IntSoftmax | n:  10
IntGELU    | n:  4
IntSoftmax | n:  10
IntGELU    | n:  4
IntSoftmax | n:  10
IntGELU    | n:  4
IntSoftmax | n:  10
IntGELU    | n:  4
IntSoftmax | n:  10
IntGELU    | n:  4
IntSoftmax | n:  10
IntGELU    | n:  4
.calib done
Test: [  0/391]	Time  3.801 ( 3.801)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.86
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=10, intgelu_exp_n=5
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=10, intgelu_exp_n=5, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  10
IntGELU    | n:  5
IntSoftmax | n:  10
IntGELU    | n:  5
IntSoftmax | n:  10
IntGELU    | n:  5
IntSoftmax | n:  10
IntGELU    | n:  5
IntSoftmax | n:  10
IntGELU    | n:  5
IntSoftmax | n:  10
IntGELU    | n:  5
IntSoftmax | n:  10
IntGELU    | n:  5
IntSoftmax | n:  10
IntGELU    | n:  5
IntSoftmax | n:  10
IntGELU    | n:  5
IntSoftmax | n:  10
IntGELU    | n:  5
IntSoftmax | n:  10
IntGELU    | n:  5
IntSoftmax | n:  10
IntGELU    | n:  5
.calib done
Test: [  0/391]	Time  3.778 ( 3.778)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.87
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=10, intgelu_exp_n=6
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=10, intgelu_exp_n=6, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  10
IntGELU    | n:  6
IntSoftmax | n:  10
IntGELU    | n:  6
IntSoftmax | n:  10
IntGELU    | n:  6
IntSoftmax | n:  10
IntGELU    | n:  6
IntSoftmax | n:  10
IntGELU    | n:  6
IntSoftmax | n:  10
IntGELU    | n:  6
IntSoftmax | n:  10
IntGELU    | n:  6
IntSoftmax | n:  10
IntGELU    | n:  6
IntSoftmax | n:  10
IntGELU    | n:  6
IntSoftmax | n:  10
IntGELU    | n:  6
IntSoftmax | n:  10
IntGELU    | n:  6
IntSoftmax | n:  10
IntGELU    | n:  6
.calib done
Test: [  0/391]	Time  3.943 ( 3.943)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.99
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=10, intgelu_exp_n=7
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=10, intgelu_exp_n=7, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  10
IntGELU    | n:  7
IntSoftmax | n:  10
IntGELU    | n:  7
IntSoftmax | n:  10
IntGELU    | n:  7
IntSoftmax | n:  10
IntGELU    | n:  7
IntSoftmax | n:  10
IntGELU    | n:  7
IntSoftmax | n:  10
IntGELU    | n:  7
IntSoftmax | n:  10
IntGELU    | n:  7
IntSoftmax | n:  10
IntGELU    | n:  7
IntSoftmax | n:  10
IntGELU    | n:  7
IntSoftmax | n:  10
IntGELU    | n:  7
IntSoftmax | n:  10
IntGELU    | n:  7
IntSoftmax | n:  10
IntGELU    | n:  7
.calib done
Test: [  0/391]	Time  3.887 ( 3.887)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.93
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=10, intgelu_exp_n=8
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=10, intgelu_exp_n=8, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  10
IntGELU    | n:  8
IntSoftmax | n:  10
IntGELU    | n:  8
IntSoftmax | n:  10
IntGELU    | n:  8
IntSoftmax | n:  10
IntGELU    | n:  8
IntSoftmax | n:  10
IntGELU    | n:  8
IntSoftmax | n:  10
IntGELU    | n:  8
IntSoftmax | n:  10
IntGELU    | n:  8
IntSoftmax | n:  10
IntGELU    | n:  8
IntSoftmax | n:  10
IntGELU    | n:  8
IntSoftmax | n:  10
IntGELU    | n:  8
IntSoftmax | n:  10
IntGELU    | n:  8
IntSoftmax | n:  10
IntGELU    | n:  8
.calib done
Test: [  0/391]	Time  3.702 ( 3.702)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.70
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=10, intgelu_exp_n=9
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=10, intgelu_exp_n=9, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  10
IntGELU    | n:  9
IntSoftmax | n:  10
IntGELU    | n:  9
IntSoftmax | n:  10
IntGELU    | n:  9
IntSoftmax | n:  10
IntGELU    | n:  9
IntSoftmax | n:  10
IntGELU    | n:  9
IntSoftmax | n:  10
IntGELU    | n:  9
IntSoftmax | n:  10
IntGELU    | n:  9
IntSoftmax | n:  10
IntGELU    | n:  9
IntSoftmax | n:  10
IntGELU    | n:  9
IntSoftmax | n:  10
IntGELU    | n:  9
IntSoftmax | n:  10
IntGELU    | n:  9
IntSoftmax | n:  10
IntGELU    | n:  9
.calib done
Test: [  0/391]	Time  3.833 ( 3.833)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.91
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=10, intgelu_exp_n=10
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=10, intgelu_exp_n=10, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  10
IntGELU    | n:  10
IntSoftmax | n:  10
IntGELU    | n:  10
IntSoftmax | n:  10
IntGELU    | n:  10
IntSoftmax | n:  10
IntGELU    | n:  10
IntSoftmax | n:  10
IntGELU    | n:  10
IntSoftmax | n:  10
IntGELU    | n:  10
IntSoftmax | n:  10
IntGELU    | n:  10
IntSoftmax | n:  10
IntGELU    | n:  10
IntSoftmax | n:  10
IntGELU    | n:  10
IntSoftmax | n:  10
IntGELU    | n:  10
IntSoftmax | n:  10
IntGELU    | n:  10
IntSoftmax | n:  10
IntGELU    | n:  10
.calib done
Test: [  0/391]	Time  3.707 ( 3.707)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.81
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=10, intgelu_exp_n=11
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=10, intgelu_exp_n=11, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  10
IntGELU    | n:  11
IntSoftmax | n:  10
IntGELU    | n:  11
IntSoftmax | n:  10
IntGELU    | n:  11
IntSoftmax | n:  10
IntGELU    | n:  11
IntSoftmax | n:  10
IntGELU    | n:  11
IntSoftmax | n:  10
IntGELU    | n:  11
IntSoftmax | n:  10
IntGELU    | n:  11
IntSoftmax | n:  10
IntGELU    | n:  11
IntSoftmax | n:  10
IntGELU    | n:  11
IntSoftmax | n:  10
IntGELU    | n:  11
IntSoftmax | n:  10
IntGELU    | n:  11
IntSoftmax | n:  10
IntGELU    | n:  11
.calib done
Test: [  0/391]	Time  3.984 ( 3.984)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.01
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=10, intgelu_exp_n=12
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=10, intgelu_exp_n=12, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  10
IntGELU    | n:  12
IntSoftmax | n:  10
IntGELU    | n:  12
IntSoftmax | n:  10
IntGELU    | n:  12
IntSoftmax | n:  10
IntGELU    | n:  12
IntSoftmax | n:  10
IntGELU    | n:  12
IntSoftmax | n:  10
IntGELU    | n:  12
IntSoftmax | n:  10
IntGELU    | n:  12
IntSoftmax | n:  10
IntGELU    | n:  12
IntSoftmax | n:  10
IntGELU    | n:  12
IntSoftmax | n:  10
IntGELU    | n:  12
IntSoftmax | n:  10
IntGELU    | n:  12
IntSoftmax | n:  10
IntGELU    | n:  12
.calib done
Test: [  0/391]	Time  4.001 ( 4.001)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.01
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=10, intgelu_exp_n=13
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=10, intgelu_exp_n=13, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  10
IntGELU    | n:  13
IntSoftmax | n:  10
IntGELU    | n:  13
IntSoftmax | n:  10
IntGELU    | n:  13
IntSoftmax | n:  10
IntGELU    | n:  13
IntSoftmax | n:  10
IntGELU    | n:  13
IntSoftmax | n:  10
IntGELU    | n:  13
IntSoftmax | n:  10
IntGELU    | n:  13
IntSoftmax | n:  10
IntGELU    | n:  13
IntSoftmax | n:  10
IntGELU    | n:  13
IntSoftmax | n:  10
IntGELU    | n:  13
IntSoftmax | n:  10
IntGELU    | n:  13
IntSoftmax | n:  10
IntGELU    | n:  13
.calib done
Test: [  0/391]	Time  3.952 ( 3.952)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=10, intgelu_exp_n=14
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=10, intgelu_exp_n=14, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  10
IntGELU    | n:  14
IntSoftmax | n:  10
IntGELU    | n:  14
IntSoftmax | n:  10
IntGELU    | n:  14
IntSoftmax | n:  10
IntGELU    | n:  14
IntSoftmax | n:  10
IntGELU    | n:  14
IntSoftmax | n:  10
IntGELU    | n:  14
IntSoftmax | n:  10
IntGELU    | n:  14
IntSoftmax | n:  10
IntGELU    | n:  14
IntSoftmax | n:  10
IntGELU    | n:  14
IntSoftmax | n:  10
IntGELU    | n:  14
IntSoftmax | n:  10
IntGELU    | n:  14
IntSoftmax | n:  10
IntGELU    | n:  14
.calib done
Test: [  0/391]	Time  3.831 ( 3.831)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.87
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=10, intgelu_exp_n=15
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=10, intgelu_exp_n=15, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  10
IntGELU    | n:  15
IntSoftmax | n:  10
IntGELU    | n:  15
IntSoftmax | n:  10
IntGELU    | n:  15
IntSoftmax | n:  10
IntGELU    | n:  15
IntSoftmax | n:  10
IntGELU    | n:  15
IntSoftmax | n:  10
IntGELU    | n:  15
IntSoftmax | n:  10
IntGELU    | n:  15
IntSoftmax | n:  10
IntGELU    | n:  15
IntSoftmax | n:  10
IntGELU    | n:  15
IntSoftmax | n:  10
IntGELU    | n:  15
IntSoftmax | n:  10
IntGELU    | n:  15
IntSoftmax | n:  10
IntGELU    | n:  15
.calib done
Test: [  0/391]	Time  3.885 ( 3.885)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.91
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=10, intgelu_exp_n=16
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=10, intgelu_exp_n=16, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  10
IntGELU    | n:  16
IntSoftmax | n:  10
IntGELU    | n:  16
IntSoftmax | n:  10
IntGELU    | n:  16
IntSoftmax | n:  10
IntGELU    | n:  16
IntSoftmax | n:  10
IntGELU    | n:  16
IntSoftmax | n:  10
IntGELU    | n:  16
IntSoftmax | n:  10
IntGELU    | n:  16
IntSoftmax | n:  10
IntGELU    | n:  16
IntSoftmax | n:  10
IntGELU    | n:  16
IntSoftmax | n:  10
IntGELU    | n:  16
IntSoftmax | n:  10
IntGELU    | n:  16
IntSoftmax | n:  10
IntGELU    | n:  16
.calib done
Test: [  0/391]	Time  3.980 ( 3.980)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.04
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=10, intgelu_exp_n=17
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=10, intgelu_exp_n=17, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  10
IntGELU    | n:  17
IntSoftmax | n:  10
IntGELU    | n:  17
IntSoftmax | n:  10
IntGELU    | n:  17
IntSoftmax | n:  10
IntGELU    | n:  17
IntSoftmax | n:  10
IntGELU    | n:  17
IntSoftmax | n:  10
IntGELU    | n:  17
IntSoftmax | n:  10
IntGELU    | n:  17
IntSoftmax | n:  10
IntGELU    | n:  17
IntSoftmax | n:  10
IntGELU    | n:  17
IntSoftmax | n:  10
IntGELU    | n:  17
IntSoftmax | n:  10
IntGELU    | n:  17
IntSoftmax | n:  10
IntGELU    | n:  17
.calib done
Test: [  0/391]	Time  3.899 ( 3.899)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.00
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=10, intgelu_exp_n=18
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=10, intgelu_exp_n=18, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  10
IntGELU    | n:  18
IntSoftmax | n:  10
IntGELU    | n:  18
IntSoftmax | n:  10
IntGELU    | n:  18
IntSoftmax | n:  10
IntGELU    | n:  18
IntSoftmax | n:  10
IntGELU    | n:  18
IntSoftmax | n:  10
IntGELU    | n:  18
IntSoftmax | n:  10
IntGELU    | n:  18
IntSoftmax | n:  10
IntGELU    | n:  18
IntSoftmax | n:  10
IntGELU    | n:  18
IntSoftmax | n:  10
IntGELU    | n:  18
IntSoftmax | n:  10
IntGELU    | n:  18
IntSoftmax | n:  10
IntGELU    | n:  18
.calib done
Test: [  0/391]	Time  3.900 ( 3.900)	Acc@1   0.78 (  0.78)	Acc@5   0.78 (  0.78)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.781 Prec@5 0.781
Time: 8.98
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=10, intgelu_exp_n=19
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=10, intgelu_exp_n=19, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  10
IntGELU    | n:  19
IntSoftmax | n:  10
IntGELU    | n:  19
IntSoftmax | n:  10
IntGELU    | n:  19
IntSoftmax | n:  10
IntGELU    | n:  19
IntSoftmax | n:  10
IntGELU    | n:  19
IntSoftmax | n:  10
IntGELU    | n:  19
IntSoftmax | n:  10
IntGELU    | n:  19
IntSoftmax | n:  10
IntGELU    | n:  19
IntSoftmax | n:  10
IntGELU    | n:  19
IntSoftmax | n:  10
IntGELU    | n:  19
IntSoftmax | n:  10
IntGELU    | n:  19
IntSoftmax | n:  10
IntGELU    | n:  19
.calib done
Test: [  0/391]	Time  3.914 ( 3.914)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.04
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=10, intgelu_exp_n=20
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=10, intgelu_exp_n=20, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  10
IntGELU    | n:  20
IntSoftmax | n:  10
IntGELU    | n:  20
IntSoftmax | n:  10
IntGELU    | n:  20
IntSoftmax | n:  10
IntGELU    | n:  20
IntSoftmax | n:  10
IntGELU    | n:  20
IntSoftmax | n:  10
IntGELU    | n:  20
IntSoftmax | n:  10
IntGELU    | n:  20
IntSoftmax | n:  10
IntGELU    | n:  20
IntSoftmax | n:  10
IntGELU    | n:  20
IntSoftmax | n:  10
IntGELU    | n:  20
IntSoftmax | n:  10
IntGELU    | n:  20
IntSoftmax | n:  10
IntGELU    | n:  20
.calib done
Test: [  0/391]	Time  3.849 ( 3.849)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.85
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=10, intgelu_exp_n=21
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=10, intgelu_exp_n=21, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  10
IntGELU    | n:  21
IntSoftmax | n:  10
IntGELU    | n:  21
IntSoftmax | n:  10
IntGELU    | n:  21
IntSoftmax | n:  10
IntGELU    | n:  21
IntSoftmax | n:  10
IntGELU    | n:  21
IntSoftmax | n:  10
IntGELU    | n:  21
IntSoftmax | n:  10
IntGELU    | n:  21
IntSoftmax | n:  10
IntGELU    | n:  21
IntSoftmax | n:  10
IntGELU    | n:  21
IntSoftmax | n:  10
IntGELU    | n:  21
IntSoftmax | n:  10
IntGELU    | n:  21
IntSoftmax | n:  10
IntGELU    | n:  21
.calib done
Test: [  0/391]	Time  3.906 ( 3.906)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.92
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=10, intgelu_exp_n=22
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=10, intgelu_exp_n=22, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  10
IntGELU    | n:  22
IntSoftmax | n:  10
IntGELU    | n:  22
IntSoftmax | n:  10
IntGELU    | n:  22
IntSoftmax | n:  10
IntGELU    | n:  22
IntSoftmax | n:  10
IntGELU    | n:  22
IntSoftmax | n:  10
IntGELU    | n:  22
IntSoftmax | n:  10
IntGELU    | n:  22
IntSoftmax | n:  10
IntGELU    | n:  22
IntSoftmax | n:  10
IntGELU    | n:  22
IntSoftmax | n:  10
IntGELU    | n:  22
IntSoftmax | n:  10
IntGELU    | n:  22
IntSoftmax | n:  10
IntGELU    | n:  22
.calib done
Test: [  0/391]	Time  3.787 ( 3.787)	Acc@1   0.00 (  0.00)	Acc@5   0.78 (  0.78)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.781
Time: 8.84
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=10, intgelu_exp_n=23
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=10, intgelu_exp_n=23, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  10
IntGELU    | n:  23
IntSoftmax | n:  10
IntGELU    | n:  23
IntSoftmax | n:  10
IntGELU    | n:  23
IntSoftmax | n:  10
IntGELU    | n:  23
IntSoftmax | n:  10
IntGELU    | n:  23
IntSoftmax | n:  10
IntGELU    | n:  23
IntSoftmax | n:  10
IntGELU    | n:  23
IntSoftmax | n:  10
IntGELU    | n:  23
IntSoftmax | n:  10
IntGELU    | n:  23
IntSoftmax | n:  10
IntGELU    | n:  23
IntSoftmax | n:  10
IntGELU    | n:  23
IntSoftmax | n:  10
IntGELU    | n:  23
.calib done
Test: [  0/391]	Time  4.023 ( 4.023)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.15
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=10, intgelu_exp_n=24
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=10, intgelu_exp_n=24, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  10
IntGELU    | n:  24
IntSoftmax | n:  10
IntGELU    | n:  24
IntSoftmax | n:  10
IntGELU    | n:  24
IntSoftmax | n:  10
IntGELU    | n:  24
IntSoftmax | n:  10
IntGELU    | n:  24
IntSoftmax | n:  10
IntGELU    | n:  24
IntSoftmax | n:  10
IntGELU    | n:  24
IntSoftmax | n:  10
IntGELU    | n:  24
IntSoftmax | n:  10
IntGELU    | n:  24
IntSoftmax | n:  10
IntGELU    | n:  24
IntSoftmax | n:  10
IntGELU    | n:  24
IntSoftmax | n:  10
IntGELU    | n:  24
.calib done
Test: [  0/391]	Time  3.942 ( 3.942)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.01
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=10, intgelu_exp_n=25
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=10, intgelu_exp_n=25, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  10
IntGELU    | n:  25
IntSoftmax | n:  10
IntGELU    | n:  25
IntSoftmax | n:  10
IntGELU    | n:  25
IntSoftmax | n:  10
IntGELU    | n:  25
IntSoftmax | n:  10
IntGELU    | n:  25
IntSoftmax | n:  10
IntGELU    | n:  25
IntSoftmax | n:  10
IntGELU    | n:  25
IntSoftmax | n:  10
IntGELU    | n:  25
IntSoftmax | n:  10
IntGELU    | n:  25
IntSoftmax | n:  10
IntGELU    | n:  25
IntSoftmax | n:  10
IntGELU    | n:  25
IntSoftmax | n:  10
IntGELU    | n:  25
.calib done
Test: [  0/391]	Time  3.936 ( 3.936)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.94
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=10, intgelu_exp_n=26
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=10, intgelu_exp_n=26, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  10
IntGELU    | n:  26
IntSoftmax | n:  10
IntGELU    | n:  26
IntSoftmax | n:  10
IntGELU    | n:  26
IntSoftmax | n:  10
IntGELU    | n:  26
IntSoftmax | n:  10
IntGELU    | n:  26
IntSoftmax | n:  10
IntGELU    | n:  26
IntSoftmax | n:  10
IntGELU    | n:  26
IntSoftmax | n:  10
IntGELU    | n:  26
IntSoftmax | n:  10
IntGELU    | n:  26
IntSoftmax | n:  10
IntGELU    | n:  26
IntSoftmax | n:  10
IntGELU    | n:  26
IntSoftmax | n:  10
IntGELU    | n:  26
.calib done
Test: [  0/391]	Time  3.774 ( 3.774)	Acc@1   0.00 (  0.00)	Acc@5   0.78 (  0.78)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.781
Time: 8.81
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=10, intgelu_exp_n=27
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=10, intgelu_exp_n=27, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  10
IntGELU    | n:  27
IntSoftmax | n:  10
IntGELU    | n:  27
IntSoftmax | n:  10
IntGELU    | n:  27
IntSoftmax | n:  10
IntGELU    | n:  27
IntSoftmax | n:  10
IntGELU    | n:  27
IntSoftmax | n:  10
IntGELU    | n:  27
IntSoftmax | n:  10
IntGELU    | n:  27
IntSoftmax | n:  10
IntGELU    | n:  27
IntSoftmax | n:  10
IntGELU    | n:  27
IntSoftmax | n:  10
IntGELU    | n:  27
IntSoftmax | n:  10
IntGELU    | n:  27
IntSoftmax | n:  10
IntGELU    | n:  27
.calib done
Test: [  0/391]	Time  3.954 ( 3.954)	Acc@1   0.00 (  0.00)	Acc@5   0.78 (  0.78)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.781
Time: 9.09
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=10, intgelu_exp_n=28
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=10, intgelu_exp_n=28, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  10
IntGELU    | n:  28
IntSoftmax | n:  10
IntGELU    | n:  28
IntSoftmax | n:  10
IntGELU    | n:  28
IntSoftmax | n:  10
IntGELU    | n:  28
IntSoftmax | n:  10
IntGELU    | n:  28
IntSoftmax | n:  10
IntGELU    | n:  28
IntSoftmax | n:  10
IntGELU    | n:  28
IntSoftmax | n:  10
IntGELU    | n:  28
IntSoftmax | n:  10
IntGELU    | n:  28
IntSoftmax | n:  10
IntGELU    | n:  28
IntSoftmax | n:  10
IntGELU    | n:  28
IntSoftmax | n:  10
IntGELU    | n:  28
.calib done
Test: [  0/391]	Time  4.056 ( 4.056)	Acc@1   0.00 (  0.00)	Acc@5   1.56 (  1.56)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 1.562
Time: 9.13
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=10, intgelu_exp_n=29
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=10, intgelu_exp_n=29, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  10
IntGELU    | n:  29
IntSoftmax | n:  10
IntGELU    | n:  29
IntSoftmax | n:  10
IntGELU    | n:  29
IntSoftmax | n:  10
IntGELU    | n:  29
IntSoftmax | n:  10
IntGELU    | n:  29
IntSoftmax | n:  10
IntGELU    | n:  29
IntSoftmax | n:  10
IntGELU    | n:  29
IntSoftmax | n:  10
IntGELU    | n:  29
IntSoftmax | n:  10
IntGELU    | n:  29
IntSoftmax | n:  10
IntGELU    | n:  29
IntSoftmax | n:  10
IntGELU    | n:  29
IntSoftmax | n:  10
IntGELU    | n:  29
.calib done
Test: [  0/391]	Time  3.962 ( 3.962)	Acc@1   1.56 (  1.56)	Acc@5   5.47 (  5.47)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 1.562 Prec@5 5.469
Time: 9.04
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=10, intgelu_exp_n=30
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=10, intgelu_exp_n=30, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  10
IntGELU    | n:  30
IntSoftmax | n:  10
IntGELU    | n:  30
IntSoftmax | n:  10
IntGELU    | n:  30
IntSoftmax | n:  10
IntGELU    | n:  30
IntSoftmax | n:  10
IntGELU    | n:  30
IntSoftmax | n:  10
IntGELU    | n:  30
IntSoftmax | n:  10
IntGELU    | n:  30
IntSoftmax | n:  10
IntGELU    | n:  30
IntSoftmax | n:  10
IntGELU    | n:  30
IntSoftmax | n:  10
IntGELU    | n:  30
IntSoftmax | n:  10
IntGELU    | n:  30
IntSoftmax | n:  10
IntGELU    | n:  30
.calib done
Test: [  0/391]	Time  3.847 ( 3.847)	Acc@1   1.56 (  1.56)	Acc@5   7.81 (  7.81)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 1.562 Prec@5 7.812
Time: 8.90
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=10, intgelu_exp_n=31
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=10, intgelu_exp_n=31, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  10
IntGELU    | n:  31
IntSoftmax | n:  10
IntGELU    | n:  31
IntSoftmax | n:  10
IntGELU    | n:  31
IntSoftmax | n:  10
IntGELU    | n:  31
IntSoftmax | n:  10
IntGELU    | n:  31
IntSoftmax | n:  10
IntGELU    | n:  31
IntSoftmax | n:  10
IntGELU    | n:  31
IntSoftmax | n:  10
IntGELU    | n:  31
IntSoftmax | n:  10
IntGELU    | n:  31
IntSoftmax | n:  10
IntGELU    | n:  31
IntSoftmax | n:  10
IntGELU    | n:  31
IntSoftmax | n:  10
IntGELU    | n:  31
.calib done
Test: [  0/391]	Time  3.793 ( 3.793)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.85
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=11, intgelu_exp_n=0
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=11, intgelu_exp_n=0, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  11
IntGELU    | n:  0
IntSoftmax | n:  11
IntGELU    | n:  0
IntSoftmax | n:  11
IntGELU    | n:  0
IntSoftmax | n:  11
IntGELU    | n:  0
IntSoftmax | n:  11
IntGELU    | n:  0
IntSoftmax | n:  11
IntGELU    | n:  0
IntSoftmax | n:  11
IntGELU    | n:  0
IntSoftmax | n:  11
IntGELU    | n:  0
IntSoftmax | n:  11
IntGELU    | n:  0
IntSoftmax | n:  11
IntGELU    | n:  0
IntSoftmax | n:  11
IntGELU    | n:  0
IntSoftmax | n:  11
IntGELU    | n:  0
.calib done
Test: [  0/391]	Time  3.838 ( 3.838)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.84
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=11, intgelu_exp_n=1
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=11, intgelu_exp_n=1, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  11
IntGELU    | n:  1
IntSoftmax | n:  11
IntGELU    | n:  1
IntSoftmax | n:  11
IntGELU    | n:  1
IntSoftmax | n:  11
IntGELU    | n:  1
IntSoftmax | n:  11
IntGELU    | n:  1
IntSoftmax | n:  11
IntGELU    | n:  1
IntSoftmax | n:  11
IntGELU    | n:  1
IntSoftmax | n:  11
IntGELU    | n:  1
IntSoftmax | n:  11
IntGELU    | n:  1
IntSoftmax | n:  11
IntGELU    | n:  1
IntSoftmax | n:  11
IntGELU    | n:  1
IntSoftmax | n:  11
IntGELU    | n:  1
.calib done
Test: [  0/391]	Time  3.682 ( 3.682)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.66
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=11, intgelu_exp_n=2
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=11, intgelu_exp_n=2, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  11
IntGELU    | n:  2
IntSoftmax | n:  11
IntGELU    | n:  2
IntSoftmax | n:  11
IntGELU    | n:  2
IntSoftmax | n:  11
IntGELU    | n:  2
IntSoftmax | n:  11
IntGELU    | n:  2
IntSoftmax | n:  11
IntGELU    | n:  2
IntSoftmax | n:  11
IntGELU    | n:  2
IntSoftmax | n:  11
IntGELU    | n:  2
IntSoftmax | n:  11
IntGELU    | n:  2
IntSoftmax | n:  11
IntGELU    | n:  2
IntSoftmax | n:  11
IntGELU    | n:  2
IntSoftmax | n:  11
IntGELU    | n:  2
.calib done
Test: [  0/391]	Time  3.955 ( 3.955)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.02
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=11, intgelu_exp_n=3
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=11, intgelu_exp_n=3, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  11
IntGELU    | n:  3
IntSoftmax | n:  11
IntGELU    | n:  3
IntSoftmax | n:  11
IntGELU    | n:  3
IntSoftmax | n:  11
IntGELU    | n:  3
IntSoftmax | n:  11
IntGELU    | n:  3
IntSoftmax | n:  11
IntGELU    | n:  3
IntSoftmax | n:  11
IntGELU    | n:  3
IntSoftmax | n:  11
IntGELU    | n:  3
IntSoftmax | n:  11
IntGELU    | n:  3
IntSoftmax | n:  11
IntGELU    | n:  3
IntSoftmax | n:  11
IntGELU    | n:  3
IntSoftmax | n:  11
IntGELU    | n:  3
.calib done
Test: [  0/391]	Time  3.924 ( 3.924)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.98
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=11, intgelu_exp_n=4
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=11, intgelu_exp_n=4, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  11
IntGELU    | n:  4
IntSoftmax | n:  11
IntGELU    | n:  4
IntSoftmax | n:  11
IntGELU    | n:  4
IntSoftmax | n:  11
IntGELU    | n:  4
IntSoftmax | n:  11
IntGELU    | n:  4
IntSoftmax | n:  11
IntGELU    | n:  4
IntSoftmax | n:  11
IntGELU    | n:  4
IntSoftmax | n:  11
IntGELU    | n:  4
IntSoftmax | n:  11
IntGELU    | n:  4
IntSoftmax | n:  11
IntGELU    | n:  4
IntSoftmax | n:  11
IntGELU    | n:  4
IntSoftmax | n:  11
IntGELU    | n:  4
.calib done
Test: [  0/391]	Time  3.960 ( 3.960)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.00
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=11, intgelu_exp_n=5
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=11, intgelu_exp_n=5, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  11
IntGELU    | n:  5
IntSoftmax | n:  11
IntGELU    | n:  5
IntSoftmax | n:  11
IntGELU    | n:  5
IntSoftmax | n:  11
IntGELU    | n:  5
IntSoftmax | n:  11
IntGELU    | n:  5
IntSoftmax | n:  11
IntGELU    | n:  5
IntSoftmax | n:  11
IntGELU    | n:  5
IntSoftmax | n:  11
IntGELU    | n:  5
IntSoftmax | n:  11
IntGELU    | n:  5
IntSoftmax | n:  11
IntGELU    | n:  5
IntSoftmax | n:  11
IntGELU    | n:  5
IntSoftmax | n:  11
IntGELU    | n:  5
.calib done
Test: [  0/391]	Time  3.858 ( 3.858)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.88
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=11, intgelu_exp_n=6
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=11, intgelu_exp_n=6, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  11
IntGELU    | n:  6
IntSoftmax | n:  11
IntGELU    | n:  6
IntSoftmax | n:  11
IntGELU    | n:  6
IntSoftmax | n:  11
IntGELU    | n:  6
IntSoftmax | n:  11
IntGELU    | n:  6
IntSoftmax | n:  11
IntGELU    | n:  6
IntSoftmax | n:  11
IntGELU    | n:  6
IntSoftmax | n:  11
IntGELU    | n:  6
IntSoftmax | n:  11
IntGELU    | n:  6
IntSoftmax | n:  11
IntGELU    | n:  6
IntSoftmax | n:  11
IntGELU    | n:  6
IntSoftmax | n:  11
IntGELU    | n:  6
.calib done
Test: [  0/391]	Time  4.006 ( 4.006)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.05
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=11, intgelu_exp_n=7
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=11, intgelu_exp_n=7, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  11
IntGELU    | n:  7
IntSoftmax | n:  11
IntGELU    | n:  7
IntSoftmax | n:  11
IntGELU    | n:  7
IntSoftmax | n:  11
IntGELU    | n:  7
IntSoftmax | n:  11
IntGELU    | n:  7
IntSoftmax | n:  11
IntGELU    | n:  7
IntSoftmax | n:  11
IntGELU    | n:  7
IntSoftmax | n:  11
IntGELU    | n:  7
IntSoftmax | n:  11
IntGELU    | n:  7
IntSoftmax | n:  11
IntGELU    | n:  7
IntSoftmax | n:  11
IntGELU    | n:  7
IntSoftmax | n:  11
IntGELU    | n:  7
.calib done
Test: [  0/391]	Time  3.776 ( 3.776)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.76
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=11, intgelu_exp_n=8
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=11, intgelu_exp_n=8, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  11
IntGELU    | n:  8
IntSoftmax | n:  11
IntGELU    | n:  8
IntSoftmax | n:  11
IntGELU    | n:  8
IntSoftmax | n:  11
IntGELU    | n:  8
IntSoftmax | n:  11
IntGELU    | n:  8
IntSoftmax | n:  11
IntGELU    | n:  8
IntSoftmax | n:  11
IntGELU    | n:  8
IntSoftmax | n:  11
IntGELU    | n:  8
IntSoftmax | n:  11
IntGELU    | n:  8
IntSoftmax | n:  11
IntGELU    | n:  8
IntSoftmax | n:  11
IntGELU    | n:  8
IntSoftmax | n:  11
IntGELU    | n:  8
.calib done
Test: [  0/391]	Time  3.712 ( 3.712)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.73
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=11, intgelu_exp_n=9
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=11, intgelu_exp_n=9, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  11
IntGELU    | n:  9
IntSoftmax | n:  11
IntGELU    | n:  9
IntSoftmax | n:  11
IntGELU    | n:  9
IntSoftmax | n:  11
IntGELU    | n:  9
IntSoftmax | n:  11
IntGELU    | n:  9
IntSoftmax | n:  11
IntGELU    | n:  9
IntSoftmax | n:  11
IntGELU    | n:  9
IntSoftmax | n:  11
IntGELU    | n:  9
IntSoftmax | n:  11
IntGELU    | n:  9
IntSoftmax | n:  11
IntGELU    | n:  9
IntSoftmax | n:  11
IntGELU    | n:  9
IntSoftmax | n:  11
IntGELU    | n:  9
.calib done
Test: [  0/391]	Time  3.731 ( 3.731)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.76
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=11, intgelu_exp_n=10
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=11, intgelu_exp_n=10, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  11
IntGELU    | n:  10
IntSoftmax | n:  11
IntGELU    | n:  10
IntSoftmax | n:  11
IntGELU    | n:  10
IntSoftmax | n:  11
IntGELU    | n:  10
IntSoftmax | n:  11
IntGELU    | n:  10
IntSoftmax | n:  11
IntGELU    | n:  10
IntSoftmax | n:  11
IntGELU    | n:  10
IntSoftmax | n:  11
IntGELU    | n:  10
IntSoftmax | n:  11
IntGELU    | n:  10
IntSoftmax | n:  11
IntGELU    | n:  10
IntSoftmax | n:  11
IntGELU    | n:  10
IntSoftmax | n:  11
IntGELU    | n:  10
.calib done
Test: [  0/391]	Time  3.901 ( 3.901)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.04
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=11, intgelu_exp_n=11
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=11, intgelu_exp_n=11, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  11
IntGELU    | n:  11
IntSoftmax | n:  11
IntGELU    | n:  11
IntSoftmax | n:  11
IntGELU    | n:  11
IntSoftmax | n:  11
IntGELU    | n:  11
IntSoftmax | n:  11
IntGELU    | n:  11
IntSoftmax | n:  11
IntGELU    | n:  11
IntSoftmax | n:  11
IntGELU    | n:  11
IntSoftmax | n:  11
IntGELU    | n:  11
IntSoftmax | n:  11
IntGELU    | n:  11
IntSoftmax | n:  11
IntGELU    | n:  11
IntSoftmax | n:  11
IntGELU    | n:  11
IntSoftmax | n:  11
IntGELU    | n:  11
.calib done
Test: [  0/391]	Time  3.770 ( 3.770)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.88
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=11, intgelu_exp_n=12
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=11, intgelu_exp_n=12, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  11
IntGELU    | n:  12
IntSoftmax | n:  11
IntGELU    | n:  12
IntSoftmax | n:  11
IntGELU    | n:  12
IntSoftmax | n:  11
IntGELU    | n:  12
IntSoftmax | n:  11
IntGELU    | n:  12
IntSoftmax | n:  11
IntGELU    | n:  12
IntSoftmax | n:  11
IntGELU    | n:  12
IntSoftmax | n:  11
IntGELU    | n:  12
IntSoftmax | n:  11
IntGELU    | n:  12
IntSoftmax | n:  11
IntGELU    | n:  12
IntSoftmax | n:  11
IntGELU    | n:  12
IntSoftmax | n:  11
IntGELU    | n:  12
.calib done
Test: [  0/391]	Time  3.889 ( 3.889)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.95
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=11, intgelu_exp_n=13
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=11, intgelu_exp_n=13, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  11
IntGELU    | n:  13
IntSoftmax | n:  11
IntGELU    | n:  13
IntSoftmax | n:  11
IntGELU    | n:  13
IntSoftmax | n:  11
IntGELU    | n:  13
IntSoftmax | n:  11
IntGELU    | n:  13
IntSoftmax | n:  11
IntGELU    | n:  13
IntSoftmax | n:  11
IntGELU    | n:  13
IntSoftmax | n:  11
IntGELU    | n:  13
IntSoftmax | n:  11
IntGELU    | n:  13
IntSoftmax | n:  11
IntGELU    | n:  13
IntSoftmax | n:  11
IntGELU    | n:  13
IntSoftmax | n:  11
IntGELU    | n:  13
.calib done
Test: [  0/391]	Time  3.865 ( 3.865)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.92
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=11, intgelu_exp_n=14
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=11, intgelu_exp_n=14, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  11
IntGELU    | n:  14
IntSoftmax | n:  11
IntGELU    | n:  14
IntSoftmax | n:  11
IntGELU    | n:  14
IntSoftmax | n:  11
IntGELU    | n:  14
IntSoftmax | n:  11
IntGELU    | n:  14
IntSoftmax | n:  11
IntGELU    | n:  14
IntSoftmax | n:  11
IntGELU    | n:  14
IntSoftmax | n:  11
IntGELU    | n:  14
IntSoftmax | n:  11
IntGELU    | n:  14
IntSoftmax | n:  11
IntGELU    | n:  14
IntSoftmax | n:  11
IntGELU    | n:  14
IntSoftmax | n:  11
IntGELU    | n:  14
.calib done
Test: [  0/391]	Time  3.966 ( 3.966)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.03
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=11, intgelu_exp_n=15
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=11, intgelu_exp_n=15, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  11
IntGELU    | n:  15
IntSoftmax | n:  11
IntGELU    | n:  15
IntSoftmax | n:  11
IntGELU    | n:  15
IntSoftmax | n:  11
IntGELU    | n:  15
IntSoftmax | n:  11
IntGELU    | n:  15
IntSoftmax | n:  11
IntGELU    | n:  15
IntSoftmax | n:  11
IntGELU    | n:  15
IntSoftmax | n:  11
IntGELU    | n:  15
IntSoftmax | n:  11
IntGELU    | n:  15
IntSoftmax | n:  11
IntGELU    | n:  15
IntSoftmax | n:  11
IntGELU    | n:  15
IntSoftmax | n:  11
IntGELU    | n:  15
.calib done
Test: [  0/391]	Time  3.809 ( 3.809)	Acc@1   0.00 (  0.00)	Acc@5   0.78 (  0.78)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.781
Time: 8.88
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=11, intgelu_exp_n=16
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=11, intgelu_exp_n=16, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  11
IntGELU    | n:  16
IntSoftmax | n:  11
IntGELU    | n:  16
IntSoftmax | n:  11
IntGELU    | n:  16
IntSoftmax | n:  11
IntGELU    | n:  16
IntSoftmax | n:  11
IntGELU    | n:  16
IntSoftmax | n:  11
IntGELU    | n:  16
IntSoftmax | n:  11
IntGELU    | n:  16
IntSoftmax | n:  11
IntGELU    | n:  16
IntSoftmax | n:  11
IntGELU    | n:  16
IntSoftmax | n:  11
IntGELU    | n:  16
IntSoftmax | n:  11
IntGELU    | n:  16
IntSoftmax | n:  11
IntGELU    | n:  16
.calib done
Test: [  0/391]	Time  4.003 ( 4.003)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.04
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=11, intgelu_exp_n=17
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=11, intgelu_exp_n=17, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  11
IntGELU    | n:  17
IntSoftmax | n:  11
IntGELU    | n:  17
IntSoftmax | n:  11
IntGELU    | n:  17
IntSoftmax | n:  11
IntGELU    | n:  17
IntSoftmax | n:  11
IntGELU    | n:  17
IntSoftmax | n:  11
IntGELU    | n:  17
IntSoftmax | n:  11
IntGELU    | n:  17
IntSoftmax | n:  11
IntGELU    | n:  17
IntSoftmax | n:  11
IntGELU    | n:  17
IntSoftmax | n:  11
IntGELU    | n:  17
IntSoftmax | n:  11
IntGELU    | n:  17
IntSoftmax | n:  11
IntGELU    | n:  17
.calib done
Test: [  0/391]	Time  3.869 ( 3.869)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.94
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=11, intgelu_exp_n=18
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=11, intgelu_exp_n=18, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  11
IntGELU    | n:  18
IntSoftmax | n:  11
IntGELU    | n:  18
IntSoftmax | n:  11
IntGELU    | n:  18
IntSoftmax | n:  11
IntGELU    | n:  18
IntSoftmax | n:  11
IntGELU    | n:  18
IntSoftmax | n:  11
IntGELU    | n:  18
IntSoftmax | n:  11
IntGELU    | n:  18
IntSoftmax | n:  11
IntGELU    | n:  18
IntSoftmax | n:  11
IntGELU    | n:  18
IntSoftmax | n:  11
IntGELU    | n:  18
IntSoftmax | n:  11
IntGELU    | n:  18
IntSoftmax | n:  11
IntGELU    | n:  18
.calib done
Test: [  0/391]	Time  3.903 ( 3.903)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.01
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=11, intgelu_exp_n=19
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=11, intgelu_exp_n=19, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  11
IntGELU    | n:  19
IntSoftmax | n:  11
IntGELU    | n:  19
IntSoftmax | n:  11
IntGELU    | n:  19
IntSoftmax | n:  11
IntGELU    | n:  19
IntSoftmax | n:  11
IntGELU    | n:  19
IntSoftmax | n:  11
IntGELU    | n:  19
IntSoftmax | n:  11
IntGELU    | n:  19
IntSoftmax | n:  11
IntGELU    | n:  19
IntSoftmax | n:  11
IntGELU    | n:  19
IntSoftmax | n:  11
IntGELU    | n:  19
IntSoftmax | n:  11
IntGELU    | n:  19
IntSoftmax | n:  11
IntGELU    | n:  19
.calib done
Test: [  0/391]	Time  3.803 ( 3.803)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.88
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=11, intgelu_exp_n=20
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=11, intgelu_exp_n=20, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  11
IntGELU    | n:  20
IntSoftmax | n:  11
IntGELU    | n:  20
IntSoftmax | n:  11
IntGELU    | n:  20
IntSoftmax | n:  11
IntGELU    | n:  20
IntSoftmax | n:  11
IntGELU    | n:  20
IntSoftmax | n:  11
IntGELU    | n:  20
IntSoftmax | n:  11
IntGELU    | n:  20
IntSoftmax | n:  11
IntGELU    | n:  20
IntSoftmax | n:  11
IntGELU    | n:  20
IntSoftmax | n:  11
IntGELU    | n:  20
IntSoftmax | n:  11
IntGELU    | n:  20
IntSoftmax | n:  11
IntGELU    | n:  20
.calib done
Test: [  0/391]	Time  3.747 ( 3.747)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.75
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=11, intgelu_exp_n=21
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=11, intgelu_exp_n=21, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  11
IntGELU    | n:  21
IntSoftmax | n:  11
IntGELU    | n:  21
IntSoftmax | n:  11
IntGELU    | n:  21
IntSoftmax | n:  11
IntGELU    | n:  21
IntSoftmax | n:  11
IntGELU    | n:  21
IntSoftmax | n:  11
IntGELU    | n:  21
IntSoftmax | n:  11
IntGELU    | n:  21
IntSoftmax | n:  11
IntGELU    | n:  21
IntSoftmax | n:  11
IntGELU    | n:  21
IntSoftmax | n:  11
IntGELU    | n:  21
IntSoftmax | n:  11
IntGELU    | n:  21
IntSoftmax | n:  11
IntGELU    | n:  21
.calib done
Test: [  0/391]	Time  3.955 ( 3.955)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.99
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=11, intgelu_exp_n=22
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=11, intgelu_exp_n=22, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  11
IntGELU    | n:  22
IntSoftmax | n:  11
IntGELU    | n:  22
IntSoftmax | n:  11
IntGELU    | n:  22
IntSoftmax | n:  11
IntGELU    | n:  22
IntSoftmax | n:  11
IntGELU    | n:  22
IntSoftmax | n:  11
IntGELU    | n:  22
IntSoftmax | n:  11
IntGELU    | n:  22
IntSoftmax | n:  11
IntGELU    | n:  22
IntSoftmax | n:  11
IntGELU    | n:  22
IntSoftmax | n:  11
IntGELU    | n:  22
IntSoftmax | n:  11
IntGELU    | n:  22
IntSoftmax | n:  11
IntGELU    | n:  22
.calib done
Test: [  0/391]	Time  3.907 ( 3.907)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=11, intgelu_exp_n=23
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=11, intgelu_exp_n=23, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  11
IntGELU    | n:  23
IntSoftmax | n:  11
IntGELU    | n:  23
IntSoftmax | n:  11
IntGELU    | n:  23
IntSoftmax | n:  11
IntGELU    | n:  23
IntSoftmax | n:  11
IntGELU    | n:  23
IntSoftmax | n:  11
IntGELU    | n:  23
IntSoftmax | n:  11
IntGELU    | n:  23
IntSoftmax | n:  11
IntGELU    | n:  23
IntSoftmax | n:  11
IntGELU    | n:  23
IntSoftmax | n:  11
IntGELU    | n:  23
IntSoftmax | n:  11
IntGELU    | n:  23
IntSoftmax | n:  11
IntGELU    | n:  23
.calib done
Test: [  0/391]	Time  3.911 ( 3.911)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.98
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=11, intgelu_exp_n=24
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=11, intgelu_exp_n=24, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  11
IntGELU    | n:  24
IntSoftmax | n:  11
IntGELU    | n:  24
IntSoftmax | n:  11
IntGELU    | n:  24
IntSoftmax | n:  11
IntGELU    | n:  24
IntSoftmax | n:  11
IntGELU    | n:  24
IntSoftmax | n:  11
IntGELU    | n:  24
IntSoftmax | n:  11
IntGELU    | n:  24
IntSoftmax | n:  11
IntGELU    | n:  24
IntSoftmax | n:  11
IntGELU    | n:  24
IntSoftmax | n:  11
IntGELU    | n:  24
IntSoftmax | n:  11
IntGELU    | n:  24
IntSoftmax | n:  11
IntGELU    | n:  24
.calib done
Test: [  0/391]	Time  3.809 ( 3.809)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.80
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=11, intgelu_exp_n=25
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=11, intgelu_exp_n=25, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  11
IntGELU    | n:  25
IntSoftmax | n:  11
IntGELU    | n:  25
IntSoftmax | n:  11
IntGELU    | n:  25
IntSoftmax | n:  11
IntGELU    | n:  25
IntSoftmax | n:  11
IntGELU    | n:  25
IntSoftmax | n:  11
IntGELU    | n:  25
IntSoftmax | n:  11
IntGELU    | n:  25
IntSoftmax | n:  11
IntGELU    | n:  25
IntSoftmax | n:  11
IntGELU    | n:  25
IntSoftmax | n:  11
IntGELU    | n:  25
IntSoftmax | n:  11
IntGELU    | n:  25
IntSoftmax | n:  11
IntGELU    | n:  25
.calib done
Test: [  0/391]	Time  3.896 ( 3.896)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.95
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=11, intgelu_exp_n=26
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=11, intgelu_exp_n=26, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  11
IntGELU    | n:  26
IntSoftmax | n:  11
IntGELU    | n:  26
IntSoftmax | n:  11
IntGELU    | n:  26
IntSoftmax | n:  11
IntGELU    | n:  26
IntSoftmax | n:  11
IntGELU    | n:  26
IntSoftmax | n:  11
IntGELU    | n:  26
IntSoftmax | n:  11
IntGELU    | n:  26
IntSoftmax | n:  11
IntGELU    | n:  26
IntSoftmax | n:  11
IntGELU    | n:  26
IntSoftmax | n:  11
IntGELU    | n:  26
IntSoftmax | n:  11
IntGELU    | n:  26
IntSoftmax | n:  11
IntGELU    | n:  26
.calib done
Test: [  0/391]	Time  3.984 ( 3.984)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.99
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=11, intgelu_exp_n=27
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=11, intgelu_exp_n=27, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  11
IntGELU    | n:  27
IntSoftmax | n:  11
IntGELU    | n:  27
IntSoftmax | n:  11
IntGELU    | n:  27
IntSoftmax | n:  11
IntGELU    | n:  27
IntSoftmax | n:  11
IntGELU    | n:  27
IntSoftmax | n:  11
IntGELU    | n:  27
IntSoftmax | n:  11
IntGELU    | n:  27
IntSoftmax | n:  11
IntGELU    | n:  27
IntSoftmax | n:  11
IntGELU    | n:  27
IntSoftmax | n:  11
IntGELU    | n:  27
IntSoftmax | n:  11
IntGELU    | n:  27
IntSoftmax | n:  11
IntGELU    | n:  27
.calib done
Test: [  0/391]	Time  3.901 ( 3.901)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=11, intgelu_exp_n=28
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=11, intgelu_exp_n=28, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  11
IntGELU    | n:  28
IntSoftmax | n:  11
IntGELU    | n:  28
IntSoftmax | n:  11
IntGELU    | n:  28
IntSoftmax | n:  11
IntGELU    | n:  28
IntSoftmax | n:  11
IntGELU    | n:  28
IntSoftmax | n:  11
IntGELU    | n:  28
IntSoftmax | n:  11
IntGELU    | n:  28
IntSoftmax | n:  11
IntGELU    | n:  28
IntSoftmax | n:  11
IntGELU    | n:  28
IntSoftmax | n:  11
IntGELU    | n:  28
IntSoftmax | n:  11
IntGELU    | n:  28
IntSoftmax | n:  11
IntGELU    | n:  28
.calib done
Test: [  0/391]	Time  3.778 ( 3.778)	Acc@1   0.00 (  0.00)	Acc@5   0.78 (  0.78)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.781
Time: 8.77
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=11, intgelu_exp_n=29
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=11, intgelu_exp_n=29, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  11
IntGELU    | n:  29
IntSoftmax | n:  11
IntGELU    | n:  29
IntSoftmax | n:  11
IntGELU    | n:  29
IntSoftmax | n:  11
IntGELU    | n:  29
IntSoftmax | n:  11
IntGELU    | n:  29
IntSoftmax | n:  11
IntGELU    | n:  29
IntSoftmax | n:  11
IntGELU    | n:  29
IntSoftmax | n:  11
IntGELU    | n:  29
IntSoftmax | n:  11
IntGELU    | n:  29
IntSoftmax | n:  11
IntGELU    | n:  29
IntSoftmax | n:  11
IntGELU    | n:  29
IntSoftmax | n:  11
IntGELU    | n:  29
.calib done
Test: [  0/391]	Time  3.953 ( 3.953)	Acc@1  13.28 ( 13.28)	Acc@5  32.03 ( 32.03)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 13.281 Prec@5 32.031
Time: 9.05
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=11, intgelu_exp_n=30
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=11, intgelu_exp_n=30, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  11
IntGELU    | n:  30
IntSoftmax | n:  11
IntGELU    | n:  30
IntSoftmax | n:  11
IntGELU    | n:  30
IntSoftmax | n:  11
IntGELU    | n:  30
IntSoftmax | n:  11
IntGELU    | n:  30
IntSoftmax | n:  11
IntGELU    | n:  30
IntSoftmax | n:  11
IntGELU    | n:  30
IntSoftmax | n:  11
IntGELU    | n:  30
IntSoftmax | n:  11
IntGELU    | n:  30
IntSoftmax | n:  11
IntGELU    | n:  30
IntSoftmax | n:  11
IntGELU    | n:  30
IntSoftmax | n:  11
IntGELU    | n:  30
.calib done
Test: [  0/391]	Time  3.845 ( 3.845)	Acc@1   3.91 (  3.91)	Acc@5  21.09 ( 21.09)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 3.906 Prec@5 21.094
Time: 8.86
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=11, intgelu_exp_n=31
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=11, intgelu_exp_n=31, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  11
IntGELU    | n:  31
IntSoftmax | n:  11
IntGELU    | n:  31
IntSoftmax | n:  11
IntGELU    | n:  31
IntSoftmax | n:  11
IntGELU    | n:  31
IntSoftmax | n:  11
IntGELU    | n:  31
IntSoftmax | n:  11
IntGELU    | n:  31
IntSoftmax | n:  11
IntGELU    | n:  31
IntSoftmax | n:  11
IntGELU    | n:  31
IntSoftmax | n:  11
IntGELU    | n:  31
IntSoftmax | n:  11
IntGELU    | n:  31
IntSoftmax | n:  11
IntGELU    | n:  31
IntSoftmax | n:  11
IntGELU    | n:  31
.calib done
Test: [  0/391]	Time  4.000 ( 4.000)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.08
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=12, intgelu_exp_n=0
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=12, intgelu_exp_n=0, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  12
IntGELU    | n:  0
IntSoftmax | n:  12
IntGELU    | n:  0
IntSoftmax | n:  12
IntGELU    | n:  0
IntSoftmax | n:  12
IntGELU    | n:  0
IntSoftmax | n:  12
IntGELU    | n:  0
IntSoftmax | n:  12
IntGELU    | n:  0
IntSoftmax | n:  12
IntGELU    | n:  0
IntSoftmax | n:  12
IntGELU    | n:  0
IntSoftmax | n:  12
IntGELU    | n:  0
IntSoftmax | n:  12
IntGELU    | n:  0
IntSoftmax | n:  12
IntGELU    | n:  0
IntSoftmax | n:  12
IntGELU    | n:  0
.calib done
Test: [  0/391]	Time  3.870 ( 3.870)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.93
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=12, intgelu_exp_n=1
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=12, intgelu_exp_n=1, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  12
IntGELU    | n:  1
IntSoftmax | n:  12
IntGELU    | n:  1
IntSoftmax | n:  12
IntGELU    | n:  1
IntSoftmax | n:  12
IntGELU    | n:  1
IntSoftmax | n:  12
IntGELU    | n:  1
IntSoftmax | n:  12
IntGELU    | n:  1
IntSoftmax | n:  12
IntGELU    | n:  1
IntSoftmax | n:  12
IntGELU    | n:  1
IntSoftmax | n:  12
IntGELU    | n:  1
IntSoftmax | n:  12
IntGELU    | n:  1
IntSoftmax | n:  12
IntGELU    | n:  1
IntSoftmax | n:  12
IntGELU    | n:  1
.calib done
Test: [  0/391]	Time  3.850 ( 3.850)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.88
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=12, intgelu_exp_n=2
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=12, intgelu_exp_n=2, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  12
IntGELU    | n:  2
IntSoftmax | n:  12
IntGELU    | n:  2
IntSoftmax | n:  12
IntGELU    | n:  2
IntSoftmax | n:  12
IntGELU    | n:  2
IntSoftmax | n:  12
IntGELU    | n:  2
IntSoftmax | n:  12
IntGELU    | n:  2
IntSoftmax | n:  12
IntGELU    | n:  2
IntSoftmax | n:  12
IntGELU    | n:  2
IntSoftmax | n:  12
IntGELU    | n:  2
IntSoftmax | n:  12
IntGELU    | n:  2
IntSoftmax | n:  12
IntGELU    | n:  2
IntSoftmax | n:  12
IntGELU    | n:  2
.calib done
Test: [  0/391]	Time  3.890 ( 3.890)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.91
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=12, intgelu_exp_n=3
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=12, intgelu_exp_n=3, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  12
IntGELU    | n:  3
IntSoftmax | n:  12
IntGELU    | n:  3
IntSoftmax | n:  12
IntGELU    | n:  3
IntSoftmax | n:  12
IntGELU    | n:  3
IntSoftmax | n:  12
IntGELU    | n:  3
IntSoftmax | n:  12
IntGELU    | n:  3
IntSoftmax | n:  12
IntGELU    | n:  3
IntSoftmax | n:  12
IntGELU    | n:  3
IntSoftmax | n:  12
IntGELU    | n:  3
IntSoftmax | n:  12
IntGELU    | n:  3
IntSoftmax | n:  12
IntGELU    | n:  3
IntSoftmax | n:  12
IntGELU    | n:  3
.calib done
Test: [  0/391]	Time  3.758 ( 3.758)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.79
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=12, intgelu_exp_n=4
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=12, intgelu_exp_n=4, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  12
IntGELU    | n:  4
IntSoftmax | n:  12
IntGELU    | n:  4
IntSoftmax | n:  12
IntGELU    | n:  4
IntSoftmax | n:  12
IntGELU    | n:  4
IntSoftmax | n:  12
IntGELU    | n:  4
IntSoftmax | n:  12
IntGELU    | n:  4
IntSoftmax | n:  12
IntGELU    | n:  4
IntSoftmax | n:  12
IntGELU    | n:  4
IntSoftmax | n:  12
IntGELU    | n:  4
IntSoftmax | n:  12
IntGELU    | n:  4
IntSoftmax | n:  12
IntGELU    | n:  4
IntSoftmax | n:  12
IntGELU    | n:  4
.calib done
Test: [  0/391]	Time  3.891 ( 3.891)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.91
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=12, intgelu_exp_n=5
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=12, intgelu_exp_n=5, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  12
IntGELU    | n:  5
IntSoftmax | n:  12
IntGELU    | n:  5
IntSoftmax | n:  12
IntGELU    | n:  5
IntSoftmax | n:  12
IntGELU    | n:  5
IntSoftmax | n:  12
IntGELU    | n:  5
IntSoftmax | n:  12
IntGELU    | n:  5
IntSoftmax | n:  12
IntGELU    | n:  5
IntSoftmax | n:  12
IntGELU    | n:  5
IntSoftmax | n:  12
IntGELU    | n:  5
IntSoftmax | n:  12
IntGELU    | n:  5
IntSoftmax | n:  12
IntGELU    | n:  5
IntSoftmax | n:  12
IntGELU    | n:  5
.calib done
Test: [  0/391]	Time  3.854 ( 3.854)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.88
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=12, intgelu_exp_n=6
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=12, intgelu_exp_n=6, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  12
IntGELU    | n:  6
IntSoftmax | n:  12
IntGELU    | n:  6
IntSoftmax | n:  12
IntGELU    | n:  6
IntSoftmax | n:  12
IntGELU    | n:  6
IntSoftmax | n:  12
IntGELU    | n:  6
IntSoftmax | n:  12
IntGELU    | n:  6
IntSoftmax | n:  12
IntGELU    | n:  6
IntSoftmax | n:  12
IntGELU    | n:  6
IntSoftmax | n:  12
IntGELU    | n:  6
IntSoftmax | n:  12
IntGELU    | n:  6
IntSoftmax | n:  12
IntGELU    | n:  6
IntSoftmax | n:  12
IntGELU    | n:  6
.calib done
Test: [  0/391]	Time  3.669 ( 3.669)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.71
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=12, intgelu_exp_n=7
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=12, intgelu_exp_n=7, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  12
IntGELU    | n:  7
IntSoftmax | n:  12
IntGELU    | n:  7
IntSoftmax | n:  12
IntGELU    | n:  7
IntSoftmax | n:  12
IntGELU    | n:  7
IntSoftmax | n:  12
IntGELU    | n:  7
IntSoftmax | n:  12
IntGELU    | n:  7
IntSoftmax | n:  12
IntGELU    | n:  7
IntSoftmax | n:  12
IntGELU    | n:  7
IntSoftmax | n:  12
IntGELU    | n:  7
IntSoftmax | n:  12
IntGELU    | n:  7
IntSoftmax | n:  12
IntGELU    | n:  7
IntSoftmax | n:  12
IntGELU    | n:  7
.calib done
Test: [  0/391]	Time  3.886 ( 3.886)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.89
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=12, intgelu_exp_n=8
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=12, intgelu_exp_n=8, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  12
IntGELU    | n:  8
IntSoftmax | n:  12
IntGELU    | n:  8
IntSoftmax | n:  12
IntGELU    | n:  8
IntSoftmax | n:  12
IntGELU    | n:  8
IntSoftmax | n:  12
IntGELU    | n:  8
IntSoftmax | n:  12
IntGELU    | n:  8
IntSoftmax | n:  12
IntGELU    | n:  8
IntSoftmax | n:  12
IntGELU    | n:  8
IntSoftmax | n:  12
IntGELU    | n:  8
IntSoftmax | n:  12
IntGELU    | n:  8
IntSoftmax | n:  12
IntGELU    | n:  8
IntSoftmax | n:  12
IntGELU    | n:  8
.calib done
Test: [  0/391]	Time  3.744 ( 3.744)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.79
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=12, intgelu_exp_n=9
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=12, intgelu_exp_n=9, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  12
IntGELU    | n:  9
IntSoftmax | n:  12
IntGELU    | n:  9
IntSoftmax | n:  12
IntGELU    | n:  9
IntSoftmax | n:  12
IntGELU    | n:  9
IntSoftmax | n:  12
IntGELU    | n:  9
IntSoftmax | n:  12
IntGELU    | n:  9
IntSoftmax | n:  12
IntGELU    | n:  9
IntSoftmax | n:  12
IntGELU    | n:  9
IntSoftmax | n:  12
IntGELU    | n:  9
IntSoftmax | n:  12
IntGELU    | n:  9
IntSoftmax | n:  12
IntGELU    | n:  9
IntSoftmax | n:  12
IntGELU    | n:  9
.calib done
Test: [  0/391]	Time  3.738 ( 3.738)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.90
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=12, intgelu_exp_n=10
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=12, intgelu_exp_n=10, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  12
IntGELU    | n:  10
IntSoftmax | n:  12
IntGELU    | n:  10
IntSoftmax | n:  12
IntGELU    | n:  10
IntSoftmax | n:  12
IntGELU    | n:  10
IntSoftmax | n:  12
IntGELU    | n:  10
IntSoftmax | n:  12
IntGELU    | n:  10
IntSoftmax | n:  12
IntGELU    | n:  10
IntSoftmax | n:  12
IntGELU    | n:  10
IntSoftmax | n:  12
IntGELU    | n:  10
IntSoftmax | n:  12
IntGELU    | n:  10
IntSoftmax | n:  12
IntGELU    | n:  10
IntSoftmax | n:  12
IntGELU    | n:  10
.calib done
Test: [  0/391]	Time  3.700 ( 3.700)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.75
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=12, intgelu_exp_n=11
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=12, intgelu_exp_n=11, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  12
IntGELU    | n:  11
IntSoftmax | n:  12
IntGELU    | n:  11
IntSoftmax | n:  12
IntGELU    | n:  11
IntSoftmax | n:  12
IntGELU    | n:  11
IntSoftmax | n:  12
IntGELU    | n:  11
IntSoftmax | n:  12
IntGELU    | n:  11
IntSoftmax | n:  12
IntGELU    | n:  11
IntSoftmax | n:  12
IntGELU    | n:  11
IntSoftmax | n:  12
IntGELU    | n:  11
IntSoftmax | n:  12
IntGELU    | n:  11
IntSoftmax | n:  12
IntGELU    | n:  11
IntSoftmax | n:  12
IntGELU    | n:  11
.calib done
Test: [  0/391]	Time  3.856 ( 3.856)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.92
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=12, intgelu_exp_n=12
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=12, intgelu_exp_n=12, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  12
IntGELU    | n:  12
IntSoftmax | n:  12
IntGELU    | n:  12
IntSoftmax | n:  12
IntGELU    | n:  12
IntSoftmax | n:  12
IntGELU    | n:  12
IntSoftmax | n:  12
IntGELU    | n:  12
IntSoftmax | n:  12
IntGELU    | n:  12
IntSoftmax | n:  12
IntGELU    | n:  12
IntSoftmax | n:  12
IntGELU    | n:  12
IntSoftmax | n:  12
IntGELU    | n:  12
IntSoftmax | n:  12
IntGELU    | n:  12
IntSoftmax | n:  12
IntGELU    | n:  12
IntSoftmax | n:  12
IntGELU    | n:  12
.calib done
Test: [  0/391]	Time  3.883 ( 3.883)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.87
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=12, intgelu_exp_n=13
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=12, intgelu_exp_n=13, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  12
IntGELU    | n:  13
IntSoftmax | n:  12
IntGELU    | n:  13
IntSoftmax | n:  12
IntGELU    | n:  13
IntSoftmax | n:  12
IntGELU    | n:  13
IntSoftmax | n:  12
IntGELU    | n:  13
IntSoftmax | n:  12
IntGELU    | n:  13
IntSoftmax | n:  12
IntGELU    | n:  13
IntSoftmax | n:  12
IntGELU    | n:  13
IntSoftmax | n:  12
IntGELU    | n:  13
IntSoftmax | n:  12
IntGELU    | n:  13
IntSoftmax | n:  12
IntGELU    | n:  13
IntSoftmax | n:  12
IntGELU    | n:  13
.calib done
Test: [  0/391]	Time  3.834 ( 3.834)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.87
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=12, intgelu_exp_n=14
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=12, intgelu_exp_n=14, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  12
IntGELU    | n:  14
IntSoftmax | n:  12
IntGELU    | n:  14
IntSoftmax | n:  12
IntGELU    | n:  14
IntSoftmax | n:  12
IntGELU    | n:  14
IntSoftmax | n:  12
IntGELU    | n:  14
IntSoftmax | n:  12
IntGELU    | n:  14
IntSoftmax | n:  12
IntGELU    | n:  14
IntSoftmax | n:  12
IntGELU    | n:  14
IntSoftmax | n:  12
IntGELU    | n:  14
IntSoftmax | n:  12
IntGELU    | n:  14
IntSoftmax | n:  12
IntGELU    | n:  14
IntSoftmax | n:  12
IntGELU    | n:  14
.calib done
Test: [  0/391]	Time  3.902 ( 3.902)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=12, intgelu_exp_n=15
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=12, intgelu_exp_n=15, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  12
IntGELU    | n:  15
IntSoftmax | n:  12
IntGELU    | n:  15
IntSoftmax | n:  12
IntGELU    | n:  15
IntSoftmax | n:  12
IntGELU    | n:  15
IntSoftmax | n:  12
IntGELU    | n:  15
IntSoftmax | n:  12
IntGELU    | n:  15
IntSoftmax | n:  12
IntGELU    | n:  15
IntSoftmax | n:  12
IntGELU    | n:  15
IntSoftmax | n:  12
IntGELU    | n:  15
IntSoftmax | n:  12
IntGELU    | n:  15
IntSoftmax | n:  12
IntGELU    | n:  15
IntSoftmax | n:  12
IntGELU    | n:  15
.calib done
Test: [  0/391]	Time  3.955 ( 3.955)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.04
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=12, intgelu_exp_n=16
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=12, intgelu_exp_n=16, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  12
IntGELU    | n:  16
IntSoftmax | n:  12
IntGELU    | n:  16
IntSoftmax | n:  12
IntGELU    | n:  16
IntSoftmax | n:  12
IntGELU    | n:  16
IntSoftmax | n:  12
IntGELU    | n:  16
IntSoftmax | n:  12
IntGELU    | n:  16
IntSoftmax | n:  12
IntGELU    | n:  16
IntSoftmax | n:  12
IntGELU    | n:  16
IntSoftmax | n:  12
IntGELU    | n:  16
IntSoftmax | n:  12
IntGELU    | n:  16
IntSoftmax | n:  12
IntGELU    | n:  16
IntSoftmax | n:  12
IntGELU    | n:  16
.calib done
Test: [  0/391]	Time  3.814 ( 3.814)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.83
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=12, intgelu_exp_n=17
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=12, intgelu_exp_n=17, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  12
IntGELU    | n:  17
IntSoftmax | n:  12
IntGELU    | n:  17
IntSoftmax | n:  12
IntGELU    | n:  17
IntSoftmax | n:  12
IntGELU    | n:  17
IntSoftmax | n:  12
IntGELU    | n:  17
IntSoftmax | n:  12
IntGELU    | n:  17
IntSoftmax | n:  12
IntGELU    | n:  17
IntSoftmax | n:  12
IntGELU    | n:  17
IntSoftmax | n:  12
IntGELU    | n:  17
IntSoftmax | n:  12
IntGELU    | n:  17
IntSoftmax | n:  12
IntGELU    | n:  17
IntSoftmax | n:  12
IntGELU    | n:  17
.calib done
Test: [  0/391]	Time  3.982 ( 3.982)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.05
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=12, intgelu_exp_n=18
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=12, intgelu_exp_n=18, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  12
IntGELU    | n:  18
IntSoftmax | n:  12
IntGELU    | n:  18
IntSoftmax | n:  12
IntGELU    | n:  18
IntSoftmax | n:  12
IntGELU    | n:  18
IntSoftmax | n:  12
IntGELU    | n:  18
IntSoftmax | n:  12
IntGELU    | n:  18
IntSoftmax | n:  12
IntGELU    | n:  18
IntSoftmax | n:  12
IntGELU    | n:  18
IntSoftmax | n:  12
IntGELU    | n:  18
IntSoftmax | n:  12
IntGELU    | n:  18
IntSoftmax | n:  12
IntGELU    | n:  18
IntSoftmax | n:  12
IntGELU    | n:  18
.calib done
Test: [  0/391]	Time  3.978 ( 3.978)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.99
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=12, intgelu_exp_n=19
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=12, intgelu_exp_n=19, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  12
IntGELU    | n:  19
IntSoftmax | n:  12
IntGELU    | n:  19
IntSoftmax | n:  12
IntGELU    | n:  19
IntSoftmax | n:  12
IntGELU    | n:  19
IntSoftmax | n:  12
IntGELU    | n:  19
IntSoftmax | n:  12
IntGELU    | n:  19
IntSoftmax | n:  12
IntGELU    | n:  19
IntSoftmax | n:  12
IntGELU    | n:  19
IntSoftmax | n:  12
IntGELU    | n:  19
IntSoftmax | n:  12
IntGELU    | n:  19
IntSoftmax | n:  12
IntGELU    | n:  19
IntSoftmax | n:  12
IntGELU    | n:  19
.calib done
Test: [  0/391]	Time  3.922 ( 3.922)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=12, intgelu_exp_n=20
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=12, intgelu_exp_n=20, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  12
IntGELU    | n:  20
IntSoftmax | n:  12
IntGELU    | n:  20
IntSoftmax | n:  12
IntGELU    | n:  20
IntSoftmax | n:  12
IntGELU    | n:  20
IntSoftmax | n:  12
IntGELU    | n:  20
IntSoftmax | n:  12
IntGELU    | n:  20
IntSoftmax | n:  12
IntGELU    | n:  20
IntSoftmax | n:  12
IntGELU    | n:  20
IntSoftmax | n:  12
IntGELU    | n:  20
IntSoftmax | n:  12
IntGELU    | n:  20
IntSoftmax | n:  12
IntGELU    | n:  20
IntSoftmax | n:  12
IntGELU    | n:  20
.calib done
Test: [  0/391]	Time  3.837 ( 3.837)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.86
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=12, intgelu_exp_n=21
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=12, intgelu_exp_n=21, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  12
IntGELU    | n:  21
IntSoftmax | n:  12
IntGELU    | n:  21
IntSoftmax | n:  12
IntGELU    | n:  21
IntSoftmax | n:  12
IntGELU    | n:  21
IntSoftmax | n:  12
IntGELU    | n:  21
IntSoftmax | n:  12
IntGELU    | n:  21
IntSoftmax | n:  12
IntGELU    | n:  21
IntSoftmax | n:  12
IntGELU    | n:  21
IntSoftmax | n:  12
IntGELU    | n:  21
IntSoftmax | n:  12
IntGELU    | n:  21
IntSoftmax | n:  12
IntGELU    | n:  21
IntSoftmax | n:  12
IntGELU    | n:  21
.calib done
Test: [  0/391]	Time  3.696 ( 3.696)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.75
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=12, intgelu_exp_n=22
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=12, intgelu_exp_n=22, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  12
IntGELU    | n:  22
IntSoftmax | n:  12
IntGELU    | n:  22
IntSoftmax | n:  12
IntGELU    | n:  22
IntSoftmax | n:  12
IntGELU    | n:  22
IntSoftmax | n:  12
IntGELU    | n:  22
IntSoftmax | n:  12
IntGELU    | n:  22
IntSoftmax | n:  12
IntGELU    | n:  22
IntSoftmax | n:  12
IntGELU    | n:  22
IntSoftmax | n:  12
IntGELU    | n:  22
IntSoftmax | n:  12
IntGELU    | n:  22
IntSoftmax | n:  12
IntGELU    | n:  22
IntSoftmax | n:  12
IntGELU    | n:  22
.calib done
Test: [  0/391]	Time  3.798 ( 3.798)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.82
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=12, intgelu_exp_n=23
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=12, intgelu_exp_n=23, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  12
IntGELU    | n:  23
IntSoftmax | n:  12
IntGELU    | n:  23
IntSoftmax | n:  12
IntGELU    | n:  23
IntSoftmax | n:  12
IntGELU    | n:  23
IntSoftmax | n:  12
IntGELU    | n:  23
IntSoftmax | n:  12
IntGELU    | n:  23
IntSoftmax | n:  12
IntGELU    | n:  23
IntSoftmax | n:  12
IntGELU    | n:  23
IntSoftmax | n:  12
IntGELU    | n:  23
IntSoftmax | n:  12
IntGELU    | n:  23
IntSoftmax | n:  12
IntGELU    | n:  23
IntSoftmax | n:  12
IntGELU    | n:  23
.calib done
Test: [  0/391]	Time  3.953 ( 3.953)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.06
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=12, intgelu_exp_n=24
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=12, intgelu_exp_n=24, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  12
IntGELU    | n:  24
IntSoftmax | n:  12
IntGELU    | n:  24
IntSoftmax | n:  12
IntGELU    | n:  24
IntSoftmax | n:  12
IntGELU    | n:  24
IntSoftmax | n:  12
IntGELU    | n:  24
IntSoftmax | n:  12
IntGELU    | n:  24
IntSoftmax | n:  12
IntGELU    | n:  24
IntSoftmax | n:  12
IntGELU    | n:  24
IntSoftmax | n:  12
IntGELU    | n:  24
IntSoftmax | n:  12
IntGELU    | n:  24
IntSoftmax | n:  12
IntGELU    | n:  24
IntSoftmax | n:  12
IntGELU    | n:  24
.calib done
Test: [  0/391]	Time  3.686 ( 3.686)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.73
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=12, intgelu_exp_n=25
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=12, intgelu_exp_n=25, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  12
IntGELU    | n:  25
IntSoftmax | n:  12
IntGELU    | n:  25
IntSoftmax | n:  12
IntGELU    | n:  25
IntSoftmax | n:  12
IntGELU    | n:  25
IntSoftmax | n:  12
IntGELU    | n:  25
IntSoftmax | n:  12
IntGELU    | n:  25
IntSoftmax | n:  12
IntGELU    | n:  25
IntSoftmax | n:  12
IntGELU    | n:  25
IntSoftmax | n:  12
IntGELU    | n:  25
IntSoftmax | n:  12
IntGELU    | n:  25
IntSoftmax | n:  12
IntGELU    | n:  25
IntSoftmax | n:  12
IntGELU    | n:  25
.calib done
Test: [  0/391]	Time  3.889 ( 3.889)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.92
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=12, intgelu_exp_n=26
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=12, intgelu_exp_n=26, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  12
IntGELU    | n:  26
IntSoftmax | n:  12
IntGELU    | n:  26
IntSoftmax | n:  12
IntGELU    | n:  26
IntSoftmax | n:  12
IntGELU    | n:  26
IntSoftmax | n:  12
IntGELU    | n:  26
IntSoftmax | n:  12
IntGELU    | n:  26
IntSoftmax | n:  12
IntGELU    | n:  26
IntSoftmax | n:  12
IntGELU    | n:  26
IntSoftmax | n:  12
IntGELU    | n:  26
IntSoftmax | n:  12
IntGELU    | n:  26
IntSoftmax | n:  12
IntGELU    | n:  26
IntSoftmax | n:  12
IntGELU    | n:  26
.calib done
Test: [  0/391]	Time  3.933 ( 3.933)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.98
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=12, intgelu_exp_n=27
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=12, intgelu_exp_n=27, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  12
IntGELU    | n:  27
IntSoftmax | n:  12
IntGELU    | n:  27
IntSoftmax | n:  12
IntGELU    | n:  27
IntSoftmax | n:  12
IntGELU    | n:  27
IntSoftmax | n:  12
IntGELU    | n:  27
IntSoftmax | n:  12
IntGELU    | n:  27
IntSoftmax | n:  12
IntGELU    | n:  27
IntSoftmax | n:  12
IntGELU    | n:  27
IntSoftmax | n:  12
IntGELU    | n:  27
IntSoftmax | n:  12
IntGELU    | n:  27
IntSoftmax | n:  12
IntGELU    | n:  27
IntSoftmax | n:  12
IntGELU    | n:  27
.calib done
Test: [  0/391]	Time  3.869 ( 3.869)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.91
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=12, intgelu_exp_n=28
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=12, intgelu_exp_n=28, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  12
IntGELU    | n:  28
IntSoftmax | n:  12
IntGELU    | n:  28
IntSoftmax | n:  12
IntGELU    | n:  28
IntSoftmax | n:  12
IntGELU    | n:  28
IntSoftmax | n:  12
IntGELU    | n:  28
IntSoftmax | n:  12
IntGELU    | n:  28
IntSoftmax | n:  12
IntGELU    | n:  28
IntSoftmax | n:  12
IntGELU    | n:  28
IntSoftmax | n:  12
IntGELU    | n:  28
IntSoftmax | n:  12
IntGELU    | n:  28
IntSoftmax | n:  12
IntGELU    | n:  28
IntSoftmax | n:  12
IntGELU    | n:  28
.calib done
Test: [  0/391]	Time  3.814 ( 3.814)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.79
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=12, intgelu_exp_n=29
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=12, intgelu_exp_n=29, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  12
IntGELU    | n:  29
IntSoftmax | n:  12
IntGELU    | n:  29
IntSoftmax | n:  12
IntGELU    | n:  29
IntSoftmax | n:  12
IntGELU    | n:  29
IntSoftmax | n:  12
IntGELU    | n:  29
IntSoftmax | n:  12
IntGELU    | n:  29
IntSoftmax | n:  12
IntGELU    | n:  29
IntSoftmax | n:  12
IntGELU    | n:  29
IntSoftmax | n:  12
IntGELU    | n:  29
IntSoftmax | n:  12
IntGELU    | n:  29
IntSoftmax | n:  12
IntGELU    | n:  29
IntSoftmax | n:  12
IntGELU    | n:  29
.calib done
Test: [  0/391]	Time  3.878 ( 3.878)	Acc@1   7.03 (  7.03)	Acc@5  17.97 ( 17.97)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 7.031 Prec@5 17.969
Time: 8.92
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=12, intgelu_exp_n=30
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=12, intgelu_exp_n=30, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  12
IntGELU    | n:  30
IntSoftmax | n:  12
IntGELU    | n:  30
IntSoftmax | n:  12
IntGELU    | n:  30
IntSoftmax | n:  12
IntGELU    | n:  30
IntSoftmax | n:  12
IntGELU    | n:  30
IntSoftmax | n:  12
IntGELU    | n:  30
IntSoftmax | n:  12
IntGELU    | n:  30
IntSoftmax | n:  12
IntGELU    | n:  30
IntSoftmax | n:  12
IntGELU    | n:  30
IntSoftmax | n:  12
IntGELU    | n:  30
IntSoftmax | n:  12
IntGELU    | n:  30
IntSoftmax | n:  12
IntGELU    | n:  30
.calib done
Test: [  0/391]	Time  3.882 ( 3.882)	Acc@1   0.78 (  0.78)	Acc@5   6.25 (  6.25)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.781 Prec@5 6.250
Time: 8.91
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=12, intgelu_exp_n=31
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=12, intgelu_exp_n=31, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  12
IntGELU    | n:  31
IntSoftmax | n:  12
IntGELU    | n:  31
IntSoftmax | n:  12
IntGELU    | n:  31
IntSoftmax | n:  12
IntGELU    | n:  31
IntSoftmax | n:  12
IntGELU    | n:  31
IntSoftmax | n:  12
IntGELU    | n:  31
IntSoftmax | n:  12
IntGELU    | n:  31
IntSoftmax | n:  12
IntGELU    | n:  31
IntSoftmax | n:  12
IntGELU    | n:  31
IntSoftmax | n:  12
IntGELU    | n:  31
IntSoftmax | n:  12
IntGELU    | n:  31
IntSoftmax | n:  12
IntGELU    | n:  31
.calib done
Test: [  0/391]	Time  3.916 ( 3.916)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.98
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=13, intgelu_exp_n=0
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=13, intgelu_exp_n=0, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  13
IntGELU    | n:  0
IntSoftmax | n:  13
IntGELU    | n:  0
IntSoftmax | n:  13
IntGELU    | n:  0
IntSoftmax | n:  13
IntGELU    | n:  0
IntSoftmax | n:  13
IntGELU    | n:  0
IntSoftmax | n:  13
IntGELU    | n:  0
IntSoftmax | n:  13
IntGELU    | n:  0
IntSoftmax | n:  13
IntGELU    | n:  0
IntSoftmax | n:  13
IntGELU    | n:  0
IntSoftmax | n:  13
IntGELU    | n:  0
IntSoftmax | n:  13
IntGELU    | n:  0
IntSoftmax | n:  13
IntGELU    | n:  0
.calib done
Test: [  0/391]	Time  3.916 ( 3.916)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.98
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=13, intgelu_exp_n=1
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=13, intgelu_exp_n=1, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  13
IntGELU    | n:  1
IntSoftmax | n:  13
IntGELU    | n:  1
IntSoftmax | n:  13
IntGELU    | n:  1
IntSoftmax | n:  13
IntGELU    | n:  1
IntSoftmax | n:  13
IntGELU    | n:  1
IntSoftmax | n:  13
IntGELU    | n:  1
IntSoftmax | n:  13
IntGELU    | n:  1
IntSoftmax | n:  13
IntGELU    | n:  1
IntSoftmax | n:  13
IntGELU    | n:  1
IntSoftmax | n:  13
IntGELU    | n:  1
IntSoftmax | n:  13
IntGELU    | n:  1
IntSoftmax | n:  13
IntGELU    | n:  1
.calib done
Test: [  0/391]	Time  3.827 ( 3.827)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.87
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=13, intgelu_exp_n=2
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=13, intgelu_exp_n=2, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  13
IntGELU    | n:  2
IntSoftmax | n:  13
IntGELU    | n:  2
IntSoftmax | n:  13
IntGELU    | n:  2
IntSoftmax | n:  13
IntGELU    | n:  2
IntSoftmax | n:  13
IntGELU    | n:  2
IntSoftmax | n:  13
IntGELU    | n:  2
IntSoftmax | n:  13
IntGELU    | n:  2
IntSoftmax | n:  13
IntGELU    | n:  2
IntSoftmax | n:  13
IntGELU    | n:  2
IntSoftmax | n:  13
IntGELU    | n:  2
IntSoftmax | n:  13
IntGELU    | n:  2
IntSoftmax | n:  13
IntGELU    | n:  2
.calib done
Test: [  0/391]	Time  3.801 ( 3.801)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.86
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=13, intgelu_exp_n=3
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=13, intgelu_exp_n=3, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  13
IntGELU    | n:  3
IntSoftmax | n:  13
IntGELU    | n:  3
IntSoftmax | n:  13
IntGELU    | n:  3
IntSoftmax | n:  13
IntGELU    | n:  3
IntSoftmax | n:  13
IntGELU    | n:  3
IntSoftmax | n:  13
IntGELU    | n:  3
IntSoftmax | n:  13
IntGELU    | n:  3
IntSoftmax | n:  13
IntGELU    | n:  3
IntSoftmax | n:  13
IntGELU    | n:  3
IntSoftmax | n:  13
IntGELU    | n:  3
IntSoftmax | n:  13
IntGELU    | n:  3
IntSoftmax | n:  13
IntGELU    | n:  3
.calib done
Test: [  0/391]	Time  3.958 ( 3.958)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.98
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=13, intgelu_exp_n=4
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=13, intgelu_exp_n=4, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  13
IntGELU    | n:  4
IntSoftmax | n:  13
IntGELU    | n:  4
IntSoftmax | n:  13
IntGELU    | n:  4
IntSoftmax | n:  13
IntGELU    | n:  4
IntSoftmax | n:  13
IntGELU    | n:  4
IntSoftmax | n:  13
IntGELU    | n:  4
IntSoftmax | n:  13
IntGELU    | n:  4
IntSoftmax | n:  13
IntGELU    | n:  4
IntSoftmax | n:  13
IntGELU    | n:  4
IntSoftmax | n:  13
IntGELU    | n:  4
IntSoftmax | n:  13
IntGELU    | n:  4
IntSoftmax | n:  13
IntGELU    | n:  4
.calib done
Test: [  0/391]	Time  3.804 ( 3.804)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.78
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=13, intgelu_exp_n=5
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=13, intgelu_exp_n=5, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  13
IntGELU    | n:  5
IntSoftmax | n:  13
IntGELU    | n:  5
IntSoftmax | n:  13
IntGELU    | n:  5
IntSoftmax | n:  13
IntGELU    | n:  5
IntSoftmax | n:  13
IntGELU    | n:  5
IntSoftmax | n:  13
IntGELU    | n:  5
IntSoftmax | n:  13
IntGELU    | n:  5
IntSoftmax | n:  13
IntGELU    | n:  5
IntSoftmax | n:  13
IntGELU    | n:  5
IntSoftmax | n:  13
IntGELU    | n:  5
IntSoftmax | n:  13
IntGELU    | n:  5
IntSoftmax | n:  13
IntGELU    | n:  5
.calib done
Test: [  0/391]	Time  3.967 ( 3.967)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.05
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=13, intgelu_exp_n=6
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=13, intgelu_exp_n=6, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  13
IntGELU    | n:  6
IntSoftmax | n:  13
IntGELU    | n:  6
IntSoftmax | n:  13
IntGELU    | n:  6
IntSoftmax | n:  13
IntGELU    | n:  6
IntSoftmax | n:  13
IntGELU    | n:  6
IntSoftmax | n:  13
IntGELU    | n:  6
IntSoftmax | n:  13
IntGELU    | n:  6
IntSoftmax | n:  13
IntGELU    | n:  6
IntSoftmax | n:  13
IntGELU    | n:  6
IntSoftmax | n:  13
IntGELU    | n:  6
IntSoftmax | n:  13
IntGELU    | n:  6
IntSoftmax | n:  13
IntGELU    | n:  6
.calib done
Test: [  0/391]	Time  3.799 ( 3.799)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.82
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=13, intgelu_exp_n=7
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=13, intgelu_exp_n=7, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  13
IntGELU    | n:  7
IntSoftmax | n:  13
IntGELU    | n:  7
IntSoftmax | n:  13
IntGELU    | n:  7
IntSoftmax | n:  13
IntGELU    | n:  7
IntSoftmax | n:  13
IntGELU    | n:  7
IntSoftmax | n:  13
IntGELU    | n:  7
IntSoftmax | n:  13
IntGELU    | n:  7
IntSoftmax | n:  13
IntGELU    | n:  7
IntSoftmax | n:  13
IntGELU    | n:  7
IntSoftmax | n:  13
IntGELU    | n:  7
IntSoftmax | n:  13
IntGELU    | n:  7
IntSoftmax | n:  13
IntGELU    | n:  7
.calib done
Test: [  0/391]	Time  3.784 ( 3.784)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.73
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=13, intgelu_exp_n=8
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=13, intgelu_exp_n=8, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  13
IntGELU    | n:  8
IntSoftmax | n:  13
IntGELU    | n:  8
IntSoftmax | n:  13
IntGELU    | n:  8
IntSoftmax | n:  13
IntGELU    | n:  8
IntSoftmax | n:  13
IntGELU    | n:  8
IntSoftmax | n:  13
IntGELU    | n:  8
IntSoftmax | n:  13
IntGELU    | n:  8
IntSoftmax | n:  13
IntGELU    | n:  8
IntSoftmax | n:  13
IntGELU    | n:  8
IntSoftmax | n:  13
IntGELU    | n:  8
IntSoftmax | n:  13
IntGELU    | n:  8
IntSoftmax | n:  13
IntGELU    | n:  8
.calib done
Test: [  0/391]	Time  3.793 ( 3.793)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.82
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=13, intgelu_exp_n=9
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=13, intgelu_exp_n=9, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  13
IntGELU    | n:  9
IntSoftmax | n:  13
IntGELU    | n:  9
IntSoftmax | n:  13
IntGELU    | n:  9
IntSoftmax | n:  13
IntGELU    | n:  9
IntSoftmax | n:  13
IntGELU    | n:  9
IntSoftmax | n:  13
IntGELU    | n:  9
IntSoftmax | n:  13
IntGELU    | n:  9
IntSoftmax | n:  13
IntGELU    | n:  9
IntSoftmax | n:  13
IntGELU    | n:  9
IntSoftmax | n:  13
IntGELU    | n:  9
IntSoftmax | n:  13
IntGELU    | n:  9
IntSoftmax | n:  13
IntGELU    | n:  9
.calib done
Test: [  0/391]	Time  3.921 ( 3.921)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.99
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=13, intgelu_exp_n=10
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=13, intgelu_exp_n=10, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  13
IntGELU    | n:  10
IntSoftmax | n:  13
IntGELU    | n:  10
IntSoftmax | n:  13
IntGELU    | n:  10
IntSoftmax | n:  13
IntGELU    | n:  10
IntSoftmax | n:  13
IntGELU    | n:  10
IntSoftmax | n:  13
IntGELU    | n:  10
IntSoftmax | n:  13
IntGELU    | n:  10
IntSoftmax | n:  13
IntGELU    | n:  10
IntSoftmax | n:  13
IntGELU    | n:  10
IntSoftmax | n:  13
IntGELU    | n:  10
IntSoftmax | n:  13
IntGELU    | n:  10
IntSoftmax | n:  13
IntGELU    | n:  10
.calib done
Test: [  0/391]	Time  3.925 ( 3.925)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.00
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=13, intgelu_exp_n=11
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=13, intgelu_exp_n=11, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  13
IntGELU    | n:  11
IntSoftmax | n:  13
IntGELU    | n:  11
IntSoftmax | n:  13
IntGELU    | n:  11
IntSoftmax | n:  13
IntGELU    | n:  11
IntSoftmax | n:  13
IntGELU    | n:  11
IntSoftmax | n:  13
IntGELU    | n:  11
IntSoftmax | n:  13
IntGELU    | n:  11
IntSoftmax | n:  13
IntGELU    | n:  11
IntSoftmax | n:  13
IntGELU    | n:  11
IntSoftmax | n:  13
IntGELU    | n:  11
IntSoftmax | n:  13
IntGELU    | n:  11
IntSoftmax | n:  13
IntGELU    | n:  11
.calib done
Test: [  0/391]	Time  3.741 ( 3.741)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.76
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=13, intgelu_exp_n=12
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=13, intgelu_exp_n=12, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  13
IntGELU    | n:  12
IntSoftmax | n:  13
IntGELU    | n:  12
IntSoftmax | n:  13
IntGELU    | n:  12
IntSoftmax | n:  13
IntGELU    | n:  12
IntSoftmax | n:  13
IntGELU    | n:  12
IntSoftmax | n:  13
IntGELU    | n:  12
IntSoftmax | n:  13
IntGELU    | n:  12
IntSoftmax | n:  13
IntGELU    | n:  12
IntSoftmax | n:  13
IntGELU    | n:  12
IntSoftmax | n:  13
IntGELU    | n:  12
IntSoftmax | n:  13
IntGELU    | n:  12
IntSoftmax | n:  13
IntGELU    | n:  12
.calib done
Test: [  0/391]	Time  3.821 ( 3.821)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.87
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=13, intgelu_exp_n=13
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=13, intgelu_exp_n=13, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  13
IntGELU    | n:  13
IntSoftmax | n:  13
IntGELU    | n:  13
IntSoftmax | n:  13
IntGELU    | n:  13
IntSoftmax | n:  13
IntGELU    | n:  13
IntSoftmax | n:  13
IntGELU    | n:  13
IntSoftmax | n:  13
IntGELU    | n:  13
IntSoftmax | n:  13
IntGELU    | n:  13
IntSoftmax | n:  13
IntGELU    | n:  13
IntSoftmax | n:  13
IntGELU    | n:  13
IntSoftmax | n:  13
IntGELU    | n:  13
IntSoftmax | n:  13
IntGELU    | n:  13
IntSoftmax | n:  13
IntGELU    | n:  13
.calib done
Test: [  0/391]	Time  3.946 ( 3.946)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=13, intgelu_exp_n=14
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=13, intgelu_exp_n=14, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  13
IntGELU    | n:  14
IntSoftmax | n:  13
IntGELU    | n:  14
IntSoftmax | n:  13
IntGELU    | n:  14
IntSoftmax | n:  13
IntGELU    | n:  14
IntSoftmax | n:  13
IntGELU    | n:  14
IntSoftmax | n:  13
IntGELU    | n:  14
IntSoftmax | n:  13
IntGELU    | n:  14
IntSoftmax | n:  13
IntGELU    | n:  14
IntSoftmax | n:  13
IntGELU    | n:  14
IntSoftmax | n:  13
IntGELU    | n:  14
IntSoftmax | n:  13
IntGELU    | n:  14
IntSoftmax | n:  13
IntGELU    | n:  14
.calib done
Test: [  0/391]	Time  3.811 ( 3.811)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.90
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=13, intgelu_exp_n=15
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=13, intgelu_exp_n=15, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  13
IntGELU    | n:  15
IntSoftmax | n:  13
IntGELU    | n:  15
IntSoftmax | n:  13
IntGELU    | n:  15
IntSoftmax | n:  13
IntGELU    | n:  15
IntSoftmax | n:  13
IntGELU    | n:  15
IntSoftmax | n:  13
IntGELU    | n:  15
IntSoftmax | n:  13
IntGELU    | n:  15
IntSoftmax | n:  13
IntGELU    | n:  15
IntSoftmax | n:  13
IntGELU    | n:  15
IntSoftmax | n:  13
IntGELU    | n:  15
IntSoftmax | n:  13
IntGELU    | n:  15
IntSoftmax | n:  13
IntGELU    | n:  15
.calib done
Test: [  0/391]	Time  3.919 ( 3.919)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.98
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=13, intgelu_exp_n=16
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=13, intgelu_exp_n=16, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  13
IntGELU    | n:  16
IntSoftmax | n:  13
IntGELU    | n:  16
IntSoftmax | n:  13
IntGELU    | n:  16
IntSoftmax | n:  13
IntGELU    | n:  16
IntSoftmax | n:  13
IntGELU    | n:  16
IntSoftmax | n:  13
IntGELU    | n:  16
IntSoftmax | n:  13
IntGELU    | n:  16
IntSoftmax | n:  13
IntGELU    | n:  16
IntSoftmax | n:  13
IntGELU    | n:  16
IntSoftmax | n:  13
IntGELU    | n:  16
IntSoftmax | n:  13
IntGELU    | n:  16
IntSoftmax | n:  13
IntGELU    | n:  16
.calib done
Test: [  0/391]	Time  4.036 ( 4.036)	Acc@1   0.00 (  0.00)	Acc@5   0.78 (  0.78)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.781
Time: 9.04
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=13, intgelu_exp_n=17
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=13, intgelu_exp_n=17, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  13
IntGELU    | n:  17
IntSoftmax | n:  13
IntGELU    | n:  17
IntSoftmax | n:  13
IntGELU    | n:  17
IntSoftmax | n:  13
IntGELU    | n:  17
IntSoftmax | n:  13
IntGELU    | n:  17
IntSoftmax | n:  13
IntGELU    | n:  17
IntSoftmax | n:  13
IntGELU    | n:  17
IntSoftmax | n:  13
IntGELU    | n:  17
IntSoftmax | n:  13
IntGELU    | n:  17
IntSoftmax | n:  13
IntGELU    | n:  17
IntSoftmax | n:  13
IntGELU    | n:  17
IntSoftmax | n:  13
IntGELU    | n:  17
.calib done
Test: [  0/391]	Time  3.852 ( 3.852)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.91
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=13, intgelu_exp_n=18
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=13, intgelu_exp_n=18, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  13
IntGELU    | n:  18
IntSoftmax | n:  13
IntGELU    | n:  18
IntSoftmax | n:  13
IntGELU    | n:  18
IntSoftmax | n:  13
IntGELU    | n:  18
IntSoftmax | n:  13
IntGELU    | n:  18
IntSoftmax | n:  13
IntGELU    | n:  18
IntSoftmax | n:  13
IntGELU    | n:  18
IntSoftmax | n:  13
IntGELU    | n:  18
IntSoftmax | n:  13
IntGELU    | n:  18
IntSoftmax | n:  13
IntGELU    | n:  18
IntSoftmax | n:  13
IntGELU    | n:  18
IntSoftmax | n:  13
IntGELU    | n:  18
.calib done
Test: [  0/391]	Time  3.805 ( 3.805)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.87
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=13, intgelu_exp_n=19
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=13, intgelu_exp_n=19, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  13
IntGELU    | n:  19
IntSoftmax | n:  13
IntGELU    | n:  19
IntSoftmax | n:  13
IntGELU    | n:  19
IntSoftmax | n:  13
IntGELU    | n:  19
IntSoftmax | n:  13
IntGELU    | n:  19
IntSoftmax | n:  13
IntGELU    | n:  19
IntSoftmax | n:  13
IntGELU    | n:  19
IntSoftmax | n:  13
IntGELU    | n:  19
IntSoftmax | n:  13
IntGELU    | n:  19
IntSoftmax | n:  13
IntGELU    | n:  19
IntSoftmax | n:  13
IntGELU    | n:  19
IntSoftmax | n:  13
IntGELU    | n:  19
.calib done
Test: [  0/391]	Time  3.788 ( 3.788)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.84
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=13, intgelu_exp_n=20
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=13, intgelu_exp_n=20, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  13
IntGELU    | n:  20
IntSoftmax | n:  13
IntGELU    | n:  20
IntSoftmax | n:  13
IntGELU    | n:  20
IntSoftmax | n:  13
IntGELU    | n:  20
IntSoftmax | n:  13
IntGELU    | n:  20
IntSoftmax | n:  13
IntGELU    | n:  20
IntSoftmax | n:  13
IntGELU    | n:  20
IntSoftmax | n:  13
IntGELU    | n:  20
IntSoftmax | n:  13
IntGELU    | n:  20
IntSoftmax | n:  13
IntGELU    | n:  20
IntSoftmax | n:  13
IntGELU    | n:  20
IntSoftmax | n:  13
IntGELU    | n:  20
.calib done
Test: [  0/391]	Time  3.778 ( 3.778)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.80
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=13, intgelu_exp_n=21
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=13, intgelu_exp_n=21, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  13
IntGELU    | n:  21
IntSoftmax | n:  13
IntGELU    | n:  21
IntSoftmax | n:  13
IntGELU    | n:  21
IntSoftmax | n:  13
IntGELU    | n:  21
IntSoftmax | n:  13
IntGELU    | n:  21
IntSoftmax | n:  13
IntGELU    | n:  21
IntSoftmax | n:  13
IntGELU    | n:  21
IntSoftmax | n:  13
IntGELU    | n:  21
IntSoftmax | n:  13
IntGELU    | n:  21
IntSoftmax | n:  13
IntGELU    | n:  21
IntSoftmax | n:  13
IntGELU    | n:  21
IntSoftmax | n:  13
IntGELU    | n:  21
.calib done
Test: [  0/391]	Time  3.921 ( 3.921)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=13, intgelu_exp_n=22
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=13, intgelu_exp_n=22, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  13
IntGELU    | n:  22
IntSoftmax | n:  13
IntGELU    | n:  22
IntSoftmax | n:  13
IntGELU    | n:  22
IntSoftmax | n:  13
IntGELU    | n:  22
IntSoftmax | n:  13
IntGELU    | n:  22
IntSoftmax | n:  13
IntGELU    | n:  22
IntSoftmax | n:  13
IntGELU    | n:  22
IntSoftmax | n:  13
IntGELU    | n:  22
IntSoftmax | n:  13
IntGELU    | n:  22
IntSoftmax | n:  13
IntGELU    | n:  22
IntSoftmax | n:  13
IntGELU    | n:  22
IntSoftmax | n:  13
IntGELU    | n:  22
.calib done
Test: [  0/391]	Time  3.868 ( 3.868)	Acc@1   1.56 (  1.56)	Acc@5   1.56 (  1.56)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 1.562 Prec@5 1.562
Time: 8.88
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=13, intgelu_exp_n=23
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=13, intgelu_exp_n=23, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  13
IntGELU    | n:  23
IntSoftmax | n:  13
IntGELU    | n:  23
IntSoftmax | n:  13
IntGELU    | n:  23
IntSoftmax | n:  13
IntGELU    | n:  23
IntSoftmax | n:  13
IntGELU    | n:  23
IntSoftmax | n:  13
IntGELU    | n:  23
IntSoftmax | n:  13
IntGELU    | n:  23
IntSoftmax | n:  13
IntGELU    | n:  23
IntSoftmax | n:  13
IntGELU    | n:  23
IntSoftmax | n:  13
IntGELU    | n:  23
IntSoftmax | n:  13
IntGELU    | n:  23
IntSoftmax | n:  13
IntGELU    | n:  23
.calib done
Test: [  0/391]	Time  3.774 ( 3.774)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.80
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=13, intgelu_exp_n=24
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=13, intgelu_exp_n=24, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  13
IntGELU    | n:  24
IntSoftmax | n:  13
IntGELU    | n:  24
IntSoftmax | n:  13
IntGELU    | n:  24
IntSoftmax | n:  13
IntGELU    | n:  24
IntSoftmax | n:  13
IntGELU    | n:  24
IntSoftmax | n:  13
IntGELU    | n:  24
IntSoftmax | n:  13
IntGELU    | n:  24
IntSoftmax | n:  13
IntGELU    | n:  24
IntSoftmax | n:  13
IntGELU    | n:  24
IntSoftmax | n:  13
IntGELU    | n:  24
IntSoftmax | n:  13
IntGELU    | n:  24
IntSoftmax | n:  13
IntGELU    | n:  24
.calib done
Test: [  0/391]	Time  3.704 ( 3.704)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.78
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=13, intgelu_exp_n=25
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=13, intgelu_exp_n=25, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  13
IntGELU    | n:  25
IntSoftmax | n:  13
IntGELU    | n:  25
IntSoftmax | n:  13
IntGELU    | n:  25
IntSoftmax | n:  13
IntGELU    | n:  25
IntSoftmax | n:  13
IntGELU    | n:  25
IntSoftmax | n:  13
IntGELU    | n:  25
IntSoftmax | n:  13
IntGELU    | n:  25
IntSoftmax | n:  13
IntGELU    | n:  25
IntSoftmax | n:  13
IntGELU    | n:  25
IntSoftmax | n:  13
IntGELU    | n:  25
IntSoftmax | n:  13
IntGELU    | n:  25
IntSoftmax | n:  13
IntGELU    | n:  25
.calib done
Test: [  0/391]	Time  3.948 ( 3.948)	Acc@1   0.00 (  0.00)	Acc@5   1.56 (  1.56)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 1.562
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=13, intgelu_exp_n=26
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=13, intgelu_exp_n=26, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  13
IntGELU    | n:  26
IntSoftmax | n:  13
IntGELU    | n:  26
IntSoftmax | n:  13
IntGELU    | n:  26
IntSoftmax | n:  13
IntGELU    | n:  26
IntSoftmax | n:  13
IntGELU    | n:  26
IntSoftmax | n:  13
IntGELU    | n:  26
IntSoftmax | n:  13
IntGELU    | n:  26
IntSoftmax | n:  13
IntGELU    | n:  26
IntSoftmax | n:  13
IntGELU    | n:  26
IntSoftmax | n:  13
IntGELU    | n:  26
IntSoftmax | n:  13
IntGELU    | n:  26
IntSoftmax | n:  13
IntGELU    | n:  26
.calib done
Test: [  0/391]	Time  3.927 ( 3.927)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=13, intgelu_exp_n=27
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=13, intgelu_exp_n=27, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  13
IntGELU    | n:  27
IntSoftmax | n:  13
IntGELU    | n:  27
IntSoftmax | n:  13
IntGELU    | n:  27
IntSoftmax | n:  13
IntGELU    | n:  27
IntSoftmax | n:  13
IntGELU    | n:  27
IntSoftmax | n:  13
IntGELU    | n:  27
IntSoftmax | n:  13
IntGELU    | n:  27
IntSoftmax | n:  13
IntGELU    | n:  27
IntSoftmax | n:  13
IntGELU    | n:  27
IntSoftmax | n:  13
IntGELU    | n:  27
IntSoftmax | n:  13
IntGELU    | n:  27
IntSoftmax | n:  13
IntGELU    | n:  27
.calib done
Test: [  0/391]	Time  3.879 ( 3.879)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.93
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=13, intgelu_exp_n=28
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=13, intgelu_exp_n=28, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  13
IntGELU    | n:  28
IntSoftmax | n:  13
IntGELU    | n:  28
IntSoftmax | n:  13
IntGELU    | n:  28
IntSoftmax | n:  13
IntGELU    | n:  28
IntSoftmax | n:  13
IntGELU    | n:  28
IntSoftmax | n:  13
IntGELU    | n:  28
IntSoftmax | n:  13
IntGELU    | n:  28
IntSoftmax | n:  13
IntGELU    | n:  28
IntSoftmax | n:  13
IntGELU    | n:  28
IntSoftmax | n:  13
IntGELU    | n:  28
IntSoftmax | n:  13
IntGELU    | n:  28
IntSoftmax | n:  13
IntGELU    | n:  28
.calib done
Test: [  0/391]	Time  3.847 ( 3.847)	Acc@1   0.00 (  0.00)	Acc@5   0.78 (  0.78)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.781
Time: 8.92
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=13, intgelu_exp_n=29
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=13, intgelu_exp_n=29, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  13
IntGELU    | n:  29
IntSoftmax | n:  13
IntGELU    | n:  29
IntSoftmax | n:  13
IntGELU    | n:  29
IntSoftmax | n:  13
IntGELU    | n:  29
IntSoftmax | n:  13
IntGELU    | n:  29
IntSoftmax | n:  13
IntGELU    | n:  29
IntSoftmax | n:  13
IntGELU    | n:  29
IntSoftmax | n:  13
IntGELU    | n:  29
IntSoftmax | n:  13
IntGELU    | n:  29
IntSoftmax | n:  13
IntGELU    | n:  29
IntSoftmax | n:  13
IntGELU    | n:  29
IntSoftmax | n:  13
IntGELU    | n:  29
.calib done
Test: [  0/391]	Time  3.972 ( 3.972)	Acc@1   3.12 (  3.12)	Acc@5   5.47 (  5.47)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 3.125 Prec@5 5.469
Time: 8.95
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=13, intgelu_exp_n=30
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=13, intgelu_exp_n=30, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  13
IntGELU    | n:  30
IntSoftmax | n:  13
IntGELU    | n:  30
IntSoftmax | n:  13
IntGELU    | n:  30
IntSoftmax | n:  13
IntGELU    | n:  30
IntSoftmax | n:  13
IntGELU    | n:  30
IntSoftmax | n:  13
IntGELU    | n:  30
IntSoftmax | n:  13
IntGELU    | n:  30
IntSoftmax | n:  13
IntGELU    | n:  30
IntSoftmax | n:  13
IntGELU    | n:  30
IntSoftmax | n:  13
IntGELU    | n:  30
IntSoftmax | n:  13
IntGELU    | n:  30
IntSoftmax | n:  13
IntGELU    | n:  30
.calib done
Test: [  0/391]	Time  3.963 ( 3.963)	Acc@1   0.78 (  0.78)	Acc@5   3.91 (  3.91)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.781 Prec@5 3.906
Time: 9.00
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=13, intgelu_exp_n=31
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=13, intgelu_exp_n=31, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  13
IntGELU    | n:  31
IntSoftmax | n:  13
IntGELU    | n:  31
IntSoftmax | n:  13
IntGELU    | n:  31
IntSoftmax | n:  13
IntGELU    | n:  31
IntSoftmax | n:  13
IntGELU    | n:  31
IntSoftmax | n:  13
IntGELU    | n:  31
IntSoftmax | n:  13
IntGELU    | n:  31
IntSoftmax | n:  13
IntGELU    | n:  31
IntSoftmax | n:  13
IntGELU    | n:  31
IntSoftmax | n:  13
IntGELU    | n:  31
IntSoftmax | n:  13
IntGELU    | n:  31
IntSoftmax | n:  13
IntGELU    | n:  31
.calib done
Test: [  0/391]	Time  3.958 ( 3.958)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.03
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=14, intgelu_exp_n=0
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=14, intgelu_exp_n=0, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  14
IntGELU    | n:  0
IntSoftmax | n:  14
IntGELU    | n:  0
IntSoftmax | n:  14
IntGELU    | n:  0
IntSoftmax | n:  14
IntGELU    | n:  0
IntSoftmax | n:  14
IntGELU    | n:  0
IntSoftmax | n:  14
IntGELU    | n:  0
IntSoftmax | n:  14
IntGELU    | n:  0
IntSoftmax | n:  14
IntGELU    | n:  0
IntSoftmax | n:  14
IntGELU    | n:  0
IntSoftmax | n:  14
IntGELU    | n:  0
IntSoftmax | n:  14
IntGELU    | n:  0
IntSoftmax | n:  14
IntGELU    | n:  0
.calib done
Test: [  0/391]	Time  3.672 ( 3.672)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.74
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=14, intgelu_exp_n=1
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=14, intgelu_exp_n=1, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  14
IntGELU    | n:  1
IntSoftmax | n:  14
IntGELU    | n:  1
IntSoftmax | n:  14
IntGELU    | n:  1
IntSoftmax | n:  14
IntGELU    | n:  1
IntSoftmax | n:  14
IntGELU    | n:  1
IntSoftmax | n:  14
IntGELU    | n:  1
IntSoftmax | n:  14
IntGELU    | n:  1
IntSoftmax | n:  14
IntGELU    | n:  1
IntSoftmax | n:  14
IntGELU    | n:  1
IntSoftmax | n:  14
IntGELU    | n:  1
IntSoftmax | n:  14
IntGELU    | n:  1
IntSoftmax | n:  14
IntGELU    | n:  1
.calib done
Test: [  0/391]	Time  3.615 ( 3.615)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.66
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=14, intgelu_exp_n=2
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=14, intgelu_exp_n=2, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  14
IntGELU    | n:  2
IntSoftmax | n:  14
IntGELU    | n:  2
IntSoftmax | n:  14
IntGELU    | n:  2
IntSoftmax | n:  14
IntGELU    | n:  2
IntSoftmax | n:  14
IntGELU    | n:  2
IntSoftmax | n:  14
IntGELU    | n:  2
IntSoftmax | n:  14
IntGELU    | n:  2
IntSoftmax | n:  14
IntGELU    | n:  2
IntSoftmax | n:  14
IntGELU    | n:  2
IntSoftmax | n:  14
IntGELU    | n:  2
IntSoftmax | n:  14
IntGELU    | n:  2
IntSoftmax | n:  14
IntGELU    | n:  2
.calib done
Test: [  0/391]	Time  3.822 ( 3.822)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.85
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=14, intgelu_exp_n=3
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=14, intgelu_exp_n=3, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  14
IntGELU    | n:  3
IntSoftmax | n:  14
IntGELU    | n:  3
IntSoftmax | n:  14
IntGELU    | n:  3
IntSoftmax | n:  14
IntGELU    | n:  3
IntSoftmax | n:  14
IntGELU    | n:  3
IntSoftmax | n:  14
IntGELU    | n:  3
IntSoftmax | n:  14
IntGELU    | n:  3
IntSoftmax | n:  14
IntGELU    | n:  3
IntSoftmax | n:  14
IntGELU    | n:  3
IntSoftmax | n:  14
IntGELU    | n:  3
IntSoftmax | n:  14
IntGELU    | n:  3
IntSoftmax | n:  14
IntGELU    | n:  3
.calib done
Test: [  0/391]	Time  3.786 ( 3.786)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.76
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=14, intgelu_exp_n=4
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=14, intgelu_exp_n=4, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  14
IntGELU    | n:  4
IntSoftmax | n:  14
IntGELU    | n:  4
IntSoftmax | n:  14
IntGELU    | n:  4
IntSoftmax | n:  14
IntGELU    | n:  4
IntSoftmax | n:  14
IntGELU    | n:  4
IntSoftmax | n:  14
IntGELU    | n:  4
IntSoftmax | n:  14
IntGELU    | n:  4
IntSoftmax | n:  14
IntGELU    | n:  4
IntSoftmax | n:  14
IntGELU    | n:  4
IntSoftmax | n:  14
IntGELU    | n:  4
IntSoftmax | n:  14
IntGELU    | n:  4
IntSoftmax | n:  14
IntGELU    | n:  4
.calib done
Test: [  0/391]	Time  3.936 ( 3.936)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.95
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=14, intgelu_exp_n=5
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=14, intgelu_exp_n=5, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  14
IntGELU    | n:  5
IntSoftmax | n:  14
IntGELU    | n:  5
IntSoftmax | n:  14
IntGELU    | n:  5
IntSoftmax | n:  14
IntGELU    | n:  5
IntSoftmax | n:  14
IntGELU    | n:  5
IntSoftmax | n:  14
IntGELU    | n:  5
IntSoftmax | n:  14
IntGELU    | n:  5
IntSoftmax | n:  14
IntGELU    | n:  5
IntSoftmax | n:  14
IntGELU    | n:  5
IntSoftmax | n:  14
IntGELU    | n:  5
IntSoftmax | n:  14
IntGELU    | n:  5
IntSoftmax | n:  14
IntGELU    | n:  5
.calib done
Test: [  0/391]	Time  3.897 ( 3.897)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.94
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=14, intgelu_exp_n=6
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=14, intgelu_exp_n=6, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  14
IntGELU    | n:  6
IntSoftmax | n:  14
IntGELU    | n:  6
IntSoftmax | n:  14
IntGELU    | n:  6
IntSoftmax | n:  14
IntGELU    | n:  6
IntSoftmax | n:  14
IntGELU    | n:  6
IntSoftmax | n:  14
IntGELU    | n:  6
IntSoftmax | n:  14
IntGELU    | n:  6
IntSoftmax | n:  14
IntGELU    | n:  6
IntSoftmax | n:  14
IntGELU    | n:  6
IntSoftmax | n:  14
IntGELU    | n:  6
IntSoftmax | n:  14
IntGELU    | n:  6
IntSoftmax | n:  14
IntGELU    | n:  6
.calib done
Test: [  0/391]	Time  3.741 ( 3.741)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.74
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=14, intgelu_exp_n=7
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=14, intgelu_exp_n=7, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  14
IntGELU    | n:  7
IntSoftmax | n:  14
IntGELU    | n:  7
IntSoftmax | n:  14
IntGELU    | n:  7
IntSoftmax | n:  14
IntGELU    | n:  7
IntSoftmax | n:  14
IntGELU    | n:  7
IntSoftmax | n:  14
IntGELU    | n:  7
IntSoftmax | n:  14
IntGELU    | n:  7
IntSoftmax | n:  14
IntGELU    | n:  7
IntSoftmax | n:  14
IntGELU    | n:  7
IntSoftmax | n:  14
IntGELU    | n:  7
IntSoftmax | n:  14
IntGELU    | n:  7
IntSoftmax | n:  14
IntGELU    | n:  7
.calib done
Test: [  0/391]	Time  3.948 ( 3.948)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.98
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=14, intgelu_exp_n=8
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=14, intgelu_exp_n=8, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  14
IntGELU    | n:  8
IntSoftmax | n:  14
IntGELU    | n:  8
IntSoftmax | n:  14
IntGELU    | n:  8
IntSoftmax | n:  14
IntGELU    | n:  8
IntSoftmax | n:  14
IntGELU    | n:  8
IntSoftmax | n:  14
IntGELU    | n:  8
IntSoftmax | n:  14
IntGELU    | n:  8
IntSoftmax | n:  14
IntGELU    | n:  8
IntSoftmax | n:  14
IntGELU    | n:  8
IntSoftmax | n:  14
IntGELU    | n:  8
IntSoftmax | n:  14
IntGELU    | n:  8
IntSoftmax | n:  14
IntGELU    | n:  8
.calib done
Test: [  0/391]	Time  4.036 ( 4.036)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.04
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=14, intgelu_exp_n=9
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=14, intgelu_exp_n=9, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  14
IntGELU    | n:  9
IntSoftmax | n:  14
IntGELU    | n:  9
IntSoftmax | n:  14
IntGELU    | n:  9
IntSoftmax | n:  14
IntGELU    | n:  9
IntSoftmax | n:  14
IntGELU    | n:  9
IntSoftmax | n:  14
IntGELU    | n:  9
IntSoftmax | n:  14
IntGELU    | n:  9
IntSoftmax | n:  14
IntGELU    | n:  9
IntSoftmax | n:  14
IntGELU    | n:  9
IntSoftmax | n:  14
IntGELU    | n:  9
IntSoftmax | n:  14
IntGELU    | n:  9
IntSoftmax | n:  14
IntGELU    | n:  9
.calib done
Test: [  0/391]	Time  3.740 ( 3.740)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.86
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=14, intgelu_exp_n=10
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=14, intgelu_exp_n=10, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  14
IntGELU    | n:  10
IntSoftmax | n:  14
IntGELU    | n:  10
IntSoftmax | n:  14
IntGELU    | n:  10
IntSoftmax | n:  14
IntGELU    | n:  10
IntSoftmax | n:  14
IntGELU    | n:  10
IntSoftmax | n:  14
IntGELU    | n:  10
IntSoftmax | n:  14
IntGELU    | n:  10
IntSoftmax | n:  14
IntGELU    | n:  10
IntSoftmax | n:  14
IntGELU    | n:  10
IntSoftmax | n:  14
IntGELU    | n:  10
IntSoftmax | n:  14
IntGELU    | n:  10
IntSoftmax | n:  14
IntGELU    | n:  10
.calib done
Test: [  0/391]	Time  3.860 ( 3.860)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.85
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=14, intgelu_exp_n=11
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=14, intgelu_exp_n=11, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  14
IntGELU    | n:  11
IntSoftmax | n:  14
IntGELU    | n:  11
IntSoftmax | n:  14
IntGELU    | n:  11
IntSoftmax | n:  14
IntGELU    | n:  11
IntSoftmax | n:  14
IntGELU    | n:  11
IntSoftmax | n:  14
IntGELU    | n:  11
IntSoftmax | n:  14
IntGELU    | n:  11
IntSoftmax | n:  14
IntGELU    | n:  11
IntSoftmax | n:  14
IntGELU    | n:  11
IntSoftmax | n:  14
IntGELU    | n:  11
IntSoftmax | n:  14
IntGELU    | n:  11
IntSoftmax | n:  14
IntGELU    | n:  11
.calib done
Test: [  0/391]	Time  3.975 ( 3.975)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.02
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=14, intgelu_exp_n=12
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=14, intgelu_exp_n=12, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  14
IntGELU    | n:  12
IntSoftmax | n:  14
IntGELU    | n:  12
IntSoftmax | n:  14
IntGELU    | n:  12
IntSoftmax | n:  14
IntGELU    | n:  12
IntSoftmax | n:  14
IntGELU    | n:  12
IntSoftmax | n:  14
IntGELU    | n:  12
IntSoftmax | n:  14
IntGELU    | n:  12
IntSoftmax | n:  14
IntGELU    | n:  12
IntSoftmax | n:  14
IntGELU    | n:  12
IntSoftmax | n:  14
IntGELU    | n:  12
IntSoftmax | n:  14
IntGELU    | n:  12
IntSoftmax | n:  14
IntGELU    | n:  12
.calib done
Test: [  0/391]	Time  3.984 ( 3.984)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.99
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=14, intgelu_exp_n=13
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=14, intgelu_exp_n=13, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  14
IntGELU    | n:  13
IntSoftmax | n:  14
IntGELU    | n:  13
IntSoftmax | n:  14
IntGELU    | n:  13
IntSoftmax | n:  14
IntGELU    | n:  13
IntSoftmax | n:  14
IntGELU    | n:  13
IntSoftmax | n:  14
IntGELU    | n:  13
IntSoftmax | n:  14
IntGELU    | n:  13
IntSoftmax | n:  14
IntGELU    | n:  13
IntSoftmax | n:  14
IntGELU    | n:  13
IntSoftmax | n:  14
IntGELU    | n:  13
IntSoftmax | n:  14
IntGELU    | n:  13
IntSoftmax | n:  14
IntGELU    | n:  13
.calib done
Test: [  0/391]	Time  3.931 ( 3.931)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.95
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=14, intgelu_exp_n=14
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=14, intgelu_exp_n=14, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  14
IntGELU    | n:  14
IntSoftmax | n:  14
IntGELU    | n:  14
IntSoftmax | n:  14
IntGELU    | n:  14
IntSoftmax | n:  14
IntGELU    | n:  14
IntSoftmax | n:  14
IntGELU    | n:  14
IntSoftmax | n:  14
IntGELU    | n:  14
IntSoftmax | n:  14
IntGELU    | n:  14
IntSoftmax | n:  14
IntGELU    | n:  14
IntSoftmax | n:  14
IntGELU    | n:  14
IntSoftmax | n:  14
IntGELU    | n:  14
IntSoftmax | n:  14
IntGELU    | n:  14
IntSoftmax | n:  14
IntGELU    | n:  14
.calib done
Test: [  0/391]	Time  3.908 ( 3.908)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.93
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=14, intgelu_exp_n=15
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=14, intgelu_exp_n=15, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  14
IntGELU    | n:  15
IntSoftmax | n:  14
IntGELU    | n:  15
IntSoftmax | n:  14
IntGELU    | n:  15
IntSoftmax | n:  14
IntGELU    | n:  15
IntSoftmax | n:  14
IntGELU    | n:  15
IntSoftmax | n:  14
IntGELU    | n:  15
IntSoftmax | n:  14
IntGELU    | n:  15
IntSoftmax | n:  14
IntGELU    | n:  15
IntSoftmax | n:  14
IntGELU    | n:  15
IntSoftmax | n:  14
IntGELU    | n:  15
IntSoftmax | n:  14
IntGELU    | n:  15
IntSoftmax | n:  14
IntGELU    | n:  15
.calib done
Test: [  0/391]	Time  3.877 ( 3.877)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=14, intgelu_exp_n=16
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=14, intgelu_exp_n=16, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  14
IntGELU    | n:  16
IntSoftmax | n:  14
IntGELU    | n:  16
IntSoftmax | n:  14
IntGELU    | n:  16
IntSoftmax | n:  14
IntGELU    | n:  16
IntSoftmax | n:  14
IntGELU    | n:  16
IntSoftmax | n:  14
IntGELU    | n:  16
IntSoftmax | n:  14
IntGELU    | n:  16
IntSoftmax | n:  14
IntGELU    | n:  16
IntSoftmax | n:  14
IntGELU    | n:  16
IntSoftmax | n:  14
IntGELU    | n:  16
IntSoftmax | n:  14
IntGELU    | n:  16
IntSoftmax | n:  14
IntGELU    | n:  16
.calib done
Test: [  0/391]	Time  3.891 ( 3.891)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.95
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=14, intgelu_exp_n=17
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=14, intgelu_exp_n=17, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  14
IntGELU    | n:  17
IntSoftmax | n:  14
IntGELU    | n:  17
IntSoftmax | n:  14
IntGELU    | n:  17
IntSoftmax | n:  14
IntGELU    | n:  17
IntSoftmax | n:  14
IntGELU    | n:  17
IntSoftmax | n:  14
IntGELU    | n:  17
IntSoftmax | n:  14
IntGELU    | n:  17
IntSoftmax | n:  14
IntGELU    | n:  17
IntSoftmax | n:  14
IntGELU    | n:  17
IntSoftmax | n:  14
IntGELU    | n:  17
IntSoftmax | n:  14
IntGELU    | n:  17
IntSoftmax | n:  14
IntGELU    | n:  17
.calib done
Test: [  0/391]	Time  3.840 ( 3.840)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.89
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=14, intgelu_exp_n=18
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=14, intgelu_exp_n=18, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  14
IntGELU    | n:  18
IntSoftmax | n:  14
IntGELU    | n:  18
IntSoftmax | n:  14
IntGELU    | n:  18
IntSoftmax | n:  14
IntGELU    | n:  18
IntSoftmax | n:  14
IntGELU    | n:  18
IntSoftmax | n:  14
IntGELU    | n:  18
IntSoftmax | n:  14
IntGELU    | n:  18
IntSoftmax | n:  14
IntGELU    | n:  18
IntSoftmax | n:  14
IntGELU    | n:  18
IntSoftmax | n:  14
IntGELU    | n:  18
IntSoftmax | n:  14
IntGELU    | n:  18
IntSoftmax | n:  14
IntGELU    | n:  18
.calib done
Test: [  0/391]	Time  4.056 ( 4.056)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.06
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=14, intgelu_exp_n=19
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=14, intgelu_exp_n=19, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  14
IntGELU    | n:  19
IntSoftmax | n:  14
IntGELU    | n:  19
IntSoftmax | n:  14
IntGELU    | n:  19
IntSoftmax | n:  14
IntGELU    | n:  19
IntSoftmax | n:  14
IntGELU    | n:  19
IntSoftmax | n:  14
IntGELU    | n:  19
IntSoftmax | n:  14
IntGELU    | n:  19
IntSoftmax | n:  14
IntGELU    | n:  19
IntSoftmax | n:  14
IntGELU    | n:  19
IntSoftmax | n:  14
IntGELU    | n:  19
IntSoftmax | n:  14
IntGELU    | n:  19
IntSoftmax | n:  14
IntGELU    | n:  19
.calib done
Test: [  0/391]	Time  3.892 ( 3.892)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.90
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=14, intgelu_exp_n=20
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=14, intgelu_exp_n=20, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  14
IntGELU    | n:  20
IntSoftmax | n:  14
IntGELU    | n:  20
IntSoftmax | n:  14
IntGELU    | n:  20
IntSoftmax | n:  14
IntGELU    | n:  20
IntSoftmax | n:  14
IntGELU    | n:  20
IntSoftmax | n:  14
IntGELU    | n:  20
IntSoftmax | n:  14
IntGELU    | n:  20
IntSoftmax | n:  14
IntGELU    | n:  20
IntSoftmax | n:  14
IntGELU    | n:  20
IntSoftmax | n:  14
IntGELU    | n:  20
IntSoftmax | n:  14
IntGELU    | n:  20
IntSoftmax | n:  14
IntGELU    | n:  20
.calib done
Test: [  0/391]	Time  3.765 ( 3.765)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.79
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=14, intgelu_exp_n=21
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=14, intgelu_exp_n=21, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  14
IntGELU    | n:  21
IntSoftmax | n:  14
IntGELU    | n:  21
IntSoftmax | n:  14
IntGELU    | n:  21
IntSoftmax | n:  14
IntGELU    | n:  21
IntSoftmax | n:  14
IntGELU    | n:  21
IntSoftmax | n:  14
IntGELU    | n:  21
IntSoftmax | n:  14
IntGELU    | n:  21
IntSoftmax | n:  14
IntGELU    | n:  21
IntSoftmax | n:  14
IntGELU    | n:  21
IntSoftmax | n:  14
IntGELU    | n:  21
IntSoftmax | n:  14
IntGELU    | n:  21
IntSoftmax | n:  14
IntGELU    | n:  21
.calib done
Test: [  0/391]	Time  3.849 ( 3.849)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.92
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=14, intgelu_exp_n=22
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=14, intgelu_exp_n=22, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  14
IntGELU    | n:  22
IntSoftmax | n:  14
IntGELU    | n:  22
IntSoftmax | n:  14
IntGELU    | n:  22
IntSoftmax | n:  14
IntGELU    | n:  22
IntSoftmax | n:  14
IntGELU    | n:  22
IntSoftmax | n:  14
IntGELU    | n:  22
IntSoftmax | n:  14
IntGELU    | n:  22
IntSoftmax | n:  14
IntGELU    | n:  22
IntSoftmax | n:  14
IntGELU    | n:  22
IntSoftmax | n:  14
IntGELU    | n:  22
IntSoftmax | n:  14
IntGELU    | n:  22
IntSoftmax | n:  14
IntGELU    | n:  22
.calib done
Test: [  0/391]	Time  3.842 ( 3.842)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.86
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=14, intgelu_exp_n=23
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=14, intgelu_exp_n=23, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  14
IntGELU    | n:  23
IntSoftmax | n:  14
IntGELU    | n:  23
IntSoftmax | n:  14
IntGELU    | n:  23
IntSoftmax | n:  14
IntGELU    | n:  23
IntSoftmax | n:  14
IntGELU    | n:  23
IntSoftmax | n:  14
IntGELU    | n:  23
IntSoftmax | n:  14
IntGELU    | n:  23
IntSoftmax | n:  14
IntGELU    | n:  23
IntSoftmax | n:  14
IntGELU    | n:  23
IntSoftmax | n:  14
IntGELU    | n:  23
IntSoftmax | n:  14
IntGELU    | n:  23
IntSoftmax | n:  14
IntGELU    | n:  23
.calib done
Test: [  0/391]	Time  3.721 ( 3.721)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.75
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=14, intgelu_exp_n=24
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=14, intgelu_exp_n=24, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  14
IntGELU    | n:  24
IntSoftmax | n:  14
IntGELU    | n:  24
IntSoftmax | n:  14
IntGELU    | n:  24
IntSoftmax | n:  14
IntGELU    | n:  24
IntSoftmax | n:  14
IntGELU    | n:  24
IntSoftmax | n:  14
IntGELU    | n:  24
IntSoftmax | n:  14
IntGELU    | n:  24
IntSoftmax | n:  14
IntGELU    | n:  24
IntSoftmax | n:  14
IntGELU    | n:  24
IntSoftmax | n:  14
IntGELU    | n:  24
IntSoftmax | n:  14
IntGELU    | n:  24
IntSoftmax | n:  14
IntGELU    | n:  24
.calib done
Test: [  0/391]	Time  3.871 ( 3.871)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.89
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=14, intgelu_exp_n=25
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=14, intgelu_exp_n=25, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  14
IntGELU    | n:  25
IntSoftmax | n:  14
IntGELU    | n:  25
IntSoftmax | n:  14
IntGELU    | n:  25
IntSoftmax | n:  14
IntGELU    | n:  25
IntSoftmax | n:  14
IntGELU    | n:  25
IntSoftmax | n:  14
IntGELU    | n:  25
IntSoftmax | n:  14
IntGELU    | n:  25
IntSoftmax | n:  14
IntGELU    | n:  25
IntSoftmax | n:  14
IntGELU    | n:  25
IntSoftmax | n:  14
IntGELU    | n:  25
IntSoftmax | n:  14
IntGELU    | n:  25
IntSoftmax | n:  14
IntGELU    | n:  25
.calib done
Test: [  0/391]	Time  3.812 ( 3.812)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.92
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=14, intgelu_exp_n=26
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=14, intgelu_exp_n=26, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  14
IntGELU    | n:  26
IntSoftmax | n:  14
IntGELU    | n:  26
IntSoftmax | n:  14
IntGELU    | n:  26
IntSoftmax | n:  14
IntGELU    | n:  26
IntSoftmax | n:  14
IntGELU    | n:  26
IntSoftmax | n:  14
IntGELU    | n:  26
IntSoftmax | n:  14
IntGELU    | n:  26
IntSoftmax | n:  14
IntGELU    | n:  26
IntSoftmax | n:  14
IntGELU    | n:  26
IntSoftmax | n:  14
IntGELU    | n:  26
IntSoftmax | n:  14
IntGELU    | n:  26
IntSoftmax | n:  14
IntGELU    | n:  26
.calib done
Test: [  0/391]	Time  3.874 ( 3.874)	Acc@1   0.00 (  0.00)	Acc@5   0.78 (  0.78)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.781
Time: 8.93
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=14, intgelu_exp_n=27
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=14, intgelu_exp_n=27, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  14
IntGELU    | n:  27
IntSoftmax | n:  14
IntGELU    | n:  27
IntSoftmax | n:  14
IntGELU    | n:  27
IntSoftmax | n:  14
IntGELU    | n:  27
IntSoftmax | n:  14
IntGELU    | n:  27
IntSoftmax | n:  14
IntGELU    | n:  27
IntSoftmax | n:  14
IntGELU    | n:  27
IntSoftmax | n:  14
IntGELU    | n:  27
IntSoftmax | n:  14
IntGELU    | n:  27
IntSoftmax | n:  14
IntGELU    | n:  27
IntSoftmax | n:  14
IntGELU    | n:  27
IntSoftmax | n:  14
IntGELU    | n:  27
.calib done
Test: [  0/391]	Time  3.772 ( 3.772)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.75
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=14, intgelu_exp_n=28
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=14, intgelu_exp_n=28, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  14
IntGELU    | n:  28
IntSoftmax | n:  14
IntGELU    | n:  28
IntSoftmax | n:  14
IntGELU    | n:  28
IntSoftmax | n:  14
IntGELU    | n:  28
IntSoftmax | n:  14
IntGELU    | n:  28
IntSoftmax | n:  14
IntGELU    | n:  28
IntSoftmax | n:  14
IntGELU    | n:  28
IntSoftmax | n:  14
IntGELU    | n:  28
IntSoftmax | n:  14
IntGELU    | n:  28
IntSoftmax | n:  14
IntGELU    | n:  28
IntSoftmax | n:  14
IntGELU    | n:  28
IntSoftmax | n:  14
IntGELU    | n:  28
.calib done
Test: [  0/391]	Time  3.829 ( 3.829)	Acc@1   0.00 (  0.00)	Acc@5   0.78 (  0.78)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.781
Time: 8.87
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=14, intgelu_exp_n=29
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=14, intgelu_exp_n=29, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  14
IntGELU    | n:  29
IntSoftmax | n:  14
IntGELU    | n:  29
IntSoftmax | n:  14
IntGELU    | n:  29
IntSoftmax | n:  14
IntGELU    | n:  29
IntSoftmax | n:  14
IntGELU    | n:  29
IntSoftmax | n:  14
IntGELU    | n:  29
IntSoftmax | n:  14
IntGELU    | n:  29
IntSoftmax | n:  14
IntGELU    | n:  29
IntSoftmax | n:  14
IntGELU    | n:  29
IntSoftmax | n:  14
IntGELU    | n:  29
IntSoftmax | n:  14
IntGELU    | n:  29
IntSoftmax | n:  14
IntGELU    | n:  29
.calib done
Test: [  0/391]	Time  3.742 ( 3.742)	Acc@1   6.25 (  6.25)	Acc@5  19.53 ( 19.53)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 6.250 Prec@5 19.531
Time: 8.80
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=14, intgelu_exp_n=30
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=14, intgelu_exp_n=30, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  14
IntGELU    | n:  30
IntSoftmax | n:  14
IntGELU    | n:  30
IntSoftmax | n:  14
IntGELU    | n:  30
IntSoftmax | n:  14
IntGELU    | n:  30
IntSoftmax | n:  14
IntGELU    | n:  30
IntSoftmax | n:  14
IntGELU    | n:  30
IntSoftmax | n:  14
IntGELU    | n:  30
IntSoftmax | n:  14
IntGELU    | n:  30
IntSoftmax | n:  14
IntGELU    | n:  30
IntSoftmax | n:  14
IntGELU    | n:  30
IntSoftmax | n:  14
IntGELU    | n:  30
IntSoftmax | n:  14
IntGELU    | n:  30
.calib done
Test: [  0/391]	Time  3.867 ( 3.867)	Acc@1   0.78 (  0.78)	Acc@5   4.69 (  4.69)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.781 Prec@5 4.688
Time: 8.91
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=14, intgelu_exp_n=31
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=14, intgelu_exp_n=31, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  14
IntGELU    | n:  31
IntSoftmax | n:  14
IntGELU    | n:  31
IntSoftmax | n:  14
IntGELU    | n:  31
IntSoftmax | n:  14
IntGELU    | n:  31
IntSoftmax | n:  14
IntGELU    | n:  31
IntSoftmax | n:  14
IntGELU    | n:  31
IntSoftmax | n:  14
IntGELU    | n:  31
IntSoftmax | n:  14
IntGELU    | n:  31
IntSoftmax | n:  14
IntGELU    | n:  31
IntSoftmax | n:  14
IntGELU    | n:  31
IntSoftmax | n:  14
IntGELU    | n:  31
IntSoftmax | n:  14
IntGELU    | n:  31
.calib done
Test: [  0/391]	Time  3.889 ( 3.889)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.95
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=15, intgelu_exp_n=0
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=15, intgelu_exp_n=0, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  15
IntGELU    | n:  0
IntSoftmax | n:  15
IntGELU    | n:  0
IntSoftmax | n:  15
IntGELU    | n:  0
IntSoftmax | n:  15
IntGELU    | n:  0
IntSoftmax | n:  15
IntGELU    | n:  0
IntSoftmax | n:  15
IntGELU    | n:  0
IntSoftmax | n:  15
IntGELU    | n:  0
IntSoftmax | n:  15
IntGELU    | n:  0
IntSoftmax | n:  15
IntGELU    | n:  0
IntSoftmax | n:  15
IntGELU    | n:  0
IntSoftmax | n:  15
IntGELU    | n:  0
IntSoftmax | n:  15
IntGELU    | n:  0
.calib done
Test: [  0/391]	Time  3.994 ( 3.994)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.99
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=15, intgelu_exp_n=1
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=15, intgelu_exp_n=1, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  15
IntGELU    | n:  1
IntSoftmax | n:  15
IntGELU    | n:  1
IntSoftmax | n:  15
IntGELU    | n:  1
IntSoftmax | n:  15
IntGELU    | n:  1
IntSoftmax | n:  15
IntGELU    | n:  1
IntSoftmax | n:  15
IntGELU    | n:  1
IntSoftmax | n:  15
IntGELU    | n:  1
IntSoftmax | n:  15
IntGELU    | n:  1
IntSoftmax | n:  15
IntGELU    | n:  1
IntSoftmax | n:  15
IntGELU    | n:  1
IntSoftmax | n:  15
IntGELU    | n:  1
IntSoftmax | n:  15
IntGELU    | n:  1
.calib done
Test: [  0/391]	Time  3.989 ( 3.989)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.06
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=15, intgelu_exp_n=2
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=15, intgelu_exp_n=2, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  15
IntGELU    | n:  2
IntSoftmax | n:  15
IntGELU    | n:  2
IntSoftmax | n:  15
IntGELU    | n:  2
IntSoftmax | n:  15
IntGELU    | n:  2
IntSoftmax | n:  15
IntGELU    | n:  2
IntSoftmax | n:  15
IntGELU    | n:  2
IntSoftmax | n:  15
IntGELU    | n:  2
IntSoftmax | n:  15
IntGELU    | n:  2
IntSoftmax | n:  15
IntGELU    | n:  2
IntSoftmax | n:  15
IntGELU    | n:  2
IntSoftmax | n:  15
IntGELU    | n:  2
IntSoftmax | n:  15
IntGELU    | n:  2
.calib done
Test: [  0/391]	Time  3.824 ( 3.824)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.89
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=15, intgelu_exp_n=3
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=15, intgelu_exp_n=3, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  15
IntGELU    | n:  3
IntSoftmax | n:  15
IntGELU    | n:  3
IntSoftmax | n:  15
IntGELU    | n:  3
IntSoftmax | n:  15
IntGELU    | n:  3
IntSoftmax | n:  15
IntGELU    | n:  3
IntSoftmax | n:  15
IntGELU    | n:  3
IntSoftmax | n:  15
IntGELU    | n:  3
IntSoftmax | n:  15
IntGELU    | n:  3
IntSoftmax | n:  15
IntGELU    | n:  3
IntSoftmax | n:  15
IntGELU    | n:  3
IntSoftmax | n:  15
IntGELU    | n:  3
IntSoftmax | n:  15
IntGELU    | n:  3
.calib done
Test: [  0/391]	Time  3.883 ( 3.883)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.92
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=15, intgelu_exp_n=4
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=15, intgelu_exp_n=4, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  15
IntGELU    | n:  4
IntSoftmax | n:  15
IntGELU    | n:  4
IntSoftmax | n:  15
IntGELU    | n:  4
IntSoftmax | n:  15
IntGELU    | n:  4
IntSoftmax | n:  15
IntGELU    | n:  4
IntSoftmax | n:  15
IntGELU    | n:  4
IntSoftmax | n:  15
IntGELU    | n:  4
IntSoftmax | n:  15
IntGELU    | n:  4
IntSoftmax | n:  15
IntGELU    | n:  4
IntSoftmax | n:  15
IntGELU    | n:  4
IntSoftmax | n:  15
IntGELU    | n:  4
IntSoftmax | n:  15
IntGELU    | n:  4
.calib done
Test: [  0/391]	Time  4.017 ( 4.017)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.10
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=15, intgelu_exp_n=5
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=15, intgelu_exp_n=5, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  15
IntGELU    | n:  5
IntSoftmax | n:  15
IntGELU    | n:  5
IntSoftmax | n:  15
IntGELU    | n:  5
IntSoftmax | n:  15
IntGELU    | n:  5
IntSoftmax | n:  15
IntGELU    | n:  5
IntSoftmax | n:  15
IntGELU    | n:  5
IntSoftmax | n:  15
IntGELU    | n:  5
IntSoftmax | n:  15
IntGELU    | n:  5
IntSoftmax | n:  15
IntGELU    | n:  5
IntSoftmax | n:  15
IntGELU    | n:  5
IntSoftmax | n:  15
IntGELU    | n:  5
IntSoftmax | n:  15
IntGELU    | n:  5
.calib done
Test: [  0/391]	Time  3.670 ( 3.670)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.75
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=15, intgelu_exp_n=6
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=15, intgelu_exp_n=6, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  15
IntGELU    | n:  6
IntSoftmax | n:  15
IntGELU    | n:  6
IntSoftmax | n:  15
IntGELU    | n:  6
IntSoftmax | n:  15
IntGELU    | n:  6
IntSoftmax | n:  15
IntGELU    | n:  6
IntSoftmax | n:  15
IntGELU    | n:  6
IntSoftmax | n:  15
IntGELU    | n:  6
IntSoftmax | n:  15
IntGELU    | n:  6
IntSoftmax | n:  15
IntGELU    | n:  6
IntSoftmax | n:  15
IntGELU    | n:  6
IntSoftmax | n:  15
IntGELU    | n:  6
IntSoftmax | n:  15
IntGELU    | n:  6
.calib done
Test: [  0/391]	Time  3.892 ( 3.892)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.91
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=15, intgelu_exp_n=7
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=15, intgelu_exp_n=7, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  15
IntGELU    | n:  7
IntSoftmax | n:  15
IntGELU    | n:  7
IntSoftmax | n:  15
IntGELU    | n:  7
IntSoftmax | n:  15
IntGELU    | n:  7
IntSoftmax | n:  15
IntGELU    | n:  7
IntSoftmax | n:  15
IntGELU    | n:  7
IntSoftmax | n:  15
IntGELU    | n:  7
IntSoftmax | n:  15
IntGELU    | n:  7
IntSoftmax | n:  15
IntGELU    | n:  7
IntSoftmax | n:  15
IntGELU    | n:  7
IntSoftmax | n:  15
IntGELU    | n:  7
IntSoftmax | n:  15
IntGELU    | n:  7
.calib done
Test: [  0/391]	Time  3.759 ( 3.759)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.84
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=15, intgelu_exp_n=8
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=15, intgelu_exp_n=8, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  15
IntGELU    | n:  8
IntSoftmax | n:  15
IntGELU    | n:  8
IntSoftmax | n:  15
IntGELU    | n:  8
IntSoftmax | n:  15
IntGELU    | n:  8
IntSoftmax | n:  15
IntGELU    | n:  8
IntSoftmax | n:  15
IntGELU    | n:  8
IntSoftmax | n:  15
IntGELU    | n:  8
IntSoftmax | n:  15
IntGELU    | n:  8
IntSoftmax | n:  15
IntGELU    | n:  8
IntSoftmax | n:  15
IntGELU    | n:  8
IntSoftmax | n:  15
IntGELU    | n:  8
IntSoftmax | n:  15
IntGELU    | n:  8
.calib done
Test: [  0/391]	Time  3.655 ( 3.655)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.67
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=15, intgelu_exp_n=9
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=15, intgelu_exp_n=9, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  15
IntGELU    | n:  9
IntSoftmax | n:  15
IntGELU    | n:  9
IntSoftmax | n:  15
IntGELU    | n:  9
IntSoftmax | n:  15
IntGELU    | n:  9
IntSoftmax | n:  15
IntGELU    | n:  9
IntSoftmax | n:  15
IntGELU    | n:  9
IntSoftmax | n:  15
IntGELU    | n:  9
IntSoftmax | n:  15
IntGELU    | n:  9
IntSoftmax | n:  15
IntGELU    | n:  9
IntSoftmax | n:  15
IntGELU    | n:  9
IntSoftmax | n:  15
IntGELU    | n:  9
IntSoftmax | n:  15
IntGELU    | n:  9
.calib done
Test: [  0/391]	Time  3.910 ( 3.910)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.98
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=15, intgelu_exp_n=10
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=15, intgelu_exp_n=10, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  15
IntGELU    | n:  10
IntSoftmax | n:  15
IntGELU    | n:  10
IntSoftmax | n:  15
IntGELU    | n:  10
IntSoftmax | n:  15
IntGELU    | n:  10
IntSoftmax | n:  15
IntGELU    | n:  10
IntSoftmax | n:  15
IntGELU    | n:  10
IntSoftmax | n:  15
IntGELU    | n:  10
IntSoftmax | n:  15
IntGELU    | n:  10
IntSoftmax | n:  15
IntGELU    | n:  10
IntSoftmax | n:  15
IntGELU    | n:  10
IntSoftmax | n:  15
IntGELU    | n:  10
IntSoftmax | n:  15
IntGELU    | n:  10
.calib done
Test: [  0/391]	Time  3.820 ( 3.820)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.86
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=15, intgelu_exp_n=11
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=15, intgelu_exp_n=11, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  15
IntGELU    | n:  11
IntSoftmax | n:  15
IntGELU    | n:  11
IntSoftmax | n:  15
IntGELU    | n:  11
IntSoftmax | n:  15
IntGELU    | n:  11
IntSoftmax | n:  15
IntGELU    | n:  11
IntSoftmax | n:  15
IntGELU    | n:  11
IntSoftmax | n:  15
IntGELU    | n:  11
IntSoftmax | n:  15
IntGELU    | n:  11
IntSoftmax | n:  15
IntGELU    | n:  11
IntSoftmax | n:  15
IntGELU    | n:  11
IntSoftmax | n:  15
IntGELU    | n:  11
IntSoftmax | n:  15
IntGELU    | n:  11
.calib done
Test: [  0/391]	Time  3.937 ( 3.937)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.00
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=15, intgelu_exp_n=12
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=15, intgelu_exp_n=12, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  15
IntGELU    | n:  12
IntSoftmax | n:  15
IntGELU    | n:  12
IntSoftmax | n:  15
IntGELU    | n:  12
IntSoftmax | n:  15
IntGELU    | n:  12
IntSoftmax | n:  15
IntGELU    | n:  12
IntSoftmax | n:  15
IntGELU    | n:  12
IntSoftmax | n:  15
IntGELU    | n:  12
IntSoftmax | n:  15
IntGELU    | n:  12
IntSoftmax | n:  15
IntGELU    | n:  12
IntSoftmax | n:  15
IntGELU    | n:  12
IntSoftmax | n:  15
IntGELU    | n:  12
IntSoftmax | n:  15
IntGELU    | n:  12
.calib done
Test: [  0/391]	Time  3.810 ( 3.810)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.80
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=15, intgelu_exp_n=13
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=15, intgelu_exp_n=13, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  15
IntGELU    | n:  13
IntSoftmax | n:  15
IntGELU    | n:  13
IntSoftmax | n:  15
IntGELU    | n:  13
IntSoftmax | n:  15
IntGELU    | n:  13
IntSoftmax | n:  15
IntGELU    | n:  13
IntSoftmax | n:  15
IntGELU    | n:  13
IntSoftmax | n:  15
IntGELU    | n:  13
IntSoftmax | n:  15
IntGELU    | n:  13
IntSoftmax | n:  15
IntGELU    | n:  13
IntSoftmax | n:  15
IntGELU    | n:  13
IntSoftmax | n:  15
IntGELU    | n:  13
IntSoftmax | n:  15
IntGELU    | n:  13
.calib done
Test: [  0/391]	Time  3.927 ( 3.927)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=15, intgelu_exp_n=14
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=15, intgelu_exp_n=14, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  15
IntGELU    | n:  14
IntSoftmax | n:  15
IntGELU    | n:  14
IntSoftmax | n:  15
IntGELU    | n:  14
IntSoftmax | n:  15
IntGELU    | n:  14
IntSoftmax | n:  15
IntGELU    | n:  14
IntSoftmax | n:  15
IntGELU    | n:  14
IntSoftmax | n:  15
IntGELU    | n:  14
IntSoftmax | n:  15
IntGELU    | n:  14
IntSoftmax | n:  15
IntGELU    | n:  14
IntSoftmax | n:  15
IntGELU    | n:  14
IntSoftmax | n:  15
IntGELU    | n:  14
IntSoftmax | n:  15
IntGELU    | n:  14
.calib done
Test: [  0/391]	Time  3.886 ( 3.886)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=15, intgelu_exp_n=15
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=15, intgelu_exp_n=15, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  15
IntGELU    | n:  15
IntSoftmax | n:  15
IntGELU    | n:  15
IntSoftmax | n:  15
IntGELU    | n:  15
IntSoftmax | n:  15
IntGELU    | n:  15
IntSoftmax | n:  15
IntGELU    | n:  15
IntSoftmax | n:  15
IntGELU    | n:  15
IntSoftmax | n:  15
IntGELU    | n:  15
IntSoftmax | n:  15
IntGELU    | n:  15
IntSoftmax | n:  15
IntGELU    | n:  15
IntSoftmax | n:  15
IntGELU    | n:  15
IntSoftmax | n:  15
IntGELU    | n:  15
IntSoftmax | n:  15
IntGELU    | n:  15
.calib done
Test: [  0/391]	Time  3.875 ( 3.875)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.94
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=15, intgelu_exp_n=16
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=15, intgelu_exp_n=16, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  15
IntGELU    | n:  16
IntSoftmax | n:  15
IntGELU    | n:  16
IntSoftmax | n:  15
IntGELU    | n:  16
IntSoftmax | n:  15
IntGELU    | n:  16
IntSoftmax | n:  15
IntGELU    | n:  16
IntSoftmax | n:  15
IntGELU    | n:  16
IntSoftmax | n:  15
IntGELU    | n:  16
IntSoftmax | n:  15
IntGELU    | n:  16
IntSoftmax | n:  15
IntGELU    | n:  16
IntSoftmax | n:  15
IntGELU    | n:  16
IntSoftmax | n:  15
IntGELU    | n:  16
IntSoftmax | n:  15
IntGELU    | n:  16
.calib done
Test: [  0/391]	Time  3.645 ( 3.645)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.71
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=15, intgelu_exp_n=17
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=15, intgelu_exp_n=17, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  15
IntGELU    | n:  17
IntSoftmax | n:  15
IntGELU    | n:  17
IntSoftmax | n:  15
IntGELU    | n:  17
IntSoftmax | n:  15
IntGELU    | n:  17
IntSoftmax | n:  15
IntGELU    | n:  17
IntSoftmax | n:  15
IntGELU    | n:  17
IntSoftmax | n:  15
IntGELU    | n:  17
IntSoftmax | n:  15
IntGELU    | n:  17
IntSoftmax | n:  15
IntGELU    | n:  17
IntSoftmax | n:  15
IntGELU    | n:  17
IntSoftmax | n:  15
IntGELU    | n:  17
IntSoftmax | n:  15
IntGELU    | n:  17
.calib done
Test: [  0/391]	Time  3.896 ( 3.896)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=15, intgelu_exp_n=18
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=15, intgelu_exp_n=18, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  15
IntGELU    | n:  18
IntSoftmax | n:  15
IntGELU    | n:  18
IntSoftmax | n:  15
IntGELU    | n:  18
IntSoftmax | n:  15
IntGELU    | n:  18
IntSoftmax | n:  15
IntGELU    | n:  18
IntSoftmax | n:  15
IntGELU    | n:  18
IntSoftmax | n:  15
IntGELU    | n:  18
IntSoftmax | n:  15
IntGELU    | n:  18
IntSoftmax | n:  15
IntGELU    | n:  18
IntSoftmax | n:  15
IntGELU    | n:  18
IntSoftmax | n:  15
IntGELU    | n:  18
IntSoftmax | n:  15
IntGELU    | n:  18
.calib done
Test: [  0/391]	Time  3.866 ( 3.866)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.89
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=15, intgelu_exp_n=19
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=15, intgelu_exp_n=19, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  15
IntGELU    | n:  19
IntSoftmax | n:  15
IntGELU    | n:  19
IntSoftmax | n:  15
IntGELU    | n:  19
IntSoftmax | n:  15
IntGELU    | n:  19
IntSoftmax | n:  15
IntGELU    | n:  19
IntSoftmax | n:  15
IntGELU    | n:  19
IntSoftmax | n:  15
IntGELU    | n:  19
IntSoftmax | n:  15
IntGELU    | n:  19
IntSoftmax | n:  15
IntGELU    | n:  19
IntSoftmax | n:  15
IntGELU    | n:  19
IntSoftmax | n:  15
IntGELU    | n:  19
IntSoftmax | n:  15
IntGELU    | n:  19
.calib done
Test: [  0/391]	Time  3.861 ( 3.861)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.86
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=15, intgelu_exp_n=20
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=15, intgelu_exp_n=20, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  15
IntGELU    | n:  20
IntSoftmax | n:  15
IntGELU    | n:  20
IntSoftmax | n:  15
IntGELU    | n:  20
IntSoftmax | n:  15
IntGELU    | n:  20
IntSoftmax | n:  15
IntGELU    | n:  20
IntSoftmax | n:  15
IntGELU    | n:  20
IntSoftmax | n:  15
IntGELU    | n:  20
IntSoftmax | n:  15
IntGELU    | n:  20
IntSoftmax | n:  15
IntGELU    | n:  20
IntSoftmax | n:  15
IntGELU    | n:  20
IntSoftmax | n:  15
IntGELU    | n:  20
IntSoftmax | n:  15
IntGELU    | n:  20
.calib done
Test: [  0/391]	Time  3.862 ( 3.862)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.93
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=15, intgelu_exp_n=21
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=15, intgelu_exp_n=21, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  15
IntGELU    | n:  21
IntSoftmax | n:  15
IntGELU    | n:  21
IntSoftmax | n:  15
IntGELU    | n:  21
IntSoftmax | n:  15
IntGELU    | n:  21
IntSoftmax | n:  15
IntGELU    | n:  21
IntSoftmax | n:  15
IntGELU    | n:  21
IntSoftmax | n:  15
IntGELU    | n:  21
IntSoftmax | n:  15
IntGELU    | n:  21
IntSoftmax | n:  15
IntGELU    | n:  21
IntSoftmax | n:  15
IntGELU    | n:  21
IntSoftmax | n:  15
IntGELU    | n:  21
IntSoftmax | n:  15
IntGELU    | n:  21
.calib done
Test: [  0/391]	Time  3.675 ( 3.675)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.71
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=15, intgelu_exp_n=22
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=15, intgelu_exp_n=22, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  15
IntGELU    | n:  22
IntSoftmax | n:  15
IntGELU    | n:  22
IntSoftmax | n:  15
IntGELU    | n:  22
IntSoftmax | n:  15
IntGELU    | n:  22
IntSoftmax | n:  15
IntGELU    | n:  22
IntSoftmax | n:  15
IntGELU    | n:  22
IntSoftmax | n:  15
IntGELU    | n:  22
IntSoftmax | n:  15
IntGELU    | n:  22
IntSoftmax | n:  15
IntGELU    | n:  22
IntSoftmax | n:  15
IntGELU    | n:  22
IntSoftmax | n:  15
IntGELU    | n:  22
IntSoftmax | n:  15
IntGELU    | n:  22
.calib done
Test: [  0/391]	Time  3.842 ( 3.842)	Acc@1   0.78 (  0.78)	Acc@5   0.78 (  0.78)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.781 Prec@5 0.781
Time: 8.92
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=15, intgelu_exp_n=23
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=15, intgelu_exp_n=23, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  15
IntGELU    | n:  23
IntSoftmax | n:  15
IntGELU    | n:  23
IntSoftmax | n:  15
IntGELU    | n:  23
IntSoftmax | n:  15
IntGELU    | n:  23
IntSoftmax | n:  15
IntGELU    | n:  23
IntSoftmax | n:  15
IntGELU    | n:  23
IntSoftmax | n:  15
IntGELU    | n:  23
IntSoftmax | n:  15
IntGELU    | n:  23
IntSoftmax | n:  15
IntGELU    | n:  23
IntSoftmax | n:  15
IntGELU    | n:  23
IntSoftmax | n:  15
IntGELU    | n:  23
IntSoftmax | n:  15
IntGELU    | n:  23
.calib done
Test: [  0/391]	Time  4.058 ( 4.058)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.08
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=15, intgelu_exp_n=24
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=15, intgelu_exp_n=24, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  15
IntGELU    | n:  24
IntSoftmax | n:  15
IntGELU    | n:  24
IntSoftmax | n:  15
IntGELU    | n:  24
IntSoftmax | n:  15
IntGELU    | n:  24
IntSoftmax | n:  15
IntGELU    | n:  24
IntSoftmax | n:  15
IntGELU    | n:  24
IntSoftmax | n:  15
IntGELU    | n:  24
IntSoftmax | n:  15
IntGELU    | n:  24
IntSoftmax | n:  15
IntGELU    | n:  24
IntSoftmax | n:  15
IntGELU    | n:  24
IntSoftmax | n:  15
IntGELU    | n:  24
IntSoftmax | n:  15
IntGELU    | n:  24
.calib done
Test: [  0/391]	Time  3.968 ( 3.968)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.98
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=15, intgelu_exp_n=25
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=15, intgelu_exp_n=25, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  15
IntGELU    | n:  25
IntSoftmax | n:  15
IntGELU    | n:  25
IntSoftmax | n:  15
IntGELU    | n:  25
IntSoftmax | n:  15
IntGELU    | n:  25
IntSoftmax | n:  15
IntGELU    | n:  25
IntSoftmax | n:  15
IntGELU    | n:  25
IntSoftmax | n:  15
IntGELU    | n:  25
IntSoftmax | n:  15
IntGELU    | n:  25
IntSoftmax | n:  15
IntGELU    | n:  25
IntSoftmax | n:  15
IntGELU    | n:  25
IntSoftmax | n:  15
IntGELU    | n:  25
IntSoftmax | n:  15
IntGELU    | n:  25
.calib done
Test: [  0/391]	Time  3.715 ( 3.715)	Acc@1   0.78 (  0.78)	Acc@5   0.78 (  0.78)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.781 Prec@5 0.781
Time: 8.74
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=15, intgelu_exp_n=26
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=15, intgelu_exp_n=26, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  15
IntGELU    | n:  26
IntSoftmax | n:  15
IntGELU    | n:  26
IntSoftmax | n:  15
IntGELU    | n:  26
IntSoftmax | n:  15
IntGELU    | n:  26
IntSoftmax | n:  15
IntGELU    | n:  26
IntSoftmax | n:  15
IntGELU    | n:  26
IntSoftmax | n:  15
IntGELU    | n:  26
IntSoftmax | n:  15
IntGELU    | n:  26
IntSoftmax | n:  15
IntGELU    | n:  26
IntSoftmax | n:  15
IntGELU    | n:  26
IntSoftmax | n:  15
IntGELU    | n:  26
IntSoftmax | n:  15
IntGELU    | n:  26
.calib done
Test: [  0/391]	Time  3.781 ( 3.781)	Acc@1   0.00 (  0.00)	Acc@5   0.78 (  0.78)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.781
Time: 8.79
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=15, intgelu_exp_n=27
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=15, intgelu_exp_n=27, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  15
IntGELU    | n:  27
IntSoftmax | n:  15
IntGELU    | n:  27
IntSoftmax | n:  15
IntGELU    | n:  27
IntSoftmax | n:  15
IntGELU    | n:  27
IntSoftmax | n:  15
IntGELU    | n:  27
IntSoftmax | n:  15
IntGELU    | n:  27
IntSoftmax | n:  15
IntGELU    | n:  27
IntSoftmax | n:  15
IntGELU    | n:  27
IntSoftmax | n:  15
IntGELU    | n:  27
IntSoftmax | n:  15
IntGELU    | n:  27
IntSoftmax | n:  15
IntGELU    | n:  27
IntSoftmax | n:  15
IntGELU    | n:  27
.calib done
Test: [  0/391]	Time  3.822 ( 3.822)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.81
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=15, intgelu_exp_n=28
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=15, intgelu_exp_n=28, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  15
IntGELU    | n:  28
IntSoftmax | n:  15
IntGELU    | n:  28
IntSoftmax | n:  15
IntGELU    | n:  28
IntSoftmax | n:  15
IntGELU    | n:  28
IntSoftmax | n:  15
IntGELU    | n:  28
IntSoftmax | n:  15
IntGELU    | n:  28
IntSoftmax | n:  15
IntGELU    | n:  28
IntSoftmax | n:  15
IntGELU    | n:  28
IntSoftmax | n:  15
IntGELU    | n:  28
IntSoftmax | n:  15
IntGELU    | n:  28
IntSoftmax | n:  15
IntGELU    | n:  28
IntSoftmax | n:  15
IntGELU    | n:  28
.calib done
Test: [  0/391]	Time  4.033 ( 4.033)	Acc@1   0.00 (  0.00)	Acc@5   0.78 (  0.78)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.781
Time: 9.07
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=15, intgelu_exp_n=29
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=15, intgelu_exp_n=29, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
IntSoftmax | n:  15
IntGELU    | n:  29
.calib done
Test: [  0/391]	Time  3.971 ( 3.971)	Acc@1   0.78 (  0.78)	Acc@5   1.56 (  1.56)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.781 Prec@5 1.562
Time: 9.00
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=15, intgelu_exp_n=30
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=15, intgelu_exp_n=30, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  15
IntGELU    | n:  30
IntSoftmax | n:  15
IntGELU    | n:  30
IntSoftmax | n:  15
IntGELU    | n:  30
IntSoftmax | n:  15
IntGELU    | n:  30
IntSoftmax | n:  15
IntGELU    | n:  30
IntSoftmax | n:  15
IntGELU    | n:  30
IntSoftmax | n:  15
IntGELU    | n:  30
IntSoftmax | n:  15
IntGELU    | n:  30
IntSoftmax | n:  15
IntGELU    | n:  30
IntSoftmax | n:  15
IntGELU    | n:  30
IntSoftmax | n:  15
IntGELU    | n:  30
IntSoftmax | n:  15
IntGELU    | n:  30
.calib done
Test: [  0/391]	Time  3.838 ( 3.838)	Acc@1   3.91 (  3.91)	Acc@5   5.47 (  5.47)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 3.906 Prec@5 5.469
Time: 8.86
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=15, intgelu_exp_n=31
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=15, intgelu_exp_n=31, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  15
IntGELU    | n:  31
IntSoftmax | n:  15
IntGELU    | n:  31
IntSoftmax | n:  15
IntGELU    | n:  31
IntSoftmax | n:  15
IntGELU    | n:  31
IntSoftmax | n:  15
IntGELU    | n:  31
IntSoftmax | n:  15
IntGELU    | n:  31
IntSoftmax | n:  15
IntGELU    | n:  31
IntSoftmax | n:  15
IntGELU    | n:  31
IntSoftmax | n:  15
IntGELU    | n:  31
IntSoftmax | n:  15
IntGELU    | n:  31
IntSoftmax | n:  15
IntGELU    | n:  31
IntSoftmax | n:  15
IntGELU    | n:  31
.calib done
Test: [  0/391]	Time  3.977 ( 3.977)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.06
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=16, intgelu_exp_n=0
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=16, intgelu_exp_n=0, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  16
IntGELU    | n:  0
IntSoftmax | n:  16
IntGELU    | n:  0
IntSoftmax | n:  16
IntGELU    | n:  0
IntSoftmax | n:  16
IntGELU    | n:  0
IntSoftmax | n:  16
IntGELU    | n:  0
IntSoftmax | n:  16
IntGELU    | n:  0
IntSoftmax | n:  16
IntGELU    | n:  0
IntSoftmax | n:  16
IntGELU    | n:  0
IntSoftmax | n:  16
IntGELU    | n:  0
IntSoftmax | n:  16
IntGELU    | n:  0
IntSoftmax | n:  16
IntGELU    | n:  0
IntSoftmax | n:  16
IntGELU    | n:  0
.calib done
Test: [  0/391]	Time  3.852 ( 3.852)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.92
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=16, intgelu_exp_n=1
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=16, intgelu_exp_n=1, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  16
IntGELU    | n:  1
IntSoftmax | n:  16
IntGELU    | n:  1
IntSoftmax | n:  16
IntGELU    | n:  1
IntSoftmax | n:  16
IntGELU    | n:  1
IntSoftmax | n:  16
IntGELU    | n:  1
IntSoftmax | n:  16
IntGELU    | n:  1
IntSoftmax | n:  16
IntGELU    | n:  1
IntSoftmax | n:  16
IntGELU    | n:  1
IntSoftmax | n:  16
IntGELU    | n:  1
IntSoftmax | n:  16
IntGELU    | n:  1
IntSoftmax | n:  16
IntGELU    | n:  1
IntSoftmax | n:  16
IntGELU    | n:  1
.calib done
Test: [  0/391]	Time  3.953 ( 3.953)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.99
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=16, intgelu_exp_n=2
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=16, intgelu_exp_n=2, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  16
IntGELU    | n:  2
IntSoftmax | n:  16
IntGELU    | n:  2
IntSoftmax | n:  16
IntGELU    | n:  2
IntSoftmax | n:  16
IntGELU    | n:  2
IntSoftmax | n:  16
IntGELU    | n:  2
IntSoftmax | n:  16
IntGELU    | n:  2
IntSoftmax | n:  16
IntGELU    | n:  2
IntSoftmax | n:  16
IntGELU    | n:  2
IntSoftmax | n:  16
IntGELU    | n:  2
IntSoftmax | n:  16
IntGELU    | n:  2
IntSoftmax | n:  16
IntGELU    | n:  2
IntSoftmax | n:  16
IntGELU    | n:  2
.calib done
Test: [  0/391]	Time  3.740 ( 3.740)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.82
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=16, intgelu_exp_n=3
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=16, intgelu_exp_n=3, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  16
IntGELU    | n:  3
IntSoftmax | n:  16
IntGELU    | n:  3
IntSoftmax | n:  16
IntGELU    | n:  3
IntSoftmax | n:  16
IntGELU    | n:  3
IntSoftmax | n:  16
IntGELU    | n:  3
IntSoftmax | n:  16
IntGELU    | n:  3
IntSoftmax | n:  16
IntGELU    | n:  3
IntSoftmax | n:  16
IntGELU    | n:  3
IntSoftmax | n:  16
IntGELU    | n:  3
IntSoftmax | n:  16
IntGELU    | n:  3
IntSoftmax | n:  16
IntGELU    | n:  3
IntSoftmax | n:  16
IntGELU    | n:  3
.calib done
Test: [  0/391]	Time  3.915 ( 3.915)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.93
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=16, intgelu_exp_n=4
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=16, intgelu_exp_n=4, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  16
IntGELU    | n:  4
IntSoftmax | n:  16
IntGELU    | n:  4
IntSoftmax | n:  16
IntGELU    | n:  4
IntSoftmax | n:  16
IntGELU    | n:  4
IntSoftmax | n:  16
IntGELU    | n:  4
IntSoftmax | n:  16
IntGELU    | n:  4
IntSoftmax | n:  16
IntGELU    | n:  4
IntSoftmax | n:  16
IntGELU    | n:  4
IntSoftmax | n:  16
IntGELU    | n:  4
IntSoftmax | n:  16
IntGELU    | n:  4
IntSoftmax | n:  16
IntGELU    | n:  4
IntSoftmax | n:  16
IntGELU    | n:  4
.calib done
Test: [  0/391]	Time  3.952 ( 3.952)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.99
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=16, intgelu_exp_n=5
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=16, intgelu_exp_n=5, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  16
IntGELU    | n:  5
IntSoftmax | n:  16
IntGELU    | n:  5
IntSoftmax | n:  16
IntGELU    | n:  5
IntSoftmax | n:  16
IntGELU    | n:  5
IntSoftmax | n:  16
IntGELU    | n:  5
IntSoftmax | n:  16
IntGELU    | n:  5
IntSoftmax | n:  16
IntGELU    | n:  5
IntSoftmax | n:  16
IntGELU    | n:  5
IntSoftmax | n:  16
IntGELU    | n:  5
IntSoftmax | n:  16
IntGELU    | n:  5
IntSoftmax | n:  16
IntGELU    | n:  5
IntSoftmax | n:  16
IntGELU    | n:  5
.calib done
Test: [  0/391]	Time  3.874 ( 3.874)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.90
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=16, intgelu_exp_n=6
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=16, intgelu_exp_n=6, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  16
IntGELU    | n:  6
IntSoftmax | n:  16
IntGELU    | n:  6
IntSoftmax | n:  16
IntGELU    | n:  6
IntSoftmax | n:  16
IntGELU    | n:  6
IntSoftmax | n:  16
IntGELU    | n:  6
IntSoftmax | n:  16
IntGELU    | n:  6
IntSoftmax | n:  16
IntGELU    | n:  6
IntSoftmax | n:  16
IntGELU    | n:  6
IntSoftmax | n:  16
IntGELU    | n:  6
IntSoftmax | n:  16
IntGELU    | n:  6
IntSoftmax | n:  16
IntGELU    | n:  6
IntSoftmax | n:  16
IntGELU    | n:  6
.calib done
Test: [  0/391]	Time  3.952 ( 3.952)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.93
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=16, intgelu_exp_n=7
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=16, intgelu_exp_n=7, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  16
IntGELU    | n:  7
IntSoftmax | n:  16
IntGELU    | n:  7
IntSoftmax | n:  16
IntGELU    | n:  7
IntSoftmax | n:  16
IntGELU    | n:  7
IntSoftmax | n:  16
IntGELU    | n:  7
IntSoftmax | n:  16
IntGELU    | n:  7
IntSoftmax | n:  16
IntGELU    | n:  7
IntSoftmax | n:  16
IntGELU    | n:  7
IntSoftmax | n:  16
IntGELU    | n:  7
IntSoftmax | n:  16
IntGELU    | n:  7
IntSoftmax | n:  16
IntGELU    | n:  7
IntSoftmax | n:  16
IntGELU    | n:  7
.calib done
Test: [  0/391]	Time  3.836 ( 3.836)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.85
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=16, intgelu_exp_n=8
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=16, intgelu_exp_n=8, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  16
IntGELU    | n:  8
IntSoftmax | n:  16
IntGELU    | n:  8
IntSoftmax | n:  16
IntGELU    | n:  8
IntSoftmax | n:  16
IntGELU    | n:  8
IntSoftmax | n:  16
IntGELU    | n:  8
IntSoftmax | n:  16
IntGELU    | n:  8
IntSoftmax | n:  16
IntGELU    | n:  8
IntSoftmax | n:  16
IntGELU    | n:  8
IntSoftmax | n:  16
IntGELU    | n:  8
IntSoftmax | n:  16
IntGELU    | n:  8
IntSoftmax | n:  16
IntGELU    | n:  8
IntSoftmax | n:  16
IntGELU    | n:  8
.calib done
Test: [  0/391]	Time  3.988 ( 3.988)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.02
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=16, intgelu_exp_n=9
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=16, intgelu_exp_n=9, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  16
IntGELU    | n:  9
IntSoftmax | n:  16
IntGELU    | n:  9
IntSoftmax | n:  16
IntGELU    | n:  9
IntSoftmax | n:  16
IntGELU    | n:  9
IntSoftmax | n:  16
IntGELU    | n:  9
IntSoftmax | n:  16
IntGELU    | n:  9
IntSoftmax | n:  16
IntGELU    | n:  9
IntSoftmax | n:  16
IntGELU    | n:  9
IntSoftmax | n:  16
IntGELU    | n:  9
IntSoftmax | n:  16
IntGELU    | n:  9
IntSoftmax | n:  16
IntGELU    | n:  9
IntSoftmax | n:  16
IntGELU    | n:  9
.calib done
Test: [  0/391]	Time  3.812 ( 3.812)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.87
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=16, intgelu_exp_n=10
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=16, intgelu_exp_n=10, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  16
IntGELU    | n:  10
IntSoftmax | n:  16
IntGELU    | n:  10
IntSoftmax | n:  16
IntGELU    | n:  10
IntSoftmax | n:  16
IntGELU    | n:  10
IntSoftmax | n:  16
IntGELU    | n:  10
IntSoftmax | n:  16
IntGELU    | n:  10
IntSoftmax | n:  16
IntGELU    | n:  10
IntSoftmax | n:  16
IntGELU    | n:  10
IntSoftmax | n:  16
IntGELU    | n:  10
IntSoftmax | n:  16
IntGELU    | n:  10
IntSoftmax | n:  16
IntGELU    | n:  10
IntSoftmax | n:  16
IntGELU    | n:  10
.calib done
Test: [  0/391]	Time  3.799 ( 3.799)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.82
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=16, intgelu_exp_n=11
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=16, intgelu_exp_n=11, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  16
IntGELU    | n:  11
IntSoftmax | n:  16
IntGELU    | n:  11
IntSoftmax | n:  16
IntGELU    | n:  11
IntSoftmax | n:  16
IntGELU    | n:  11
IntSoftmax | n:  16
IntGELU    | n:  11
IntSoftmax | n:  16
IntGELU    | n:  11
IntSoftmax | n:  16
IntGELU    | n:  11
IntSoftmax | n:  16
IntGELU    | n:  11
IntSoftmax | n:  16
IntGELU    | n:  11
IntSoftmax | n:  16
IntGELU    | n:  11
IntSoftmax | n:  16
IntGELU    | n:  11
IntSoftmax | n:  16
IntGELU    | n:  11
.calib done
Test: [  0/391]	Time  3.952 ( 3.952)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.99
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=16, intgelu_exp_n=12
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=16, intgelu_exp_n=12, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  16
IntGELU    | n:  12
IntSoftmax | n:  16
IntGELU    | n:  12
IntSoftmax | n:  16
IntGELU    | n:  12
IntSoftmax | n:  16
IntGELU    | n:  12
IntSoftmax | n:  16
IntGELU    | n:  12
IntSoftmax | n:  16
IntGELU    | n:  12
IntSoftmax | n:  16
IntGELU    | n:  12
IntSoftmax | n:  16
IntGELU    | n:  12
IntSoftmax | n:  16
IntGELU    | n:  12
IntSoftmax | n:  16
IntGELU    | n:  12
IntSoftmax | n:  16
IntGELU    | n:  12
IntSoftmax | n:  16
IntGELU    | n:  12
.calib done
Test: [  0/391]	Time  3.899 ( 3.899)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.93
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=16, intgelu_exp_n=13
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=16, intgelu_exp_n=13, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  16
IntGELU    | n:  13
IntSoftmax | n:  16
IntGELU    | n:  13
IntSoftmax | n:  16
IntGELU    | n:  13
IntSoftmax | n:  16
IntGELU    | n:  13
IntSoftmax | n:  16
IntGELU    | n:  13
IntSoftmax | n:  16
IntGELU    | n:  13
IntSoftmax | n:  16
IntGELU    | n:  13
IntSoftmax | n:  16
IntGELU    | n:  13
IntSoftmax | n:  16
IntGELU    | n:  13
IntSoftmax | n:  16
IntGELU    | n:  13
IntSoftmax | n:  16
IntGELU    | n:  13
IntSoftmax | n:  16
IntGELU    | n:  13
.calib done
Test: [  0/391]	Time  3.793 ( 3.793)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.90
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=16, intgelu_exp_n=14
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=16, intgelu_exp_n=14, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  16
IntGELU    | n:  14
IntSoftmax | n:  16
IntGELU    | n:  14
IntSoftmax | n:  16
IntGELU    | n:  14
IntSoftmax | n:  16
IntGELU    | n:  14
IntSoftmax | n:  16
IntGELU    | n:  14
IntSoftmax | n:  16
IntGELU    | n:  14
IntSoftmax | n:  16
IntGELU    | n:  14
IntSoftmax | n:  16
IntGELU    | n:  14
IntSoftmax | n:  16
IntGELU    | n:  14
IntSoftmax | n:  16
IntGELU    | n:  14
IntSoftmax | n:  16
IntGELU    | n:  14
IntSoftmax | n:  16
IntGELU    | n:  14
.calib done
Test: [  0/391]	Time  3.881 ( 3.881)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.91
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=16, intgelu_exp_n=15
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=16, intgelu_exp_n=15, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  16
IntGELU    | n:  15
IntSoftmax | n:  16
IntGELU    | n:  15
IntSoftmax | n:  16
IntGELU    | n:  15
IntSoftmax | n:  16
IntGELU    | n:  15
IntSoftmax | n:  16
IntGELU    | n:  15
IntSoftmax | n:  16
IntGELU    | n:  15
IntSoftmax | n:  16
IntGELU    | n:  15
IntSoftmax | n:  16
IntGELU    | n:  15
IntSoftmax | n:  16
IntGELU    | n:  15
IntSoftmax | n:  16
IntGELU    | n:  15
IntSoftmax | n:  16
IntGELU    | n:  15
IntSoftmax | n:  16
IntGELU    | n:  15
.calib done
Test: [  0/391]	Time  3.890 ( 3.890)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.93
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=16, intgelu_exp_n=16
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=16, intgelu_exp_n=16, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  16
IntGELU    | n:  16
IntSoftmax | n:  16
IntGELU    | n:  16
IntSoftmax | n:  16
IntGELU    | n:  16
IntSoftmax | n:  16
IntGELU    | n:  16
IntSoftmax | n:  16
IntGELU    | n:  16
IntSoftmax | n:  16
IntGELU    | n:  16
IntSoftmax | n:  16
IntGELU    | n:  16
IntSoftmax | n:  16
IntGELU    | n:  16
IntSoftmax | n:  16
IntGELU    | n:  16
IntSoftmax | n:  16
IntGELU    | n:  16
IntSoftmax | n:  16
IntGELU    | n:  16
IntSoftmax | n:  16
IntGELU    | n:  16
.calib done
Test: [  0/391]	Time  3.863 ( 3.863)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=16, intgelu_exp_n=17
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=16, intgelu_exp_n=17, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  16
IntGELU    | n:  17
IntSoftmax | n:  16
IntGELU    | n:  17
IntSoftmax | n:  16
IntGELU    | n:  17
IntSoftmax | n:  16
IntGELU    | n:  17
IntSoftmax | n:  16
IntGELU    | n:  17
IntSoftmax | n:  16
IntGELU    | n:  17
IntSoftmax | n:  16
IntGELU    | n:  17
IntSoftmax | n:  16
IntGELU    | n:  17
IntSoftmax | n:  16
IntGELU    | n:  17
IntSoftmax | n:  16
IntGELU    | n:  17
IntSoftmax | n:  16
IntGELU    | n:  17
IntSoftmax | n:  16
IntGELU    | n:  17
.calib done
Test: [  0/391]	Time  4.010 ( 4.010)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.03
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=16, intgelu_exp_n=18
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=16, intgelu_exp_n=18, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  16
IntGELU    | n:  18
IntSoftmax | n:  16
IntGELU    | n:  18
IntSoftmax | n:  16
IntGELU    | n:  18
IntSoftmax | n:  16
IntGELU    | n:  18
IntSoftmax | n:  16
IntGELU    | n:  18
IntSoftmax | n:  16
IntGELU    | n:  18
IntSoftmax | n:  16
IntGELU    | n:  18
IntSoftmax | n:  16
IntGELU    | n:  18
IntSoftmax | n:  16
IntGELU    | n:  18
IntSoftmax | n:  16
IntGELU    | n:  18
IntSoftmax | n:  16
IntGELU    | n:  18
IntSoftmax | n:  16
IntGELU    | n:  18
.calib done
Test: [  0/391]	Time  3.849 ( 3.849)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.91
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=16, intgelu_exp_n=19
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=16, intgelu_exp_n=19, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  16
IntGELU    | n:  19
IntSoftmax | n:  16
IntGELU    | n:  19
IntSoftmax | n:  16
IntGELU    | n:  19
IntSoftmax | n:  16
IntGELU    | n:  19
IntSoftmax | n:  16
IntGELU    | n:  19
IntSoftmax | n:  16
IntGELU    | n:  19
IntSoftmax | n:  16
IntGELU    | n:  19
IntSoftmax | n:  16
IntGELU    | n:  19
IntSoftmax | n:  16
IntGELU    | n:  19
IntSoftmax | n:  16
IntGELU    | n:  19
IntSoftmax | n:  16
IntGELU    | n:  19
IntSoftmax | n:  16
IntGELU    | n:  19
.calib done
Test: [  0/391]	Time  3.935 ( 3.935)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.98
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=16, intgelu_exp_n=20
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=16, intgelu_exp_n=20, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  16
IntGELU    | n:  20
IntSoftmax | n:  16
IntGELU    | n:  20
IntSoftmax | n:  16
IntGELU    | n:  20
IntSoftmax | n:  16
IntGELU    | n:  20
IntSoftmax | n:  16
IntGELU    | n:  20
IntSoftmax | n:  16
IntGELU    | n:  20
IntSoftmax | n:  16
IntGELU    | n:  20
IntSoftmax | n:  16
IntGELU    | n:  20
IntSoftmax | n:  16
IntGELU    | n:  20
IntSoftmax | n:  16
IntGELU    | n:  20
IntSoftmax | n:  16
IntGELU    | n:  20
IntSoftmax | n:  16
IntGELU    | n:  20
.calib done
Test: [  0/391]	Time  3.834 ( 3.834)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.90
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=16, intgelu_exp_n=21
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=16, intgelu_exp_n=21, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  16
IntGELU    | n:  21
IntSoftmax | n:  16
IntGELU    | n:  21
IntSoftmax | n:  16
IntGELU    | n:  21
IntSoftmax | n:  16
IntGELU    | n:  21
IntSoftmax | n:  16
IntGELU    | n:  21
IntSoftmax | n:  16
IntGELU    | n:  21
IntSoftmax | n:  16
IntGELU    | n:  21
IntSoftmax | n:  16
IntGELU    | n:  21
IntSoftmax | n:  16
IntGELU    | n:  21
IntSoftmax | n:  16
IntGELU    | n:  21
IntSoftmax | n:  16
IntGELU    | n:  21
IntSoftmax | n:  16
IntGELU    | n:  21
.calib done
Test: [  0/391]	Time  3.991 ( 3.991)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.99
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=16, intgelu_exp_n=22
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=16, intgelu_exp_n=22, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  16
IntGELU    | n:  22
IntSoftmax | n:  16
IntGELU    | n:  22
IntSoftmax | n:  16
IntGELU    | n:  22
IntSoftmax | n:  16
IntGELU    | n:  22
IntSoftmax | n:  16
IntGELU    | n:  22
IntSoftmax | n:  16
IntGELU    | n:  22
IntSoftmax | n:  16
IntGELU    | n:  22
IntSoftmax | n:  16
IntGELU    | n:  22
IntSoftmax | n:  16
IntGELU    | n:  22
IntSoftmax | n:  16
IntGELU    | n:  22
IntSoftmax | n:  16
IntGELU    | n:  22
IntSoftmax | n:  16
IntGELU    | n:  22
.calib done
Test: [  0/391]	Time  3.978 ( 3.978)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.99
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=16, intgelu_exp_n=23
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=16, intgelu_exp_n=23, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  16
IntGELU    | n:  23
IntSoftmax | n:  16
IntGELU    | n:  23
IntSoftmax | n:  16
IntGELU    | n:  23
IntSoftmax | n:  16
IntGELU    | n:  23
IntSoftmax | n:  16
IntGELU    | n:  23
IntSoftmax | n:  16
IntGELU    | n:  23
IntSoftmax | n:  16
IntGELU    | n:  23
IntSoftmax | n:  16
IntGELU    | n:  23
IntSoftmax | n:  16
IntGELU    | n:  23
IntSoftmax | n:  16
IntGELU    | n:  23
IntSoftmax | n:  16
IntGELU    | n:  23
IntSoftmax | n:  16
IntGELU    | n:  23
.calib done
Test: [  0/391]	Time  3.823 ( 3.823)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.90
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=16, intgelu_exp_n=24
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=16, intgelu_exp_n=24, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  16
IntGELU    | n:  24
IntSoftmax | n:  16
IntGELU    | n:  24
IntSoftmax | n:  16
IntGELU    | n:  24
IntSoftmax | n:  16
IntGELU    | n:  24
IntSoftmax | n:  16
IntGELU    | n:  24
IntSoftmax | n:  16
IntGELU    | n:  24
IntSoftmax | n:  16
IntGELU    | n:  24
IntSoftmax | n:  16
IntGELU    | n:  24
IntSoftmax | n:  16
IntGELU    | n:  24
IntSoftmax | n:  16
IntGELU    | n:  24
IntSoftmax | n:  16
IntGELU    | n:  24
IntSoftmax | n:  16
IntGELU    | n:  24
.calib done
Test: [  0/391]	Time  3.721 ( 3.721)	Acc@1   0.00 (  0.00)	Acc@5   0.78 (  0.78)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.781
Time: 8.76
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=16, intgelu_exp_n=25
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=16, intgelu_exp_n=25, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  16
IntGELU    | n:  25
IntSoftmax | n:  16
IntGELU    | n:  25
IntSoftmax | n:  16
IntGELU    | n:  25
IntSoftmax | n:  16
IntGELU    | n:  25
IntSoftmax | n:  16
IntGELU    | n:  25
IntSoftmax | n:  16
IntGELU    | n:  25
IntSoftmax | n:  16
IntGELU    | n:  25
IntSoftmax | n:  16
IntGELU    | n:  25
IntSoftmax | n:  16
IntGELU    | n:  25
IntSoftmax | n:  16
IntGELU    | n:  25
IntSoftmax | n:  16
IntGELU    | n:  25
IntSoftmax | n:  16
IntGELU    | n:  25
.calib done
Test: [  0/391]	Time  3.658 ( 3.658)	Acc@1   0.00 (  0.00)	Acc@5   0.78 (  0.78)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.781
Time: 8.70
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=16, intgelu_exp_n=26
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=16, intgelu_exp_n=26, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  16
IntGELU    | n:  26
IntSoftmax | n:  16
IntGELU    | n:  26
IntSoftmax | n:  16
IntGELU    | n:  26
IntSoftmax | n:  16
IntGELU    | n:  26
IntSoftmax | n:  16
IntGELU    | n:  26
IntSoftmax | n:  16
IntGELU    | n:  26
IntSoftmax | n:  16
IntGELU    | n:  26
IntSoftmax | n:  16
IntGELU    | n:  26
IntSoftmax | n:  16
IntGELU    | n:  26
IntSoftmax | n:  16
IntGELU    | n:  26
IntSoftmax | n:  16
IntGELU    | n:  26
IntSoftmax | n:  16
IntGELU    | n:  26
.calib done
Test: [  0/391]	Time  3.808 ( 3.808)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.89
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=16, intgelu_exp_n=27
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=16, intgelu_exp_n=27, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  16
IntGELU    | n:  27
IntSoftmax | n:  16
IntGELU    | n:  27
IntSoftmax | n:  16
IntGELU    | n:  27
IntSoftmax | n:  16
IntGELU    | n:  27
IntSoftmax | n:  16
IntGELU    | n:  27
IntSoftmax | n:  16
IntGELU    | n:  27
IntSoftmax | n:  16
IntGELU    | n:  27
IntSoftmax | n:  16
IntGELU    | n:  27
IntSoftmax | n:  16
IntGELU    | n:  27
IntSoftmax | n:  16
IntGELU    | n:  27
IntSoftmax | n:  16
IntGELU    | n:  27
IntSoftmax | n:  16
IntGELU    | n:  27
.calib done
Test: [  0/391]	Time  3.919 ( 3.919)	Acc@1   0.00 (  0.00)	Acc@5   1.56 (  1.56)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 1.562
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=16, intgelu_exp_n=28
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=16, intgelu_exp_n=28, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  16
IntGELU    | n:  28
IntSoftmax | n:  16
IntGELU    | n:  28
IntSoftmax | n:  16
IntGELU    | n:  28
IntSoftmax | n:  16
IntGELU    | n:  28
IntSoftmax | n:  16
IntGELU    | n:  28
IntSoftmax | n:  16
IntGELU    | n:  28
IntSoftmax | n:  16
IntGELU    | n:  28
IntSoftmax | n:  16
IntGELU    | n:  28
IntSoftmax | n:  16
IntGELU    | n:  28
IntSoftmax | n:  16
IntGELU    | n:  28
IntSoftmax | n:  16
IntGELU    | n:  28
IntSoftmax | n:  16
IntGELU    | n:  28
.calib done
Test: [  0/391]	Time  3.914 ( 3.914)	Acc@1   0.00 (  0.00)	Acc@5   0.78 (  0.78)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.781
Time: 8.98
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=16, intgelu_exp_n=29
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=16, intgelu_exp_n=29, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  16
IntGELU    | n:  29
IntSoftmax | n:  16
IntGELU    | n:  29
IntSoftmax | n:  16
IntGELU    | n:  29
IntSoftmax | n:  16
IntGELU    | n:  29
IntSoftmax | n:  16
IntGELU    | n:  29
IntSoftmax | n:  16
IntGELU    | n:  29
IntSoftmax | n:  16
IntGELU    | n:  29
IntSoftmax | n:  16
IntGELU    | n:  29
IntSoftmax | n:  16
IntGELU    | n:  29
IntSoftmax | n:  16
IntGELU    | n:  29
IntSoftmax | n:  16
IntGELU    | n:  29
IntSoftmax | n:  16
IntGELU    | n:  29
.calib done
Test: [  0/391]	Time  3.943 ( 3.943)	Acc@1   0.78 (  0.78)	Acc@5   2.34 (  2.34)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.781 Prec@5 2.344
Time: 9.00
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=16, intgelu_exp_n=30
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=16, intgelu_exp_n=30, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  16
IntGELU    | n:  30
IntSoftmax | n:  16
IntGELU    | n:  30
IntSoftmax | n:  16
IntGELU    | n:  30
IntSoftmax | n:  16
IntGELU    | n:  30
IntSoftmax | n:  16
IntGELU    | n:  30
IntSoftmax | n:  16
IntGELU    | n:  30
IntSoftmax | n:  16
IntGELU    | n:  30
IntSoftmax | n:  16
IntGELU    | n:  30
IntSoftmax | n:  16
IntGELU    | n:  30
IntSoftmax | n:  16
IntGELU    | n:  30
IntSoftmax | n:  16
IntGELU    | n:  30
IntSoftmax | n:  16
IntGELU    | n:  30
.calib done
Test: [  0/391]	Time  3.691 ( 3.691)	Acc@1   1.56 (  1.56)	Acc@5   7.81 (  7.81)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 1.562 Prec@5 7.812
Time: 8.70
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=16, intgelu_exp_n=31
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=16, intgelu_exp_n=31, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  16
IntGELU    | n:  31
IntSoftmax | n:  16
IntGELU    | n:  31
IntSoftmax | n:  16
IntGELU    | n:  31
IntSoftmax | n:  16
IntGELU    | n:  31
IntSoftmax | n:  16
IntGELU    | n:  31
IntSoftmax | n:  16
IntGELU    | n:  31
IntSoftmax | n:  16
IntGELU    | n:  31
IntSoftmax | n:  16
IntGELU    | n:  31
IntSoftmax | n:  16
IntGELU    | n:  31
IntSoftmax | n:  16
IntGELU    | n:  31
IntSoftmax | n:  16
IntGELU    | n:  31
IntSoftmax | n:  16
IntGELU    | n:  31
.calib done
Test: [  0/391]	Time  3.798 ( 3.798)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.84
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=17, intgelu_exp_n=0
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=17, intgelu_exp_n=0, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  17
IntGELU    | n:  0
IntSoftmax | n:  17
IntGELU    | n:  0
IntSoftmax | n:  17
IntGELU    | n:  0
IntSoftmax | n:  17
IntGELU    | n:  0
IntSoftmax | n:  17
IntGELU    | n:  0
IntSoftmax | n:  17
IntGELU    | n:  0
IntSoftmax | n:  17
IntGELU    | n:  0
IntSoftmax | n:  17
IntGELU    | n:  0
IntSoftmax | n:  17
IntGELU    | n:  0
IntSoftmax | n:  17
IntGELU    | n:  0
IntSoftmax | n:  17
IntGELU    | n:  0
IntSoftmax | n:  17
IntGELU    | n:  0
.calib done
Test: [  0/391]	Time  3.701 ( 3.701)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.75
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=17, intgelu_exp_n=1
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=17, intgelu_exp_n=1, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  17
IntGELU    | n:  1
IntSoftmax | n:  17
IntGELU    | n:  1
IntSoftmax | n:  17
IntGELU    | n:  1
IntSoftmax | n:  17
IntGELU    | n:  1
IntSoftmax | n:  17
IntGELU    | n:  1
IntSoftmax | n:  17
IntGELU    | n:  1
IntSoftmax | n:  17
IntGELU    | n:  1
IntSoftmax | n:  17
IntGELU    | n:  1
IntSoftmax | n:  17
IntGELU    | n:  1
IntSoftmax | n:  17
IntGELU    | n:  1
IntSoftmax | n:  17
IntGELU    | n:  1
IntSoftmax | n:  17
IntGELU    | n:  1
.calib done
Test: [  0/391]	Time  3.841 ( 3.841)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.87
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=17, intgelu_exp_n=2
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=17, intgelu_exp_n=2, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  17
IntGELU    | n:  2
IntSoftmax | n:  17
IntGELU    | n:  2
IntSoftmax | n:  17
IntGELU    | n:  2
IntSoftmax | n:  17
IntGELU    | n:  2
IntSoftmax | n:  17
IntGELU    | n:  2
IntSoftmax | n:  17
IntGELU    | n:  2
IntSoftmax | n:  17
IntGELU    | n:  2
IntSoftmax | n:  17
IntGELU    | n:  2
IntSoftmax | n:  17
IntGELU    | n:  2
IntSoftmax | n:  17
IntGELU    | n:  2
IntSoftmax | n:  17
IntGELU    | n:  2
IntSoftmax | n:  17
IntGELU    | n:  2
.calib done
Test: [  0/391]	Time  3.907 ( 3.907)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.98
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=17, intgelu_exp_n=3
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=17, intgelu_exp_n=3, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  17
IntGELU    | n:  3
IntSoftmax | n:  17
IntGELU    | n:  3
IntSoftmax | n:  17
IntGELU    | n:  3
IntSoftmax | n:  17
IntGELU    | n:  3
IntSoftmax | n:  17
IntGELU    | n:  3
IntSoftmax | n:  17
IntGELU    | n:  3
IntSoftmax | n:  17
IntGELU    | n:  3
IntSoftmax | n:  17
IntGELU    | n:  3
IntSoftmax | n:  17
IntGELU    | n:  3
IntSoftmax | n:  17
IntGELU    | n:  3
IntSoftmax | n:  17
IntGELU    | n:  3
IntSoftmax | n:  17
IntGELU    | n:  3
.calib done
Test: [  0/391]	Time  3.887 ( 3.887)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.98
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=17, intgelu_exp_n=4
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=17, intgelu_exp_n=4, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  17
IntGELU    | n:  4
IntSoftmax | n:  17
IntGELU    | n:  4
IntSoftmax | n:  17
IntGELU    | n:  4
IntSoftmax | n:  17
IntGELU    | n:  4
IntSoftmax | n:  17
IntGELU    | n:  4
IntSoftmax | n:  17
IntGELU    | n:  4
IntSoftmax | n:  17
IntGELU    | n:  4
IntSoftmax | n:  17
IntGELU    | n:  4
IntSoftmax | n:  17
IntGELU    | n:  4
IntSoftmax | n:  17
IntGELU    | n:  4
IntSoftmax | n:  17
IntGELU    | n:  4
IntSoftmax | n:  17
IntGELU    | n:  4
.calib done
Test: [  0/391]	Time  3.944 ( 3.944)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.03
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=17, intgelu_exp_n=5
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=17, intgelu_exp_n=5, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  17
IntGELU    | n:  5
IntSoftmax | n:  17
IntGELU    | n:  5
IntSoftmax | n:  17
IntGELU    | n:  5
IntSoftmax | n:  17
IntGELU    | n:  5
IntSoftmax | n:  17
IntGELU    | n:  5
IntSoftmax | n:  17
IntGELU    | n:  5
IntSoftmax | n:  17
IntGELU    | n:  5
IntSoftmax | n:  17
IntGELU    | n:  5
IntSoftmax | n:  17
IntGELU    | n:  5
IntSoftmax | n:  17
IntGELU    | n:  5
IntSoftmax | n:  17
IntGELU    | n:  5
IntSoftmax | n:  17
IntGELU    | n:  5
.calib done
Test: [  0/391]	Time  3.998 ( 3.998)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.03
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=17, intgelu_exp_n=6
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=17, intgelu_exp_n=6, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  17
IntGELU    | n:  6
IntSoftmax | n:  17
IntGELU    | n:  6
IntSoftmax | n:  17
IntGELU    | n:  6
IntSoftmax | n:  17
IntGELU    | n:  6
IntSoftmax | n:  17
IntGELU    | n:  6
IntSoftmax | n:  17
IntGELU    | n:  6
IntSoftmax | n:  17
IntGELU    | n:  6
IntSoftmax | n:  17
IntGELU    | n:  6
IntSoftmax | n:  17
IntGELU    | n:  6
IntSoftmax | n:  17
IntGELU    | n:  6
IntSoftmax | n:  17
IntGELU    | n:  6
IntSoftmax | n:  17
IntGELU    | n:  6
.calib done
Test: [  0/391]	Time  3.926 ( 3.926)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.01
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=17, intgelu_exp_n=7
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=17, intgelu_exp_n=7, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  17
IntGELU    | n:  7
IntSoftmax | n:  17
IntGELU    | n:  7
IntSoftmax | n:  17
IntGELU    | n:  7
IntSoftmax | n:  17
IntGELU    | n:  7
IntSoftmax | n:  17
IntGELU    | n:  7
IntSoftmax | n:  17
IntGELU    | n:  7
IntSoftmax | n:  17
IntGELU    | n:  7
IntSoftmax | n:  17
IntGELU    | n:  7
IntSoftmax | n:  17
IntGELU    | n:  7
IntSoftmax | n:  17
IntGELU    | n:  7
IntSoftmax | n:  17
IntGELU    | n:  7
IntSoftmax | n:  17
IntGELU    | n:  7
.calib done
Test: [  0/391]	Time  3.921 ( 3.921)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=17, intgelu_exp_n=8
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=17, intgelu_exp_n=8, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  17
IntGELU    | n:  8
IntSoftmax | n:  17
IntGELU    | n:  8
IntSoftmax | n:  17
IntGELU    | n:  8
IntSoftmax | n:  17
IntGELU    | n:  8
IntSoftmax | n:  17
IntGELU    | n:  8
IntSoftmax | n:  17
IntGELU    | n:  8
IntSoftmax | n:  17
IntGELU    | n:  8
IntSoftmax | n:  17
IntGELU    | n:  8
IntSoftmax | n:  17
IntGELU    | n:  8
IntSoftmax | n:  17
IntGELU    | n:  8
IntSoftmax | n:  17
IntGELU    | n:  8
IntSoftmax | n:  17
IntGELU    | n:  8
.calib done
Test: [  0/391]	Time  3.924 ( 3.924)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.95
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=17, intgelu_exp_n=9
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=17, intgelu_exp_n=9, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  17
IntGELU    | n:  9
IntSoftmax | n:  17
IntGELU    | n:  9
IntSoftmax | n:  17
IntGELU    | n:  9
IntSoftmax | n:  17
IntGELU    | n:  9
IntSoftmax | n:  17
IntGELU    | n:  9
IntSoftmax | n:  17
IntGELU    | n:  9
IntSoftmax | n:  17
IntGELU    | n:  9
IntSoftmax | n:  17
IntGELU    | n:  9
IntSoftmax | n:  17
IntGELU    | n:  9
IntSoftmax | n:  17
IntGELU    | n:  9
IntSoftmax | n:  17
IntGELU    | n:  9
IntSoftmax | n:  17
IntGELU    | n:  9
.calib done
Test: [  0/391]	Time  3.824 ( 3.824)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.87
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=17, intgelu_exp_n=10
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=17, intgelu_exp_n=10, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  17
IntGELU    | n:  10
IntSoftmax | n:  17
IntGELU    | n:  10
IntSoftmax | n:  17
IntGELU    | n:  10
IntSoftmax | n:  17
IntGELU    | n:  10
IntSoftmax | n:  17
IntGELU    | n:  10
IntSoftmax | n:  17
IntGELU    | n:  10
IntSoftmax | n:  17
IntGELU    | n:  10
IntSoftmax | n:  17
IntGELU    | n:  10
IntSoftmax | n:  17
IntGELU    | n:  10
IntSoftmax | n:  17
IntGELU    | n:  10
IntSoftmax | n:  17
IntGELU    | n:  10
IntSoftmax | n:  17
IntGELU    | n:  10
.calib done
Test: [  0/391]	Time  3.912 ( 3.912)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=17, intgelu_exp_n=11
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=17, intgelu_exp_n=11, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  17
IntGELU    | n:  11
IntSoftmax | n:  17
IntGELU    | n:  11
IntSoftmax | n:  17
IntGELU    | n:  11
IntSoftmax | n:  17
IntGELU    | n:  11
IntSoftmax | n:  17
IntGELU    | n:  11
IntSoftmax | n:  17
IntGELU    | n:  11
IntSoftmax | n:  17
IntGELU    | n:  11
IntSoftmax | n:  17
IntGELU    | n:  11
IntSoftmax | n:  17
IntGELU    | n:  11
IntSoftmax | n:  17
IntGELU    | n:  11
IntSoftmax | n:  17
IntGELU    | n:  11
IntSoftmax | n:  17
IntGELU    | n:  11
.calib done
Test: [  0/391]	Time  3.902 ( 3.902)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.98
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=17, intgelu_exp_n=12
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=17, intgelu_exp_n=12, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  17
IntGELU    | n:  12
IntSoftmax | n:  17
IntGELU    | n:  12
IntSoftmax | n:  17
IntGELU    | n:  12
IntSoftmax | n:  17
IntGELU    | n:  12
IntSoftmax | n:  17
IntGELU    | n:  12
IntSoftmax | n:  17
IntGELU    | n:  12
IntSoftmax | n:  17
IntGELU    | n:  12
IntSoftmax | n:  17
IntGELU    | n:  12
IntSoftmax | n:  17
IntGELU    | n:  12
IntSoftmax | n:  17
IntGELU    | n:  12
IntSoftmax | n:  17
IntGELU    | n:  12
IntSoftmax | n:  17
IntGELU    | n:  12
.calib done
Test: [  0/391]	Time  3.954 ( 3.954)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.06
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=17, intgelu_exp_n=13
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=17, intgelu_exp_n=13, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  17
IntGELU    | n:  13
IntSoftmax | n:  17
IntGELU    | n:  13
IntSoftmax | n:  17
IntGELU    | n:  13
IntSoftmax | n:  17
IntGELU    | n:  13
IntSoftmax | n:  17
IntGELU    | n:  13
IntSoftmax | n:  17
IntGELU    | n:  13
IntSoftmax | n:  17
IntGELU    | n:  13
IntSoftmax | n:  17
IntGELU    | n:  13
IntSoftmax | n:  17
IntGELU    | n:  13
IntSoftmax | n:  17
IntGELU    | n:  13
IntSoftmax | n:  17
IntGELU    | n:  13
IntSoftmax | n:  17
IntGELU    | n:  13
.calib done
Test: [  0/391]	Time  3.945 ( 3.945)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.98
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=17, intgelu_exp_n=14
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=17, intgelu_exp_n=14, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  17
IntGELU    | n:  14
IntSoftmax | n:  17
IntGELU    | n:  14
IntSoftmax | n:  17
IntGELU    | n:  14
IntSoftmax | n:  17
IntGELU    | n:  14
IntSoftmax | n:  17
IntGELU    | n:  14
IntSoftmax | n:  17
IntGELU    | n:  14
IntSoftmax | n:  17
IntGELU    | n:  14
IntSoftmax | n:  17
IntGELU    | n:  14
IntSoftmax | n:  17
IntGELU    | n:  14
IntSoftmax | n:  17
IntGELU    | n:  14
IntSoftmax | n:  17
IntGELU    | n:  14
IntSoftmax | n:  17
IntGELU    | n:  14
.calib done
Test: [  0/391]	Time  3.924 ( 3.924)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.02
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=17, intgelu_exp_n=15
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=17, intgelu_exp_n=15, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  17
IntGELU    | n:  15
IntSoftmax | n:  17
IntGELU    | n:  15
IntSoftmax | n:  17
IntGELU    | n:  15
IntSoftmax | n:  17
IntGELU    | n:  15
IntSoftmax | n:  17
IntGELU    | n:  15
IntSoftmax | n:  17
IntGELU    | n:  15
IntSoftmax | n:  17
IntGELU    | n:  15
IntSoftmax | n:  17
IntGELU    | n:  15
IntSoftmax | n:  17
IntGELU    | n:  15
IntSoftmax | n:  17
IntGELU    | n:  15
IntSoftmax | n:  17
IntGELU    | n:  15
IntSoftmax | n:  17
IntGELU    | n:  15
.calib done
Test: [  0/391]	Time  3.663 ( 3.663)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.67
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=17, intgelu_exp_n=16
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=17, intgelu_exp_n=16, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  17
IntGELU    | n:  16
IntSoftmax | n:  17
IntGELU    | n:  16
IntSoftmax | n:  17
IntGELU    | n:  16
IntSoftmax | n:  17
IntGELU    | n:  16
IntSoftmax | n:  17
IntGELU    | n:  16
IntSoftmax | n:  17
IntGELU    | n:  16
IntSoftmax | n:  17
IntGELU    | n:  16
IntSoftmax | n:  17
IntGELU    | n:  16
IntSoftmax | n:  17
IntGELU    | n:  16
IntSoftmax | n:  17
IntGELU    | n:  16
IntSoftmax | n:  17
IntGELU    | n:  16
IntSoftmax | n:  17
IntGELU    | n:  16
.calib done
Test: [  0/391]	Time  3.758 ( 3.758)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.74
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=17, intgelu_exp_n=17
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=17, intgelu_exp_n=17, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  17
IntGELU    | n:  17
IntSoftmax | n:  17
IntGELU    | n:  17
IntSoftmax | n:  17
IntGELU    | n:  17
IntSoftmax | n:  17
IntGELU    | n:  17
IntSoftmax | n:  17
IntGELU    | n:  17
IntSoftmax | n:  17
IntGELU    | n:  17
IntSoftmax | n:  17
IntGELU    | n:  17
IntSoftmax | n:  17
IntGELU    | n:  17
IntSoftmax | n:  17
IntGELU    | n:  17
IntSoftmax | n:  17
IntGELU    | n:  17
IntSoftmax | n:  17
IntGELU    | n:  17
IntSoftmax | n:  17
IntGELU    | n:  17
.calib done
Test: [  0/391]	Time  3.782 ( 3.782)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.77
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=17, intgelu_exp_n=18
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=17, intgelu_exp_n=18, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  17
IntGELU    | n:  18
IntSoftmax | n:  17
IntGELU    | n:  18
IntSoftmax | n:  17
IntGELU    | n:  18
IntSoftmax | n:  17
IntGELU    | n:  18
IntSoftmax | n:  17
IntGELU    | n:  18
IntSoftmax | n:  17
IntGELU    | n:  18
IntSoftmax | n:  17
IntGELU    | n:  18
IntSoftmax | n:  17
IntGELU    | n:  18
IntSoftmax | n:  17
IntGELU    | n:  18
IntSoftmax | n:  17
IntGELU    | n:  18
IntSoftmax | n:  17
IntGELU    | n:  18
IntSoftmax | n:  17
IntGELU    | n:  18
.calib done
Test: [  0/391]	Time  3.943 ( 3.943)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=17, intgelu_exp_n=19
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=17, intgelu_exp_n=19, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  17
IntGELU    | n:  19
IntSoftmax | n:  17
IntGELU    | n:  19
IntSoftmax | n:  17
IntGELU    | n:  19
IntSoftmax | n:  17
IntGELU    | n:  19
IntSoftmax | n:  17
IntGELU    | n:  19
IntSoftmax | n:  17
IntGELU    | n:  19
IntSoftmax | n:  17
IntGELU    | n:  19
IntSoftmax | n:  17
IntGELU    | n:  19
IntSoftmax | n:  17
IntGELU    | n:  19
IntSoftmax | n:  17
IntGELU    | n:  19
IntSoftmax | n:  17
IntGELU    | n:  19
IntSoftmax | n:  17
IntGELU    | n:  19
.calib done
Test: [  0/391]	Time  3.890 ( 3.890)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.88
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=17, intgelu_exp_n=20
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=17, intgelu_exp_n=20, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  17
IntGELU    | n:  20
IntSoftmax | n:  17
IntGELU    | n:  20
IntSoftmax | n:  17
IntGELU    | n:  20
IntSoftmax | n:  17
IntGELU    | n:  20
IntSoftmax | n:  17
IntGELU    | n:  20
IntSoftmax | n:  17
IntGELU    | n:  20
IntSoftmax | n:  17
IntGELU    | n:  20
IntSoftmax | n:  17
IntGELU    | n:  20
IntSoftmax | n:  17
IntGELU    | n:  20
IntSoftmax | n:  17
IntGELU    | n:  20
IntSoftmax | n:  17
IntGELU    | n:  20
IntSoftmax | n:  17
IntGELU    | n:  20
.calib done
Test: [  0/391]	Time  3.915 ( 3.915)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.99
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=17, intgelu_exp_n=21
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=17, intgelu_exp_n=21, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  17
IntGELU    | n:  21
IntSoftmax | n:  17
IntGELU    | n:  21
IntSoftmax | n:  17
IntGELU    | n:  21
IntSoftmax | n:  17
IntGELU    | n:  21
IntSoftmax | n:  17
IntGELU    | n:  21
IntSoftmax | n:  17
IntGELU    | n:  21
IntSoftmax | n:  17
IntGELU    | n:  21
IntSoftmax | n:  17
IntGELU    | n:  21
IntSoftmax | n:  17
IntGELU    | n:  21
IntSoftmax | n:  17
IntGELU    | n:  21
IntSoftmax | n:  17
IntGELU    | n:  21
IntSoftmax | n:  17
IntGELU    | n:  21
.calib done
Test: [  0/391]	Time  3.902 ( 3.902)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.95
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=17, intgelu_exp_n=22
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=17, intgelu_exp_n=22, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  17
IntGELU    | n:  22
IntSoftmax | n:  17
IntGELU    | n:  22
IntSoftmax | n:  17
IntGELU    | n:  22
IntSoftmax | n:  17
IntGELU    | n:  22
IntSoftmax | n:  17
IntGELU    | n:  22
IntSoftmax | n:  17
IntGELU    | n:  22
IntSoftmax | n:  17
IntGELU    | n:  22
IntSoftmax | n:  17
IntGELU    | n:  22
IntSoftmax | n:  17
IntGELU    | n:  22
IntSoftmax | n:  17
IntGELU    | n:  22
IntSoftmax | n:  17
IntGELU    | n:  22
IntSoftmax | n:  17
IntGELU    | n:  22
.calib done
Test: [  0/391]	Time  3.658 ( 3.658)	Acc@1   0.00 (  0.00)	Acc@5   1.56 (  1.56)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 1.562
Time: 8.68
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=17, intgelu_exp_n=23
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=17, intgelu_exp_n=23, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  17
IntGELU    | n:  23
IntSoftmax | n:  17
IntGELU    | n:  23
IntSoftmax | n:  17
IntGELU    | n:  23
IntSoftmax | n:  17
IntGELU    | n:  23
IntSoftmax | n:  17
IntGELU    | n:  23
IntSoftmax | n:  17
IntGELU    | n:  23
IntSoftmax | n:  17
IntGELU    | n:  23
IntSoftmax | n:  17
IntGELU    | n:  23
IntSoftmax | n:  17
IntGELU    | n:  23
IntSoftmax | n:  17
IntGELU    | n:  23
IntSoftmax | n:  17
IntGELU    | n:  23
IntSoftmax | n:  17
IntGELU    | n:  23
.calib done
Test: [  0/391]	Time  3.865 ( 3.865)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.88
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=17, intgelu_exp_n=24
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=17, intgelu_exp_n=24, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  17
IntGELU    | n:  24
IntSoftmax | n:  17
IntGELU    | n:  24
IntSoftmax | n:  17
IntGELU    | n:  24
IntSoftmax | n:  17
IntGELU    | n:  24
IntSoftmax | n:  17
IntGELU    | n:  24
IntSoftmax | n:  17
IntGELU    | n:  24
IntSoftmax | n:  17
IntGELU    | n:  24
IntSoftmax | n:  17
IntGELU    | n:  24
IntSoftmax | n:  17
IntGELU    | n:  24
IntSoftmax | n:  17
IntGELU    | n:  24
IntSoftmax | n:  17
IntGELU    | n:  24
IntSoftmax | n:  17
IntGELU    | n:  24
.calib done
Test: [  0/391]	Time  3.880 ( 3.880)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.92
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=17, intgelu_exp_n=25
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=17, intgelu_exp_n=25, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  17
IntGELU    | n:  25
IntSoftmax | n:  17
IntGELU    | n:  25
IntSoftmax | n:  17
IntGELU    | n:  25
IntSoftmax | n:  17
IntGELU    | n:  25
IntSoftmax | n:  17
IntGELU    | n:  25
IntSoftmax | n:  17
IntGELU    | n:  25
IntSoftmax | n:  17
IntGELU    | n:  25
IntSoftmax | n:  17
IntGELU    | n:  25
IntSoftmax | n:  17
IntGELU    | n:  25
IntSoftmax | n:  17
IntGELU    | n:  25
IntSoftmax | n:  17
IntGELU    | n:  25
IntSoftmax | n:  17
IntGELU    | n:  25
.calib done
Test: [  0/391]	Time  3.933 ( 3.933)	Acc@1   0.00 (  0.00)	Acc@5   0.78 (  0.78)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.781
Time: 8.95
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=17, intgelu_exp_n=26
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=17, intgelu_exp_n=26, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  17
IntGELU    | n:  26
IntSoftmax | n:  17
IntGELU    | n:  26
IntSoftmax | n:  17
IntGELU    | n:  26
IntSoftmax | n:  17
IntGELU    | n:  26
IntSoftmax | n:  17
IntGELU    | n:  26
IntSoftmax | n:  17
IntGELU    | n:  26
IntSoftmax | n:  17
IntGELU    | n:  26
IntSoftmax | n:  17
IntGELU    | n:  26
IntSoftmax | n:  17
IntGELU    | n:  26
IntSoftmax | n:  17
IntGELU    | n:  26
IntSoftmax | n:  17
IntGELU    | n:  26
IntSoftmax | n:  17
IntGELU    | n:  26
.calib done
Test: [  0/391]	Time  3.773 ( 3.773)	Acc@1   0.78 (  0.78)	Acc@5   2.34 (  2.34)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.781 Prec@5 2.344
Time: 8.83
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=17, intgelu_exp_n=27
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=17, intgelu_exp_n=27, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  17
IntGELU    | n:  27
IntSoftmax | n:  17
IntGELU    | n:  27
IntSoftmax | n:  17
IntGELU    | n:  27
IntSoftmax | n:  17
IntGELU    | n:  27
IntSoftmax | n:  17
IntGELU    | n:  27
IntSoftmax | n:  17
IntGELU    | n:  27
IntSoftmax | n:  17
IntGELU    | n:  27
IntSoftmax | n:  17
IntGELU    | n:  27
IntSoftmax | n:  17
IntGELU    | n:  27
IntSoftmax | n:  17
IntGELU    | n:  27
IntSoftmax | n:  17
IntGELU    | n:  27
IntSoftmax | n:  17
IntGELU    | n:  27
.calib done
Test: [  0/391]	Time  3.914 ( 3.914)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=17, intgelu_exp_n=28
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=17, intgelu_exp_n=28, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  17
IntGELU    | n:  28
IntSoftmax | n:  17
IntGELU    | n:  28
IntSoftmax | n:  17
IntGELU    | n:  28
IntSoftmax | n:  17
IntGELU    | n:  28
IntSoftmax | n:  17
IntGELU    | n:  28
IntSoftmax | n:  17
IntGELU    | n:  28
IntSoftmax | n:  17
IntGELU    | n:  28
IntSoftmax | n:  17
IntGELU    | n:  28
IntSoftmax | n:  17
IntGELU    | n:  28
IntSoftmax | n:  17
IntGELU    | n:  28
IntSoftmax | n:  17
IntGELU    | n:  28
IntSoftmax | n:  17
IntGELU    | n:  28
.calib done
Test: [  0/391]	Time  3.916 ( 3.916)	Acc@1   0.00 (  0.00)	Acc@5   0.78 (  0.78)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.781
Time: 8.94
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=17, intgelu_exp_n=29
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=17, intgelu_exp_n=29, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  17
IntGELU    | n:  29
IntSoftmax | n:  17
IntGELU    | n:  29
IntSoftmax | n:  17
IntGELU    | n:  29
IntSoftmax | n:  17
IntGELU    | n:  29
IntSoftmax | n:  17
IntGELU    | n:  29
IntSoftmax | n:  17
IntGELU    | n:  29
IntSoftmax | n:  17
IntGELU    | n:  29
IntSoftmax | n:  17
IntGELU    | n:  29
IntSoftmax | n:  17
IntGELU    | n:  29
IntSoftmax | n:  17
IntGELU    | n:  29
IntSoftmax | n:  17
IntGELU    | n:  29
IntSoftmax | n:  17
IntGELU    | n:  29
.calib done
Test: [  0/391]	Time  3.848 ( 3.848)	Acc@1   0.78 (  0.78)	Acc@5   5.47 (  5.47)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.781 Prec@5 5.469
Time: 8.90
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=17, intgelu_exp_n=30
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=17, intgelu_exp_n=30, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  17
IntGELU    | n:  30
IntSoftmax | n:  17
IntGELU    | n:  30
IntSoftmax | n:  17
IntGELU    | n:  30
IntSoftmax | n:  17
IntGELU    | n:  30
IntSoftmax | n:  17
IntGELU    | n:  30
IntSoftmax | n:  17
IntGELU    | n:  30
IntSoftmax | n:  17
IntGELU    | n:  30
IntSoftmax | n:  17
IntGELU    | n:  30
IntSoftmax | n:  17
IntGELU    | n:  30
IntSoftmax | n:  17
IntGELU    | n:  30
IntSoftmax | n:  17
IntGELU    | n:  30
IntSoftmax | n:  17
IntGELU    | n:  30
.calib done
Test: [  0/391]	Time  3.762 ( 3.762)	Acc@1   1.56 (  1.56)	Acc@5   7.03 (  7.03)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 1.562 Prec@5 7.031
Time: 8.83
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=17, intgelu_exp_n=31
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=17, intgelu_exp_n=31, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  17
IntGELU    | n:  31
IntSoftmax | n:  17
IntGELU    | n:  31
IntSoftmax | n:  17
IntGELU    | n:  31
IntSoftmax | n:  17
IntGELU    | n:  31
IntSoftmax | n:  17
IntGELU    | n:  31
IntSoftmax | n:  17
IntGELU    | n:  31
IntSoftmax | n:  17
IntGELU    | n:  31
IntSoftmax | n:  17
IntGELU    | n:  31
IntSoftmax | n:  17
IntGELU    | n:  31
IntSoftmax | n:  17
IntGELU    | n:  31
IntSoftmax | n:  17
IntGELU    | n:  31
IntSoftmax | n:  17
IntGELU    | n:  31
.calib done
Test: [  0/391]	Time  3.979 ( 3.979)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.03
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=18, intgelu_exp_n=0
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=18, intgelu_exp_n=0, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  18
IntGELU    | n:  0
IntSoftmax | n:  18
IntGELU    | n:  0
IntSoftmax | n:  18
IntGELU    | n:  0
IntSoftmax | n:  18
IntGELU    | n:  0
IntSoftmax | n:  18
IntGELU    | n:  0
IntSoftmax | n:  18
IntGELU    | n:  0
IntSoftmax | n:  18
IntGELU    | n:  0
IntSoftmax | n:  18
IntGELU    | n:  0
IntSoftmax | n:  18
IntGELU    | n:  0
IntSoftmax | n:  18
IntGELU    | n:  0
IntSoftmax | n:  18
IntGELU    | n:  0
IntSoftmax | n:  18
IntGELU    | n:  0
.calib done
Test: [  0/391]	Time  3.765 ( 3.765)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.91
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=18, intgelu_exp_n=1
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=18, intgelu_exp_n=1, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  18
IntGELU    | n:  1
IntSoftmax | n:  18
IntGELU    | n:  1
IntSoftmax | n:  18
IntGELU    | n:  1
IntSoftmax | n:  18
IntGELU    | n:  1
IntSoftmax | n:  18
IntGELU    | n:  1
IntSoftmax | n:  18
IntGELU    | n:  1
IntSoftmax | n:  18
IntGELU    | n:  1
IntSoftmax | n:  18
IntGELU    | n:  1
IntSoftmax | n:  18
IntGELU    | n:  1
IntSoftmax | n:  18
IntGELU    | n:  1
IntSoftmax | n:  18
IntGELU    | n:  1
IntSoftmax | n:  18
IntGELU    | n:  1
.calib done
Test: [  0/391]	Time  3.902 ( 3.902)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.95
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=18, intgelu_exp_n=2
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=18, intgelu_exp_n=2, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  18
IntGELU    | n:  2
IntSoftmax | n:  18
IntGELU    | n:  2
IntSoftmax | n:  18
IntGELU    | n:  2
IntSoftmax | n:  18
IntGELU    | n:  2
IntSoftmax | n:  18
IntGELU    | n:  2
IntSoftmax | n:  18
IntGELU    | n:  2
IntSoftmax | n:  18
IntGELU    | n:  2
IntSoftmax | n:  18
IntGELU    | n:  2
IntSoftmax | n:  18
IntGELU    | n:  2
IntSoftmax | n:  18
IntGELU    | n:  2
IntSoftmax | n:  18
IntGELU    | n:  2
IntSoftmax | n:  18
IntGELU    | n:  2
.calib done
Test: [  0/391]	Time  3.858 ( 3.858)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.95
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=18, intgelu_exp_n=3
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=18, intgelu_exp_n=3, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  18
IntGELU    | n:  3
IntSoftmax | n:  18
IntGELU    | n:  3
IntSoftmax | n:  18
IntGELU    | n:  3
IntSoftmax | n:  18
IntGELU    | n:  3
IntSoftmax | n:  18
IntGELU    | n:  3
IntSoftmax | n:  18
IntGELU    | n:  3
IntSoftmax | n:  18
IntGELU    | n:  3
IntSoftmax | n:  18
IntGELU    | n:  3
IntSoftmax | n:  18
IntGELU    | n:  3
IntSoftmax | n:  18
IntGELU    | n:  3
IntSoftmax | n:  18
IntGELU    | n:  3
IntSoftmax | n:  18
IntGELU    | n:  3
.calib done
Test: [  0/391]	Time  3.969 ( 3.969)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.00
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=18, intgelu_exp_n=4
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=18, intgelu_exp_n=4, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  18
IntGELU    | n:  4
IntSoftmax | n:  18
IntGELU    | n:  4
IntSoftmax | n:  18
IntGELU    | n:  4
IntSoftmax | n:  18
IntGELU    | n:  4
IntSoftmax | n:  18
IntGELU    | n:  4
IntSoftmax | n:  18
IntGELU    | n:  4
IntSoftmax | n:  18
IntGELU    | n:  4
IntSoftmax | n:  18
IntGELU    | n:  4
IntSoftmax | n:  18
IntGELU    | n:  4
IntSoftmax | n:  18
IntGELU    | n:  4
IntSoftmax | n:  18
IntGELU    | n:  4
IntSoftmax | n:  18
IntGELU    | n:  4
.calib done
Test: [  0/391]	Time  4.003 ( 4.003)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.09
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=18, intgelu_exp_n=5
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=18, intgelu_exp_n=5, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  18
IntGELU    | n:  5
IntSoftmax | n:  18
IntGELU    | n:  5
IntSoftmax | n:  18
IntGELU    | n:  5
IntSoftmax | n:  18
IntGELU    | n:  5
IntSoftmax | n:  18
IntGELU    | n:  5
IntSoftmax | n:  18
IntGELU    | n:  5
IntSoftmax | n:  18
IntGELU    | n:  5
IntSoftmax | n:  18
IntGELU    | n:  5
IntSoftmax | n:  18
IntGELU    | n:  5
IntSoftmax | n:  18
IntGELU    | n:  5
IntSoftmax | n:  18
IntGELU    | n:  5
IntSoftmax | n:  18
IntGELU    | n:  5
.calib done
Test: [  0/391]	Time  3.856 ( 3.856)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.91
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=18, intgelu_exp_n=6
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=18, intgelu_exp_n=6, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  18
IntGELU    | n:  6
IntSoftmax | n:  18
IntGELU    | n:  6
IntSoftmax | n:  18
IntGELU    | n:  6
IntSoftmax | n:  18
IntGELU    | n:  6
IntSoftmax | n:  18
IntGELU    | n:  6
IntSoftmax | n:  18
IntGELU    | n:  6
IntSoftmax | n:  18
IntGELU    | n:  6
IntSoftmax | n:  18
IntGELU    | n:  6
IntSoftmax | n:  18
IntGELU    | n:  6
IntSoftmax | n:  18
IntGELU    | n:  6
IntSoftmax | n:  18
IntGELU    | n:  6
IntSoftmax | n:  18
IntGELU    | n:  6
.calib done
Test: [  0/391]	Time  3.846 ( 3.846)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.88
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=18, intgelu_exp_n=7
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=18, intgelu_exp_n=7, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  18
IntGELU    | n:  7
IntSoftmax | n:  18
IntGELU    | n:  7
IntSoftmax | n:  18
IntGELU    | n:  7
IntSoftmax | n:  18
IntGELU    | n:  7
IntSoftmax | n:  18
IntGELU    | n:  7
IntSoftmax | n:  18
IntGELU    | n:  7
IntSoftmax | n:  18
IntGELU    | n:  7
IntSoftmax | n:  18
IntGELU    | n:  7
IntSoftmax | n:  18
IntGELU    | n:  7
IntSoftmax | n:  18
IntGELU    | n:  7
IntSoftmax | n:  18
IntGELU    | n:  7
IntSoftmax | n:  18
IntGELU    | n:  7
.calib done
Test: [  0/391]	Time  3.966 ( 3.966)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.01
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=18, intgelu_exp_n=8
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=18, intgelu_exp_n=8, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  18
IntGELU    | n:  8
IntSoftmax | n:  18
IntGELU    | n:  8
IntSoftmax | n:  18
IntGELU    | n:  8
IntSoftmax | n:  18
IntGELU    | n:  8
IntSoftmax | n:  18
IntGELU    | n:  8
IntSoftmax | n:  18
IntGELU    | n:  8
IntSoftmax | n:  18
IntGELU    | n:  8
IntSoftmax | n:  18
IntGELU    | n:  8
IntSoftmax | n:  18
IntGELU    | n:  8
IntSoftmax | n:  18
IntGELU    | n:  8
IntSoftmax | n:  18
IntGELU    | n:  8
IntSoftmax | n:  18
IntGELU    | n:  8
.calib done
Test: [  0/391]	Time  3.832 ( 3.832)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.85
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=18, intgelu_exp_n=9
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=18, intgelu_exp_n=9, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  18
IntGELU    | n:  9
IntSoftmax | n:  18
IntGELU    | n:  9
IntSoftmax | n:  18
IntGELU    | n:  9
IntSoftmax | n:  18
IntGELU    | n:  9
IntSoftmax | n:  18
IntGELU    | n:  9
IntSoftmax | n:  18
IntGELU    | n:  9
IntSoftmax | n:  18
IntGELU    | n:  9
IntSoftmax | n:  18
IntGELU    | n:  9
IntSoftmax | n:  18
IntGELU    | n:  9
IntSoftmax | n:  18
IntGELU    | n:  9
IntSoftmax | n:  18
IntGELU    | n:  9
IntSoftmax | n:  18
IntGELU    | n:  9
.calib done
Test: [  0/391]	Time  3.853 ( 3.853)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.87
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=18, intgelu_exp_n=10
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=18, intgelu_exp_n=10, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  18
IntGELU    | n:  10
IntSoftmax | n:  18
IntGELU    | n:  10
IntSoftmax | n:  18
IntGELU    | n:  10
IntSoftmax | n:  18
IntGELU    | n:  10
IntSoftmax | n:  18
IntGELU    | n:  10
IntSoftmax | n:  18
IntGELU    | n:  10
IntSoftmax | n:  18
IntGELU    | n:  10
IntSoftmax | n:  18
IntGELU    | n:  10
IntSoftmax | n:  18
IntGELU    | n:  10
IntSoftmax | n:  18
IntGELU    | n:  10
IntSoftmax | n:  18
IntGELU    | n:  10
IntSoftmax | n:  18
IntGELU    | n:  10
.calib done
Test: [  0/391]	Time  3.954 ( 3.954)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=18, intgelu_exp_n=11
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=18, intgelu_exp_n=11, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  18
IntGELU    | n:  11
IntSoftmax | n:  18
IntGELU    | n:  11
IntSoftmax | n:  18
IntGELU    | n:  11
IntSoftmax | n:  18
IntGELU    | n:  11
IntSoftmax | n:  18
IntGELU    | n:  11
IntSoftmax | n:  18
IntGELU    | n:  11
IntSoftmax | n:  18
IntGELU    | n:  11
IntSoftmax | n:  18
IntGELU    | n:  11
IntSoftmax | n:  18
IntGELU    | n:  11
IntSoftmax | n:  18
IntGELU    | n:  11
IntSoftmax | n:  18
IntGELU    | n:  11
IntSoftmax | n:  18
IntGELU    | n:  11
.calib done
Test: [  0/391]	Time  3.746 ( 3.746)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.80
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=18, intgelu_exp_n=12
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=18, intgelu_exp_n=12, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  18
IntGELU    | n:  12
IntSoftmax | n:  18
IntGELU    | n:  12
IntSoftmax | n:  18
IntGELU    | n:  12
IntSoftmax | n:  18
IntGELU    | n:  12
IntSoftmax | n:  18
IntGELU    | n:  12
IntSoftmax | n:  18
IntGELU    | n:  12
IntSoftmax | n:  18
IntGELU    | n:  12
IntSoftmax | n:  18
IntGELU    | n:  12
IntSoftmax | n:  18
IntGELU    | n:  12
IntSoftmax | n:  18
IntGELU    | n:  12
IntSoftmax | n:  18
IntGELU    | n:  12
IntSoftmax | n:  18
IntGELU    | n:  12
.calib done
Test: [  0/391]	Time  3.975 ( 3.975)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.99
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=18, intgelu_exp_n=13
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=18, intgelu_exp_n=13, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  18
IntGELU    | n:  13
IntSoftmax | n:  18
IntGELU    | n:  13
IntSoftmax | n:  18
IntGELU    | n:  13
IntSoftmax | n:  18
IntGELU    | n:  13
IntSoftmax | n:  18
IntGELU    | n:  13
IntSoftmax | n:  18
IntGELU    | n:  13
IntSoftmax | n:  18
IntGELU    | n:  13
IntSoftmax | n:  18
IntGELU    | n:  13
IntSoftmax | n:  18
IntGELU    | n:  13
IntSoftmax | n:  18
IntGELU    | n:  13
IntSoftmax | n:  18
IntGELU    | n:  13
IntSoftmax | n:  18
IntGELU    | n:  13
.calib done
Test: [  0/391]	Time  3.823 ( 3.823)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.83
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=18, intgelu_exp_n=14
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=18, intgelu_exp_n=14, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  18
IntGELU    | n:  14
IntSoftmax | n:  18
IntGELU    | n:  14
IntSoftmax | n:  18
IntGELU    | n:  14
IntSoftmax | n:  18
IntGELU    | n:  14
IntSoftmax | n:  18
IntGELU    | n:  14
IntSoftmax | n:  18
IntGELU    | n:  14
IntSoftmax | n:  18
IntGELU    | n:  14
IntSoftmax | n:  18
IntGELU    | n:  14
IntSoftmax | n:  18
IntGELU    | n:  14
IntSoftmax | n:  18
IntGELU    | n:  14
IntSoftmax | n:  18
IntGELU    | n:  14
IntSoftmax | n:  18
IntGELU    | n:  14
.calib done
Test: [  0/391]	Time  3.848 ( 3.848)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.93
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=18, intgelu_exp_n=15
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=18, intgelu_exp_n=15, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  18
IntGELU    | n:  15
IntSoftmax | n:  18
IntGELU    | n:  15
IntSoftmax | n:  18
IntGELU    | n:  15
IntSoftmax | n:  18
IntGELU    | n:  15
IntSoftmax | n:  18
IntGELU    | n:  15
IntSoftmax | n:  18
IntGELU    | n:  15
IntSoftmax | n:  18
IntGELU    | n:  15
IntSoftmax | n:  18
IntGELU    | n:  15
IntSoftmax | n:  18
IntGELU    | n:  15
IntSoftmax | n:  18
IntGELU    | n:  15
IntSoftmax | n:  18
IntGELU    | n:  15
IntSoftmax | n:  18
IntGELU    | n:  15
.calib done
Test: [  0/391]	Time  3.785 ( 3.785)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.82
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=18, intgelu_exp_n=16
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=18, intgelu_exp_n=16, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  18
IntGELU    | n:  16
IntSoftmax | n:  18
IntGELU    | n:  16
IntSoftmax | n:  18
IntGELU    | n:  16
IntSoftmax | n:  18
IntGELU    | n:  16
IntSoftmax | n:  18
IntGELU    | n:  16
IntSoftmax | n:  18
IntGELU    | n:  16
IntSoftmax | n:  18
IntGELU    | n:  16
IntSoftmax | n:  18
IntGELU    | n:  16
IntSoftmax | n:  18
IntGELU    | n:  16
IntSoftmax | n:  18
IntGELU    | n:  16
IntSoftmax | n:  18
IntGELU    | n:  16
IntSoftmax | n:  18
IntGELU    | n:  16
.calib done
Test: [  0/391]	Time  3.786 ( 3.786)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.78
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=18, intgelu_exp_n=17
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=18, intgelu_exp_n=17, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  18
IntGELU    | n:  17
IntSoftmax | n:  18
IntGELU    | n:  17
IntSoftmax | n:  18
IntGELU    | n:  17
IntSoftmax | n:  18
IntGELU    | n:  17
IntSoftmax | n:  18
IntGELU    | n:  17
IntSoftmax | n:  18
IntGELU    | n:  17
IntSoftmax | n:  18
IntGELU    | n:  17
IntSoftmax | n:  18
IntGELU    | n:  17
IntSoftmax | n:  18
IntGELU    | n:  17
IntSoftmax | n:  18
IntGELU    | n:  17
IntSoftmax | n:  18
IntGELU    | n:  17
IntSoftmax | n:  18
IntGELU    | n:  17
.calib done
Test: [  0/391]	Time  3.890 ( 3.890)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.91
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=18, intgelu_exp_n=18
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=18, intgelu_exp_n=18, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  18
IntGELU    | n:  18
IntSoftmax | n:  18
IntGELU    | n:  18
IntSoftmax | n:  18
IntGELU    | n:  18
IntSoftmax | n:  18
IntGELU    | n:  18
IntSoftmax | n:  18
IntGELU    | n:  18
IntSoftmax | n:  18
IntGELU    | n:  18
IntSoftmax | n:  18
IntGELU    | n:  18
IntSoftmax | n:  18
IntGELU    | n:  18
IntSoftmax | n:  18
IntGELU    | n:  18
IntSoftmax | n:  18
IntGELU    | n:  18
IntSoftmax | n:  18
IntGELU    | n:  18
IntSoftmax | n:  18
IntGELU    | n:  18
.calib done
Test: [  0/391]	Time  4.028 ( 4.028)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.06
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=18, intgelu_exp_n=19
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=18, intgelu_exp_n=19, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  18
IntGELU    | n:  19
IntSoftmax | n:  18
IntGELU    | n:  19
IntSoftmax | n:  18
IntGELU    | n:  19
IntSoftmax | n:  18
IntGELU    | n:  19
IntSoftmax | n:  18
IntGELU    | n:  19
IntSoftmax | n:  18
IntGELU    | n:  19
IntSoftmax | n:  18
IntGELU    | n:  19
IntSoftmax | n:  18
IntGELU    | n:  19
IntSoftmax | n:  18
IntGELU    | n:  19
IntSoftmax | n:  18
IntGELU    | n:  19
IntSoftmax | n:  18
IntGELU    | n:  19
IntSoftmax | n:  18
IntGELU    | n:  19
.calib done
Test: [  0/391]	Time  3.899 ( 3.899)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.98
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=18, intgelu_exp_n=20
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=18, intgelu_exp_n=20, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  18
IntGELU    | n:  20
IntSoftmax | n:  18
IntGELU    | n:  20
IntSoftmax | n:  18
IntGELU    | n:  20
IntSoftmax | n:  18
IntGELU    | n:  20
IntSoftmax | n:  18
IntGELU    | n:  20
IntSoftmax | n:  18
IntGELU    | n:  20
IntSoftmax | n:  18
IntGELU    | n:  20
IntSoftmax | n:  18
IntGELU    | n:  20
IntSoftmax | n:  18
IntGELU    | n:  20
IntSoftmax | n:  18
IntGELU    | n:  20
IntSoftmax | n:  18
IntGELU    | n:  20
IntSoftmax | n:  18
IntGELU    | n:  20
.calib done
Test: [  0/391]	Time  3.644 ( 3.644)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.70
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=18, intgelu_exp_n=21
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=18, intgelu_exp_n=21, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  18
IntGELU    | n:  21
IntSoftmax | n:  18
IntGELU    | n:  21
IntSoftmax | n:  18
IntGELU    | n:  21
IntSoftmax | n:  18
IntGELU    | n:  21
IntSoftmax | n:  18
IntGELU    | n:  21
IntSoftmax | n:  18
IntGELU    | n:  21
IntSoftmax | n:  18
IntGELU    | n:  21
IntSoftmax | n:  18
IntGELU    | n:  21
IntSoftmax | n:  18
IntGELU    | n:  21
IntSoftmax | n:  18
IntGELU    | n:  21
IntSoftmax | n:  18
IntGELU    | n:  21
IntSoftmax | n:  18
IntGELU    | n:  21
.calib done
Test: [  0/391]	Time  3.864 ( 3.864)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=18, intgelu_exp_n=22
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=18, intgelu_exp_n=22, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  18
IntGELU    | n:  22
IntSoftmax | n:  18
IntGELU    | n:  22
IntSoftmax | n:  18
IntGELU    | n:  22
IntSoftmax | n:  18
IntGELU    | n:  22
IntSoftmax | n:  18
IntGELU    | n:  22
IntSoftmax | n:  18
IntGELU    | n:  22
IntSoftmax | n:  18
IntGELU    | n:  22
IntSoftmax | n:  18
IntGELU    | n:  22
IntSoftmax | n:  18
IntGELU    | n:  22
IntSoftmax | n:  18
IntGELU    | n:  22
IntSoftmax | n:  18
IntGELU    | n:  22
IntSoftmax | n:  18
IntGELU    | n:  22
.calib done
Test: [  0/391]	Time  3.940 ( 3.940)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.92
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=18, intgelu_exp_n=23
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=18, intgelu_exp_n=23, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  18
IntGELU    | n:  23
IntSoftmax | n:  18
IntGELU    | n:  23
IntSoftmax | n:  18
IntGELU    | n:  23
IntSoftmax | n:  18
IntGELU    | n:  23
IntSoftmax | n:  18
IntGELU    | n:  23
IntSoftmax | n:  18
IntGELU    | n:  23
IntSoftmax | n:  18
IntGELU    | n:  23
IntSoftmax | n:  18
IntGELU    | n:  23
IntSoftmax | n:  18
IntGELU    | n:  23
IntSoftmax | n:  18
IntGELU    | n:  23
IntSoftmax | n:  18
IntGELU    | n:  23
IntSoftmax | n:  18
IntGELU    | n:  23
.calib done
Test: [  0/391]	Time  3.917 ( 3.917)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.02
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=18, intgelu_exp_n=24
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=18, intgelu_exp_n=24, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  18
IntGELU    | n:  24
IntSoftmax | n:  18
IntGELU    | n:  24
IntSoftmax | n:  18
IntGELU    | n:  24
IntSoftmax | n:  18
IntGELU    | n:  24
IntSoftmax | n:  18
IntGELU    | n:  24
IntSoftmax | n:  18
IntGELU    | n:  24
IntSoftmax | n:  18
IntGELU    | n:  24
IntSoftmax | n:  18
IntGELU    | n:  24
IntSoftmax | n:  18
IntGELU    | n:  24
IntSoftmax | n:  18
IntGELU    | n:  24
IntSoftmax | n:  18
IntGELU    | n:  24
IntSoftmax | n:  18
IntGELU    | n:  24
.calib done
Test: [  0/391]	Time  3.945 ( 3.945)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.99
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=18, intgelu_exp_n=25
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=18, intgelu_exp_n=25, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  18
IntGELU    | n:  25
IntSoftmax | n:  18
IntGELU    | n:  25
IntSoftmax | n:  18
IntGELU    | n:  25
IntSoftmax | n:  18
IntGELU    | n:  25
IntSoftmax | n:  18
IntGELU    | n:  25
IntSoftmax | n:  18
IntGELU    | n:  25
IntSoftmax | n:  18
IntGELU    | n:  25
IntSoftmax | n:  18
IntGELU    | n:  25
IntSoftmax | n:  18
IntGELU    | n:  25
IntSoftmax | n:  18
IntGELU    | n:  25
IntSoftmax | n:  18
IntGELU    | n:  25
IntSoftmax | n:  18
IntGELU    | n:  25
.calib done
Test: [  0/391]	Time  3.955 ( 3.955)	Acc@1   0.00 (  0.00)	Acc@5   0.78 (  0.78)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.781
Time: 9.02
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=18, intgelu_exp_n=26
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=18, intgelu_exp_n=26, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  18
IntGELU    | n:  26
IntSoftmax | n:  18
IntGELU    | n:  26
IntSoftmax | n:  18
IntGELU    | n:  26
IntSoftmax | n:  18
IntGELU    | n:  26
IntSoftmax | n:  18
IntGELU    | n:  26
IntSoftmax | n:  18
IntGELU    | n:  26
IntSoftmax | n:  18
IntGELU    | n:  26
IntSoftmax | n:  18
IntGELU    | n:  26
IntSoftmax | n:  18
IntGELU    | n:  26
IntSoftmax | n:  18
IntGELU    | n:  26
IntSoftmax | n:  18
IntGELU    | n:  26
IntSoftmax | n:  18
IntGELU    | n:  26
.calib done
Test: [  0/391]	Time  3.862 ( 3.862)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.92
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=18, intgelu_exp_n=27
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=18, intgelu_exp_n=27, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  18
IntGELU    | n:  27
IntSoftmax | n:  18
IntGELU    | n:  27
IntSoftmax | n:  18
IntGELU    | n:  27
IntSoftmax | n:  18
IntGELU    | n:  27
IntSoftmax | n:  18
IntGELU    | n:  27
IntSoftmax | n:  18
IntGELU    | n:  27
IntSoftmax | n:  18
IntGELU    | n:  27
IntSoftmax | n:  18
IntGELU    | n:  27
IntSoftmax | n:  18
IntGELU    | n:  27
IntSoftmax | n:  18
IntGELU    | n:  27
IntSoftmax | n:  18
IntGELU    | n:  27
IntSoftmax | n:  18
IntGELU    | n:  27
.calib done
Test: [  0/391]	Time  3.933 ( 3.933)	Acc@1   0.00 (  0.00)	Acc@5   1.56 (  1.56)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 1.562
Time: 8.98
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=18, intgelu_exp_n=28
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=18, intgelu_exp_n=28, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  18
IntGELU    | n:  28
IntSoftmax | n:  18
IntGELU    | n:  28
IntSoftmax | n:  18
IntGELU    | n:  28
IntSoftmax | n:  18
IntGELU    | n:  28
IntSoftmax | n:  18
IntGELU    | n:  28
IntSoftmax | n:  18
IntGELU    | n:  28
IntSoftmax | n:  18
IntGELU    | n:  28
IntSoftmax | n:  18
IntGELU    | n:  28
IntSoftmax | n:  18
IntGELU    | n:  28
IntSoftmax | n:  18
IntGELU    | n:  28
IntSoftmax | n:  18
IntGELU    | n:  28
IntSoftmax | n:  18
IntGELU    | n:  28
.calib done
Test: [  0/391]	Time  4.005 ( 4.005)	Acc@1   5.47 (  5.47)	Acc@5  14.06 ( 14.06)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 5.469 Prec@5 14.062
Time: 9.05
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=18, intgelu_exp_n=29
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=18, intgelu_exp_n=29, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  18
IntGELU    | n:  29
IntSoftmax | n:  18
IntGELU    | n:  29
IntSoftmax | n:  18
IntGELU    | n:  29
IntSoftmax | n:  18
IntGELU    | n:  29
IntSoftmax | n:  18
IntGELU    | n:  29
IntSoftmax | n:  18
IntGELU    | n:  29
IntSoftmax | n:  18
IntGELU    | n:  29
IntSoftmax | n:  18
IntGELU    | n:  29
IntSoftmax | n:  18
IntGELU    | n:  29
IntSoftmax | n:  18
IntGELU    | n:  29
IntSoftmax | n:  18
IntGELU    | n:  29
IntSoftmax | n:  18
IntGELU    | n:  29
.calib done
Test: [  0/391]	Time  3.997 ( 3.997)	Acc@1  17.19 ( 17.19)	Acc@5  41.41 ( 41.41)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 17.188 Prec@5 41.406
Time: 9.07
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=18, intgelu_exp_n=30
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=18, intgelu_exp_n=30, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  18
IntGELU    | n:  30
IntSoftmax | n:  18
IntGELU    | n:  30
IntSoftmax | n:  18
IntGELU    | n:  30
IntSoftmax | n:  18
IntGELU    | n:  30
IntSoftmax | n:  18
IntGELU    | n:  30
IntSoftmax | n:  18
IntGELU    | n:  30
IntSoftmax | n:  18
IntGELU    | n:  30
IntSoftmax | n:  18
IntGELU    | n:  30
IntSoftmax | n:  18
IntGELU    | n:  30
IntSoftmax | n:  18
IntGELU    | n:  30
IntSoftmax | n:  18
IntGELU    | n:  30
IntSoftmax | n:  18
IntGELU    | n:  30
.calib done
Test: [  0/391]	Time  3.929 ( 3.929)	Acc@1   3.12 (  3.12)	Acc@5  10.16 ( 10.16)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 3.125 Prec@5 10.156
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=18, intgelu_exp_n=31
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=18, intgelu_exp_n=31, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  18
IntGELU    | n:  31
IntSoftmax | n:  18
IntGELU    | n:  31
IntSoftmax | n:  18
IntGELU    | n:  31
IntSoftmax | n:  18
IntGELU    | n:  31
IntSoftmax | n:  18
IntGELU    | n:  31
IntSoftmax | n:  18
IntGELU    | n:  31
IntSoftmax | n:  18
IntGELU    | n:  31
IntSoftmax | n:  18
IntGELU    | n:  31
IntSoftmax | n:  18
IntGELU    | n:  31
IntSoftmax | n:  18
IntGELU    | n:  31
IntSoftmax | n:  18
IntGELU    | n:  31
IntSoftmax | n:  18
IntGELU    | n:  31
.calib done
Test: [  0/391]	Time  3.946 ( 3.946)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.99
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=19, intgelu_exp_n=0
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=19, intgelu_exp_n=0, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  19
IntGELU    | n:  0
IntSoftmax | n:  19
IntGELU    | n:  0
IntSoftmax | n:  19
IntGELU    | n:  0
IntSoftmax | n:  19
IntGELU    | n:  0
IntSoftmax | n:  19
IntGELU    | n:  0
IntSoftmax | n:  19
IntGELU    | n:  0
IntSoftmax | n:  19
IntGELU    | n:  0
IntSoftmax | n:  19
IntGELU    | n:  0
IntSoftmax | n:  19
IntGELU    | n:  0
IntSoftmax | n:  19
IntGELU    | n:  0
IntSoftmax | n:  19
IntGELU    | n:  0
IntSoftmax | n:  19
IntGELU    | n:  0
.calib done
Test: [  0/391]	Time  3.618 ( 3.618)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.64
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=19, intgelu_exp_n=1
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=19, intgelu_exp_n=1, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  19
IntGELU    | n:  1
IntSoftmax | n:  19
IntGELU    | n:  1
IntSoftmax | n:  19
IntGELU    | n:  1
IntSoftmax | n:  19
IntGELU    | n:  1
IntSoftmax | n:  19
IntGELU    | n:  1
IntSoftmax | n:  19
IntGELU    | n:  1
IntSoftmax | n:  19
IntGELU    | n:  1
IntSoftmax | n:  19
IntGELU    | n:  1
IntSoftmax | n:  19
IntGELU    | n:  1
IntSoftmax | n:  19
IntGELU    | n:  1
IntSoftmax | n:  19
IntGELU    | n:  1
IntSoftmax | n:  19
IntGELU    | n:  1
.calib done
Test: [  0/391]	Time  4.001 ( 4.001)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.04
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=19, intgelu_exp_n=2
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=19, intgelu_exp_n=2, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  19
IntGELU    | n:  2
IntSoftmax | n:  19
IntGELU    | n:  2
IntSoftmax | n:  19
IntGELU    | n:  2
IntSoftmax | n:  19
IntGELU    | n:  2
IntSoftmax | n:  19
IntGELU    | n:  2
IntSoftmax | n:  19
IntGELU    | n:  2
IntSoftmax | n:  19
IntGELU    | n:  2
IntSoftmax | n:  19
IntGELU    | n:  2
IntSoftmax | n:  19
IntGELU    | n:  2
IntSoftmax | n:  19
IntGELU    | n:  2
IntSoftmax | n:  19
IntGELU    | n:  2
IntSoftmax | n:  19
IntGELU    | n:  2
.calib done
Test: [  0/391]	Time  3.773 ( 3.773)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.84
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=19, intgelu_exp_n=3
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=19, intgelu_exp_n=3, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  19
IntGELU    | n:  3
IntSoftmax | n:  19
IntGELU    | n:  3
IntSoftmax | n:  19
IntGELU    | n:  3
IntSoftmax | n:  19
IntGELU    | n:  3
IntSoftmax | n:  19
IntGELU    | n:  3
IntSoftmax | n:  19
IntGELU    | n:  3
IntSoftmax | n:  19
IntGELU    | n:  3
IntSoftmax | n:  19
IntGELU    | n:  3
IntSoftmax | n:  19
IntGELU    | n:  3
IntSoftmax | n:  19
IntGELU    | n:  3
IntSoftmax | n:  19
IntGELU    | n:  3
IntSoftmax | n:  19
IntGELU    | n:  3
.calib done
Test: [  0/391]	Time  3.855 ( 3.855)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.90
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=19, intgelu_exp_n=4
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=19, intgelu_exp_n=4, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  19
IntGELU    | n:  4
IntSoftmax | n:  19
IntGELU    | n:  4
IntSoftmax | n:  19
IntGELU    | n:  4
IntSoftmax | n:  19
IntGELU    | n:  4
IntSoftmax | n:  19
IntGELU    | n:  4
IntSoftmax | n:  19
IntGELU    | n:  4
IntSoftmax | n:  19
IntGELU    | n:  4
IntSoftmax | n:  19
IntGELU    | n:  4
IntSoftmax | n:  19
IntGELU    | n:  4
IntSoftmax | n:  19
IntGELU    | n:  4
IntSoftmax | n:  19
IntGELU    | n:  4
IntSoftmax | n:  19
IntGELU    | n:  4
.calib done
Test: [  0/391]	Time  3.762 ( 3.762)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.79
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=19, intgelu_exp_n=5
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=19, intgelu_exp_n=5, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  19
IntGELU    | n:  5
IntSoftmax | n:  19
IntGELU    | n:  5
IntSoftmax | n:  19
IntGELU    | n:  5
IntSoftmax | n:  19
IntGELU    | n:  5
IntSoftmax | n:  19
IntGELU    | n:  5
IntSoftmax | n:  19
IntGELU    | n:  5
IntSoftmax | n:  19
IntGELU    | n:  5
IntSoftmax | n:  19
IntGELU    | n:  5
IntSoftmax | n:  19
IntGELU    | n:  5
IntSoftmax | n:  19
IntGELU    | n:  5
IntSoftmax | n:  19
IntGELU    | n:  5
IntSoftmax | n:  19
IntGELU    | n:  5
.calib done
Test: [  0/391]	Time  3.945 ( 3.945)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.03
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=19, intgelu_exp_n=6
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=19, intgelu_exp_n=6, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  19
IntGELU    | n:  6
IntSoftmax | n:  19
IntGELU    | n:  6
IntSoftmax | n:  19
IntGELU    | n:  6
IntSoftmax | n:  19
IntGELU    | n:  6
IntSoftmax | n:  19
IntGELU    | n:  6
IntSoftmax | n:  19
IntGELU    | n:  6
IntSoftmax | n:  19
IntGELU    | n:  6
IntSoftmax | n:  19
IntGELU    | n:  6
IntSoftmax | n:  19
IntGELU    | n:  6
IntSoftmax | n:  19
IntGELU    | n:  6
IntSoftmax | n:  19
IntGELU    | n:  6
IntSoftmax | n:  19
IntGELU    | n:  6
.calib done
Test: [  0/391]	Time  3.813 ( 3.813)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.92
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=19, intgelu_exp_n=7
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=19, intgelu_exp_n=7, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  19
IntGELU    | n:  7
IntSoftmax | n:  19
IntGELU    | n:  7
IntSoftmax | n:  19
IntGELU    | n:  7
IntSoftmax | n:  19
IntGELU    | n:  7
IntSoftmax | n:  19
IntGELU    | n:  7
IntSoftmax | n:  19
IntGELU    | n:  7
IntSoftmax | n:  19
IntGELU    | n:  7
IntSoftmax | n:  19
IntGELU    | n:  7
IntSoftmax | n:  19
IntGELU    | n:  7
IntSoftmax | n:  19
IntGELU    | n:  7
IntSoftmax | n:  19
IntGELU    | n:  7
IntSoftmax | n:  19
IntGELU    | n:  7
.calib done
Test: [  0/391]	Time  3.781 ( 3.781)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.83
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=19, intgelu_exp_n=8
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=19, intgelu_exp_n=8, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  19
IntGELU    | n:  8
IntSoftmax | n:  19
IntGELU    | n:  8
IntSoftmax | n:  19
IntGELU    | n:  8
IntSoftmax | n:  19
IntGELU    | n:  8
IntSoftmax | n:  19
IntGELU    | n:  8
IntSoftmax | n:  19
IntGELU    | n:  8
IntSoftmax | n:  19
IntGELU    | n:  8
IntSoftmax | n:  19
IntGELU    | n:  8
IntSoftmax | n:  19
IntGELU    | n:  8
IntSoftmax | n:  19
IntGELU    | n:  8
IntSoftmax | n:  19
IntGELU    | n:  8
IntSoftmax | n:  19
IntGELU    | n:  8
.calib done
Test: [  0/391]	Time  4.030 ( 4.030)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.10
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=19, intgelu_exp_n=9
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=19, intgelu_exp_n=9, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  19
IntGELU    | n:  9
IntSoftmax | n:  19
IntGELU    | n:  9
IntSoftmax | n:  19
IntGELU    | n:  9
IntSoftmax | n:  19
IntGELU    | n:  9
IntSoftmax | n:  19
IntGELU    | n:  9
IntSoftmax | n:  19
IntGELU    | n:  9
IntSoftmax | n:  19
IntGELU    | n:  9
IntSoftmax | n:  19
IntGELU    | n:  9
IntSoftmax | n:  19
IntGELU    | n:  9
IntSoftmax | n:  19
IntGELU    | n:  9
IntSoftmax | n:  19
IntGELU    | n:  9
IntSoftmax | n:  19
IntGELU    | n:  9
.calib done
Test: [  0/391]	Time  3.812 ( 3.812)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.91
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=19, intgelu_exp_n=10
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=19, intgelu_exp_n=10, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  19
IntGELU    | n:  10
IntSoftmax | n:  19
IntGELU    | n:  10
IntSoftmax | n:  19
IntGELU    | n:  10
IntSoftmax | n:  19
IntGELU    | n:  10
IntSoftmax | n:  19
IntGELU    | n:  10
IntSoftmax | n:  19
IntGELU    | n:  10
IntSoftmax | n:  19
IntGELU    | n:  10
IntSoftmax | n:  19
IntGELU    | n:  10
IntSoftmax | n:  19
IntGELU    | n:  10
IntSoftmax | n:  19
IntGELU    | n:  10
IntSoftmax | n:  19
IntGELU    | n:  10
IntSoftmax | n:  19
IntGELU    | n:  10
.calib done
Test: [  0/391]	Time  3.863 ( 3.863)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.91
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=19, intgelu_exp_n=11
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=19, intgelu_exp_n=11, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  19
IntGELU    | n:  11
IntSoftmax | n:  19
IntGELU    | n:  11
IntSoftmax | n:  19
IntGELU    | n:  11
IntSoftmax | n:  19
IntGELU    | n:  11
IntSoftmax | n:  19
IntGELU    | n:  11
IntSoftmax | n:  19
IntGELU    | n:  11
IntSoftmax | n:  19
IntGELU    | n:  11
IntSoftmax | n:  19
IntGELU    | n:  11
IntSoftmax | n:  19
IntGELU    | n:  11
IntSoftmax | n:  19
IntGELU    | n:  11
IntSoftmax | n:  19
IntGELU    | n:  11
IntSoftmax | n:  19
IntGELU    | n:  11
.calib done
Test: [  0/391]	Time  3.971 ( 3.971)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.04
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=19, intgelu_exp_n=12
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=19, intgelu_exp_n=12, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  19
IntGELU    | n:  12
IntSoftmax | n:  19
IntGELU    | n:  12
IntSoftmax | n:  19
IntGELU    | n:  12
IntSoftmax | n:  19
IntGELU    | n:  12
IntSoftmax | n:  19
IntGELU    | n:  12
IntSoftmax | n:  19
IntGELU    | n:  12
IntSoftmax | n:  19
IntGELU    | n:  12
IntSoftmax | n:  19
IntGELU    | n:  12
IntSoftmax | n:  19
IntGELU    | n:  12
IntSoftmax | n:  19
IntGELU    | n:  12
IntSoftmax | n:  19
IntGELU    | n:  12
IntSoftmax | n:  19
IntGELU    | n:  12
.calib done
Test: [  0/391]	Time  3.995 ( 3.995)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.01
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=19, intgelu_exp_n=13
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=19, intgelu_exp_n=13, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  19
IntGELU    | n:  13
IntSoftmax | n:  19
IntGELU    | n:  13
IntSoftmax | n:  19
IntGELU    | n:  13
IntSoftmax | n:  19
IntGELU    | n:  13
IntSoftmax | n:  19
IntGELU    | n:  13
IntSoftmax | n:  19
IntGELU    | n:  13
IntSoftmax | n:  19
IntGELU    | n:  13
IntSoftmax | n:  19
IntGELU    | n:  13
IntSoftmax | n:  19
IntGELU    | n:  13
IntSoftmax | n:  19
IntGELU    | n:  13
IntSoftmax | n:  19
IntGELU    | n:  13
IntSoftmax | n:  19
IntGELU    | n:  13
.calib done
Test: [  0/391]	Time  3.874 ( 3.874)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.95
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=19, intgelu_exp_n=14
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=19, intgelu_exp_n=14, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  19
IntGELU    | n:  14
IntSoftmax | n:  19
IntGELU    | n:  14
IntSoftmax | n:  19
IntGELU    | n:  14
IntSoftmax | n:  19
IntGELU    | n:  14
IntSoftmax | n:  19
IntGELU    | n:  14
IntSoftmax | n:  19
IntGELU    | n:  14
IntSoftmax | n:  19
IntGELU    | n:  14
IntSoftmax | n:  19
IntGELU    | n:  14
IntSoftmax | n:  19
IntGELU    | n:  14
IntSoftmax | n:  19
IntGELU    | n:  14
IntSoftmax | n:  19
IntGELU    | n:  14
IntSoftmax | n:  19
IntGELU    | n:  14
.calib done
Test: [  0/391]	Time  3.707 ( 3.707)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.71
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=19, intgelu_exp_n=15
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=19, intgelu_exp_n=15, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  19
IntGELU    | n:  15
IntSoftmax | n:  19
IntGELU    | n:  15
IntSoftmax | n:  19
IntGELU    | n:  15
IntSoftmax | n:  19
IntGELU    | n:  15
IntSoftmax | n:  19
IntGELU    | n:  15
IntSoftmax | n:  19
IntGELU    | n:  15
IntSoftmax | n:  19
IntGELU    | n:  15
IntSoftmax | n:  19
IntGELU    | n:  15
IntSoftmax | n:  19
IntGELU    | n:  15
IntSoftmax | n:  19
IntGELU    | n:  15
IntSoftmax | n:  19
IntGELU    | n:  15
IntSoftmax | n:  19
IntGELU    | n:  15
.calib done
Test: [  0/391]	Time  3.997 ( 3.997)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.02
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=19, intgelu_exp_n=16
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=19, intgelu_exp_n=16, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  19
IntGELU    | n:  16
IntSoftmax | n:  19
IntGELU    | n:  16
IntSoftmax | n:  19
IntGELU    | n:  16
IntSoftmax | n:  19
IntGELU    | n:  16
IntSoftmax | n:  19
IntGELU    | n:  16
IntSoftmax | n:  19
IntGELU    | n:  16
IntSoftmax | n:  19
IntGELU    | n:  16
IntSoftmax | n:  19
IntGELU    | n:  16
IntSoftmax | n:  19
IntGELU    | n:  16
IntSoftmax | n:  19
IntGELU    | n:  16
IntSoftmax | n:  19
IntGELU    | n:  16
IntSoftmax | n:  19
IntGELU    | n:  16
.calib done
Test: [  0/391]	Time  3.746 ( 3.746)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.76
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=19, intgelu_exp_n=17
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=19, intgelu_exp_n=17, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  19
IntGELU    | n:  17
IntSoftmax | n:  19
IntGELU    | n:  17
IntSoftmax | n:  19
IntGELU    | n:  17
IntSoftmax | n:  19
IntGELU    | n:  17
IntSoftmax | n:  19
IntGELU    | n:  17
IntSoftmax | n:  19
IntGELU    | n:  17
IntSoftmax | n:  19
IntGELU    | n:  17
IntSoftmax | n:  19
IntGELU    | n:  17
IntSoftmax | n:  19
IntGELU    | n:  17
IntSoftmax | n:  19
IntGELU    | n:  17
IntSoftmax | n:  19
IntGELU    | n:  17
IntSoftmax | n:  19
IntGELU    | n:  17
.calib done
Test: [  0/391]	Time  4.020 ( 4.020)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.03
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=19, intgelu_exp_n=18
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=19, intgelu_exp_n=18, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  19
IntGELU    | n:  18
IntSoftmax | n:  19
IntGELU    | n:  18
IntSoftmax | n:  19
IntGELU    | n:  18
IntSoftmax | n:  19
IntGELU    | n:  18
IntSoftmax | n:  19
IntGELU    | n:  18
IntSoftmax | n:  19
IntGELU    | n:  18
IntSoftmax | n:  19
IntGELU    | n:  18
IntSoftmax | n:  19
IntGELU    | n:  18
IntSoftmax | n:  19
IntGELU    | n:  18
IntSoftmax | n:  19
IntGELU    | n:  18
IntSoftmax | n:  19
IntGELU    | n:  18
IntSoftmax | n:  19
IntGELU    | n:  18
.calib done
Test: [  0/391]	Time  3.911 ( 3.911)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.95
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=19, intgelu_exp_n=19
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=19, intgelu_exp_n=19, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  19
IntGELU    | n:  19
IntSoftmax | n:  19
IntGELU    | n:  19
IntSoftmax | n:  19
IntGELU    | n:  19
IntSoftmax | n:  19
IntGELU    | n:  19
IntSoftmax | n:  19
IntGELU    | n:  19
IntSoftmax | n:  19
IntGELU    | n:  19
IntSoftmax | n:  19
IntGELU    | n:  19
IntSoftmax | n:  19
IntGELU    | n:  19
IntSoftmax | n:  19
IntGELU    | n:  19
IntSoftmax | n:  19
IntGELU    | n:  19
IntSoftmax | n:  19
IntGELU    | n:  19
IntSoftmax | n:  19
IntGELU    | n:  19
.calib done
Test: [  0/391]	Time  3.694 ( 3.694)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.77
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=19, intgelu_exp_n=20
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=19, intgelu_exp_n=20, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  19
IntGELU    | n:  20
IntSoftmax | n:  19
IntGELU    | n:  20
IntSoftmax | n:  19
IntGELU    | n:  20
IntSoftmax | n:  19
IntGELU    | n:  20
IntSoftmax | n:  19
IntGELU    | n:  20
IntSoftmax | n:  19
IntGELU    | n:  20
IntSoftmax | n:  19
IntGELU    | n:  20
IntSoftmax | n:  19
IntGELU    | n:  20
IntSoftmax | n:  19
IntGELU    | n:  20
IntSoftmax | n:  19
IntGELU    | n:  20
IntSoftmax | n:  19
IntGELU    | n:  20
IntSoftmax | n:  19
IntGELU    | n:  20
.calib done
Test: [  0/391]	Time  3.711 ( 3.711)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.74
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=19, intgelu_exp_n=21
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=19, intgelu_exp_n=21, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  19
IntGELU    | n:  21
IntSoftmax | n:  19
IntGELU    | n:  21
IntSoftmax | n:  19
IntGELU    | n:  21
IntSoftmax | n:  19
IntGELU    | n:  21
IntSoftmax | n:  19
IntGELU    | n:  21
IntSoftmax | n:  19
IntGELU    | n:  21
IntSoftmax | n:  19
IntGELU    | n:  21
IntSoftmax | n:  19
IntGELU    | n:  21
IntSoftmax | n:  19
IntGELU    | n:  21
IntSoftmax | n:  19
IntGELU    | n:  21
IntSoftmax | n:  19
IntGELU    | n:  21
IntSoftmax | n:  19
IntGELU    | n:  21
.calib done
Test: [  0/391]	Time  3.696 ( 3.696)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.74
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=19, intgelu_exp_n=22
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=19, intgelu_exp_n=22, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  19
IntGELU    | n:  22
IntSoftmax | n:  19
IntGELU    | n:  22
IntSoftmax | n:  19
IntGELU    | n:  22
IntSoftmax | n:  19
IntGELU    | n:  22
IntSoftmax | n:  19
IntGELU    | n:  22
IntSoftmax | n:  19
IntGELU    | n:  22
IntSoftmax | n:  19
IntGELU    | n:  22
IntSoftmax | n:  19
IntGELU    | n:  22
IntSoftmax | n:  19
IntGELU    | n:  22
IntSoftmax | n:  19
IntGELU    | n:  22
IntSoftmax | n:  19
IntGELU    | n:  22
IntSoftmax | n:  19
IntGELU    | n:  22
.calib done
Test: [  0/391]	Time  3.905 ( 3.905)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=19, intgelu_exp_n=23
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=19, intgelu_exp_n=23, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  19
IntGELU    | n:  23
IntSoftmax | n:  19
IntGELU    | n:  23
IntSoftmax | n:  19
IntGELU    | n:  23
IntSoftmax | n:  19
IntGELU    | n:  23
IntSoftmax | n:  19
IntGELU    | n:  23
IntSoftmax | n:  19
IntGELU    | n:  23
IntSoftmax | n:  19
IntGELU    | n:  23
IntSoftmax | n:  19
IntGELU    | n:  23
IntSoftmax | n:  19
IntGELU    | n:  23
IntSoftmax | n:  19
IntGELU    | n:  23
IntSoftmax | n:  19
IntGELU    | n:  23
IntSoftmax | n:  19
IntGELU    | n:  23
.calib done
Test: [  0/391]	Time  3.843 ( 3.843)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.88
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=19, intgelu_exp_n=24
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=19, intgelu_exp_n=24, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  19
IntGELU    | n:  24
IntSoftmax | n:  19
IntGELU    | n:  24
IntSoftmax | n:  19
IntGELU    | n:  24
IntSoftmax | n:  19
IntGELU    | n:  24
IntSoftmax | n:  19
IntGELU    | n:  24
IntSoftmax | n:  19
IntGELU    | n:  24
IntSoftmax | n:  19
IntGELU    | n:  24
IntSoftmax | n:  19
IntGELU    | n:  24
IntSoftmax | n:  19
IntGELU    | n:  24
IntSoftmax | n:  19
IntGELU    | n:  24
IntSoftmax | n:  19
IntGELU    | n:  24
IntSoftmax | n:  19
IntGELU    | n:  24
.calib done
Test: [  0/391]	Time  3.865 ( 3.865)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=19, intgelu_exp_n=25
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=19, intgelu_exp_n=25, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  19
IntGELU    | n:  25
IntSoftmax | n:  19
IntGELU    | n:  25
IntSoftmax | n:  19
IntGELU    | n:  25
IntSoftmax | n:  19
IntGELU    | n:  25
IntSoftmax | n:  19
IntGELU    | n:  25
IntSoftmax | n:  19
IntGELU    | n:  25
IntSoftmax | n:  19
IntGELU    | n:  25
IntSoftmax | n:  19
IntGELU    | n:  25
IntSoftmax | n:  19
IntGELU    | n:  25
IntSoftmax | n:  19
IntGELU    | n:  25
IntSoftmax | n:  19
IntGELU    | n:  25
IntSoftmax | n:  19
IntGELU    | n:  25
.calib done
Test: [  0/391]	Time  3.805 ( 3.805)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.86
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=19, intgelu_exp_n=26
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=19, intgelu_exp_n=26, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  19
IntGELU    | n:  26
IntSoftmax | n:  19
IntGELU    | n:  26
IntSoftmax | n:  19
IntGELU    | n:  26
IntSoftmax | n:  19
IntGELU    | n:  26
IntSoftmax | n:  19
IntGELU    | n:  26
IntSoftmax | n:  19
IntGELU    | n:  26
IntSoftmax | n:  19
IntGELU    | n:  26
IntSoftmax | n:  19
IntGELU    | n:  26
IntSoftmax | n:  19
IntGELU    | n:  26
IntSoftmax | n:  19
IntGELU    | n:  26
IntSoftmax | n:  19
IntGELU    | n:  26
IntSoftmax | n:  19
IntGELU    | n:  26
.calib done
Test: [  0/391]	Time  3.931 ( 3.931)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=19, intgelu_exp_n=27
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=19, intgelu_exp_n=27, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  19
IntGELU    | n:  27
IntSoftmax | n:  19
IntGELU    | n:  27
IntSoftmax | n:  19
IntGELU    | n:  27
IntSoftmax | n:  19
IntGELU    | n:  27
IntSoftmax | n:  19
IntGELU    | n:  27
IntSoftmax | n:  19
IntGELU    | n:  27
IntSoftmax | n:  19
IntGELU    | n:  27
IntSoftmax | n:  19
IntGELU    | n:  27
IntSoftmax | n:  19
IntGELU    | n:  27
IntSoftmax | n:  19
IntGELU    | n:  27
IntSoftmax | n:  19
IntGELU    | n:  27
IntSoftmax | n:  19
IntGELU    | n:  27
.calib done
Test: [  0/391]	Time  3.808 ( 3.808)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.86
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=19, intgelu_exp_n=28
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=19, intgelu_exp_n=28, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  19
IntGELU    | n:  28
IntSoftmax | n:  19
IntGELU    | n:  28
IntSoftmax | n:  19
IntGELU    | n:  28
IntSoftmax | n:  19
IntGELU    | n:  28
IntSoftmax | n:  19
IntGELU    | n:  28
IntSoftmax | n:  19
IntGELU    | n:  28
IntSoftmax | n:  19
IntGELU    | n:  28
IntSoftmax | n:  19
IntGELU    | n:  28
IntSoftmax | n:  19
IntGELU    | n:  28
IntSoftmax | n:  19
IntGELU    | n:  28
IntSoftmax | n:  19
IntGELU    | n:  28
IntSoftmax | n:  19
IntGELU    | n:  28
.calib done
Test: [  0/391]	Time  3.915 ( 3.915)	Acc@1   0.00 (  0.00)	Acc@5   0.78 (  0.78)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.781
Time: 8.98
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=19, intgelu_exp_n=29
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=19, intgelu_exp_n=29, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  19
IntGELU    | n:  29
IntSoftmax | n:  19
IntGELU    | n:  29
IntSoftmax | n:  19
IntGELU    | n:  29
IntSoftmax | n:  19
IntGELU    | n:  29
IntSoftmax | n:  19
IntGELU    | n:  29
IntSoftmax | n:  19
IntGELU    | n:  29
IntSoftmax | n:  19
IntGELU    | n:  29
IntSoftmax | n:  19
IntGELU    | n:  29
IntSoftmax | n:  19
IntGELU    | n:  29
IntSoftmax | n:  19
IntGELU    | n:  29
IntSoftmax | n:  19
IntGELU    | n:  29
IntSoftmax | n:  19
IntGELU    | n:  29
.calib done
Test: [  0/391]	Time  3.889 ( 3.889)	Acc@1   5.47 (  5.47)	Acc@5   8.59 (  8.59)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 5.469 Prec@5 8.594
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=19, intgelu_exp_n=30
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=19, intgelu_exp_n=30, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  19
IntGELU    | n:  30
IntSoftmax | n:  19
IntGELU    | n:  30
IntSoftmax | n:  19
IntGELU    | n:  30
IntSoftmax | n:  19
IntGELU    | n:  30
IntSoftmax | n:  19
IntGELU    | n:  30
IntSoftmax | n:  19
IntGELU    | n:  30
IntSoftmax | n:  19
IntGELU    | n:  30
IntSoftmax | n:  19
IntGELU    | n:  30
IntSoftmax | n:  19
IntGELU    | n:  30
IntSoftmax | n:  19
IntGELU    | n:  30
IntSoftmax | n:  19
IntGELU    | n:  30
IntSoftmax | n:  19
IntGELU    | n:  30
.calib done
Test: [  0/391]	Time  3.919 ( 3.919)	Acc@1   3.12 (  3.12)	Acc@5   8.59 (  8.59)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 3.125 Prec@5 8.594
Time: 8.91
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=19, intgelu_exp_n=31
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=19, intgelu_exp_n=31, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  19
IntGELU    | n:  31
IntSoftmax | n:  19
IntGELU    | n:  31
IntSoftmax | n:  19
IntGELU    | n:  31
IntSoftmax | n:  19
IntGELU    | n:  31
IntSoftmax | n:  19
IntGELU    | n:  31
IntSoftmax | n:  19
IntGELU    | n:  31
IntSoftmax | n:  19
IntGELU    | n:  31
IntSoftmax | n:  19
IntGELU    | n:  31
IntSoftmax | n:  19
IntGELU    | n:  31
IntSoftmax | n:  19
IntGELU    | n:  31
IntSoftmax | n:  19
IntGELU    | n:  31
IntSoftmax | n:  19
IntGELU    | n:  31
.calib done
Test: [  0/391]	Time  3.988 ( 3.988)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.02
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=20, intgelu_exp_n=0
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=20, intgelu_exp_n=0, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  20
IntGELU    | n:  0
IntSoftmax | n:  20
IntGELU    | n:  0
IntSoftmax | n:  20
IntGELU    | n:  0
IntSoftmax | n:  20
IntGELU    | n:  0
IntSoftmax | n:  20
IntGELU    | n:  0
IntSoftmax | n:  20
IntGELU    | n:  0
IntSoftmax | n:  20
IntGELU    | n:  0
IntSoftmax | n:  20
IntGELU    | n:  0
IntSoftmax | n:  20
IntGELU    | n:  0
IntSoftmax | n:  20
IntGELU    | n:  0
IntSoftmax | n:  20
IntGELU    | n:  0
IntSoftmax | n:  20
IntGELU    | n:  0
.calib done
Test: [  0/391]	Time  3.901 ( 3.901)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.99
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=20, intgelu_exp_n=1
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=20, intgelu_exp_n=1, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  20
IntGELU    | n:  1
IntSoftmax | n:  20
IntGELU    | n:  1
IntSoftmax | n:  20
IntGELU    | n:  1
IntSoftmax | n:  20
IntGELU    | n:  1
IntSoftmax | n:  20
IntGELU    | n:  1
IntSoftmax | n:  20
IntGELU    | n:  1
IntSoftmax | n:  20
IntGELU    | n:  1
IntSoftmax | n:  20
IntGELU    | n:  1
IntSoftmax | n:  20
IntGELU    | n:  1
IntSoftmax | n:  20
IntGELU    | n:  1
IntSoftmax | n:  20
IntGELU    | n:  1
IntSoftmax | n:  20
IntGELU    | n:  1
.calib done
Test: [  0/391]	Time  3.857 ( 3.857)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.89
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=20, intgelu_exp_n=2
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=20, intgelu_exp_n=2, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  20
IntGELU    | n:  2
IntSoftmax | n:  20
IntGELU    | n:  2
IntSoftmax | n:  20
IntGELU    | n:  2
IntSoftmax | n:  20
IntGELU    | n:  2
IntSoftmax | n:  20
IntGELU    | n:  2
IntSoftmax | n:  20
IntGELU    | n:  2
IntSoftmax | n:  20
IntGELU    | n:  2
IntSoftmax | n:  20
IntGELU    | n:  2
IntSoftmax | n:  20
IntGELU    | n:  2
IntSoftmax | n:  20
IntGELU    | n:  2
IntSoftmax | n:  20
IntGELU    | n:  2
IntSoftmax | n:  20
IntGELU    | n:  2
.calib done
Test: [  0/391]	Time  4.006 ( 4.006)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.07
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=20, intgelu_exp_n=3
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=20, intgelu_exp_n=3, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  20
IntGELU    | n:  3
IntSoftmax | n:  20
IntGELU    | n:  3
IntSoftmax | n:  20
IntGELU    | n:  3
IntSoftmax | n:  20
IntGELU    | n:  3
IntSoftmax | n:  20
IntGELU    | n:  3
IntSoftmax | n:  20
IntGELU    | n:  3
IntSoftmax | n:  20
IntGELU    | n:  3
IntSoftmax | n:  20
IntGELU    | n:  3
IntSoftmax | n:  20
IntGELU    | n:  3
IntSoftmax | n:  20
IntGELU    | n:  3
IntSoftmax | n:  20
IntGELU    | n:  3
IntSoftmax | n:  20
IntGELU    | n:  3
.calib done
Test: [  0/391]	Time  3.872 ( 3.872)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.93
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=20, intgelu_exp_n=4
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=20, intgelu_exp_n=4, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  20
IntGELU    | n:  4
IntSoftmax | n:  20
IntGELU    | n:  4
IntSoftmax | n:  20
IntGELU    | n:  4
IntSoftmax | n:  20
IntGELU    | n:  4
IntSoftmax | n:  20
IntGELU    | n:  4
IntSoftmax | n:  20
IntGELU    | n:  4
IntSoftmax | n:  20
IntGELU    | n:  4
IntSoftmax | n:  20
IntGELU    | n:  4
IntSoftmax | n:  20
IntGELU    | n:  4
IntSoftmax | n:  20
IntGELU    | n:  4
IntSoftmax | n:  20
IntGELU    | n:  4
IntSoftmax | n:  20
IntGELU    | n:  4
.calib done
Test: [  0/391]	Time  3.644 ( 3.644)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.71
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=20, intgelu_exp_n=5
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=20, intgelu_exp_n=5, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  20
IntGELU    | n:  5
IntSoftmax | n:  20
IntGELU    | n:  5
IntSoftmax | n:  20
IntGELU    | n:  5
IntSoftmax | n:  20
IntGELU    | n:  5
IntSoftmax | n:  20
IntGELU    | n:  5
IntSoftmax | n:  20
IntGELU    | n:  5
IntSoftmax | n:  20
IntGELU    | n:  5
IntSoftmax | n:  20
IntGELU    | n:  5
IntSoftmax | n:  20
IntGELU    | n:  5
IntSoftmax | n:  20
IntGELU    | n:  5
IntSoftmax | n:  20
IntGELU    | n:  5
IntSoftmax | n:  20
IntGELU    | n:  5
.calib done
Test: [  0/391]	Time  3.969 ( 3.969)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.01
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=20, intgelu_exp_n=6
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=20, intgelu_exp_n=6, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  20
IntGELU    | n:  6
IntSoftmax | n:  20
IntGELU    | n:  6
IntSoftmax | n:  20
IntGELU    | n:  6
IntSoftmax | n:  20
IntGELU    | n:  6
IntSoftmax | n:  20
IntGELU    | n:  6
IntSoftmax | n:  20
IntGELU    | n:  6
IntSoftmax | n:  20
IntGELU    | n:  6
IntSoftmax | n:  20
IntGELU    | n:  6
IntSoftmax | n:  20
IntGELU    | n:  6
IntSoftmax | n:  20
IntGELU    | n:  6
IntSoftmax | n:  20
IntGELU    | n:  6
IntSoftmax | n:  20
IntGELU    | n:  6
.calib done
Test: [  0/391]	Time  3.656 ( 3.656)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.73
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=20, intgelu_exp_n=7
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=20, intgelu_exp_n=7, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  20
IntGELU    | n:  7
IntSoftmax | n:  20
IntGELU    | n:  7
IntSoftmax | n:  20
IntGELU    | n:  7
IntSoftmax | n:  20
IntGELU    | n:  7
IntSoftmax | n:  20
IntGELU    | n:  7
IntSoftmax | n:  20
IntGELU    | n:  7
IntSoftmax | n:  20
IntGELU    | n:  7
IntSoftmax | n:  20
IntGELU    | n:  7
IntSoftmax | n:  20
IntGELU    | n:  7
IntSoftmax | n:  20
IntGELU    | n:  7
IntSoftmax | n:  20
IntGELU    | n:  7
IntSoftmax | n:  20
IntGELU    | n:  7
.calib done
Test: [  0/391]	Time  3.728 ( 3.728)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.82
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=20, intgelu_exp_n=8
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=20, intgelu_exp_n=8, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  20
IntGELU    | n:  8
IntSoftmax | n:  20
IntGELU    | n:  8
IntSoftmax | n:  20
IntGELU    | n:  8
IntSoftmax | n:  20
IntGELU    | n:  8
IntSoftmax | n:  20
IntGELU    | n:  8
IntSoftmax | n:  20
IntGELU    | n:  8
IntSoftmax | n:  20
IntGELU    | n:  8
IntSoftmax | n:  20
IntGELU    | n:  8
IntSoftmax | n:  20
IntGELU    | n:  8
IntSoftmax | n:  20
IntGELU    | n:  8
IntSoftmax | n:  20
IntGELU    | n:  8
IntSoftmax | n:  20
IntGELU    | n:  8
.calib done
Test: [  0/391]	Time  3.817 ( 3.817)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.85
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=20, intgelu_exp_n=9
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=20, intgelu_exp_n=9, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  20
IntGELU    | n:  9
IntSoftmax | n:  20
IntGELU    | n:  9
IntSoftmax | n:  20
IntGELU    | n:  9
IntSoftmax | n:  20
IntGELU    | n:  9
IntSoftmax | n:  20
IntGELU    | n:  9
IntSoftmax | n:  20
IntGELU    | n:  9
IntSoftmax | n:  20
IntGELU    | n:  9
IntSoftmax | n:  20
IntGELU    | n:  9
IntSoftmax | n:  20
IntGELU    | n:  9
IntSoftmax | n:  20
IntGELU    | n:  9
IntSoftmax | n:  20
IntGELU    | n:  9
IntSoftmax | n:  20
IntGELU    | n:  9
.calib done
Test: [  0/391]	Time  3.743 ( 3.743)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.73
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=20, intgelu_exp_n=10
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=20, intgelu_exp_n=10, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  20
IntGELU    | n:  10
IntSoftmax | n:  20
IntGELU    | n:  10
IntSoftmax | n:  20
IntGELU    | n:  10
IntSoftmax | n:  20
IntGELU    | n:  10
IntSoftmax | n:  20
IntGELU    | n:  10
IntSoftmax | n:  20
IntGELU    | n:  10
IntSoftmax | n:  20
IntGELU    | n:  10
IntSoftmax | n:  20
IntGELU    | n:  10
IntSoftmax | n:  20
IntGELU    | n:  10
IntSoftmax | n:  20
IntGELU    | n:  10
IntSoftmax | n:  20
IntGELU    | n:  10
IntSoftmax | n:  20
IntGELU    | n:  10
.calib done
Test: [  0/391]	Time  3.892 ( 3.892)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.95
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=20, intgelu_exp_n=11
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=20, intgelu_exp_n=11, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  20
IntGELU    | n:  11
IntSoftmax | n:  20
IntGELU    | n:  11
IntSoftmax | n:  20
IntGELU    | n:  11
IntSoftmax | n:  20
IntGELU    | n:  11
IntSoftmax | n:  20
IntGELU    | n:  11
IntSoftmax | n:  20
IntGELU    | n:  11
IntSoftmax | n:  20
IntGELU    | n:  11
IntSoftmax | n:  20
IntGELU    | n:  11
IntSoftmax | n:  20
IntGELU    | n:  11
IntSoftmax | n:  20
IntGELU    | n:  11
IntSoftmax | n:  20
IntGELU    | n:  11
IntSoftmax | n:  20
IntGELU    | n:  11
.calib done
Test: [  0/391]	Time  3.892 ( 3.892)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.92
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=20, intgelu_exp_n=12
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=20, intgelu_exp_n=12, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  20
IntGELU    | n:  12
IntSoftmax | n:  20
IntGELU    | n:  12
IntSoftmax | n:  20
IntGELU    | n:  12
IntSoftmax | n:  20
IntGELU    | n:  12
IntSoftmax | n:  20
IntGELU    | n:  12
IntSoftmax | n:  20
IntGELU    | n:  12
IntSoftmax | n:  20
IntGELU    | n:  12
IntSoftmax | n:  20
IntGELU    | n:  12
IntSoftmax | n:  20
IntGELU    | n:  12
IntSoftmax | n:  20
IntGELU    | n:  12
IntSoftmax | n:  20
IntGELU    | n:  12
IntSoftmax | n:  20
IntGELU    | n:  12
.calib done
Test: [  0/391]	Time  3.871 ( 3.871)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.89
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=20, intgelu_exp_n=13
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=20, intgelu_exp_n=13, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  20
IntGELU    | n:  13
IntSoftmax | n:  20
IntGELU    | n:  13
IntSoftmax | n:  20
IntGELU    | n:  13
IntSoftmax | n:  20
IntGELU    | n:  13
IntSoftmax | n:  20
IntGELU    | n:  13
IntSoftmax | n:  20
IntGELU    | n:  13
IntSoftmax | n:  20
IntGELU    | n:  13
IntSoftmax | n:  20
IntGELU    | n:  13
IntSoftmax | n:  20
IntGELU    | n:  13
IntSoftmax | n:  20
IntGELU    | n:  13
IntSoftmax | n:  20
IntGELU    | n:  13
IntSoftmax | n:  20
IntGELU    | n:  13
.calib done
Test: [  0/391]	Time  3.963 ( 3.963)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.04
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=20, intgelu_exp_n=14
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=20, intgelu_exp_n=14, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  20
IntGELU    | n:  14
IntSoftmax | n:  20
IntGELU    | n:  14
IntSoftmax | n:  20
IntGELU    | n:  14
IntSoftmax | n:  20
IntGELU    | n:  14
IntSoftmax | n:  20
IntGELU    | n:  14
IntSoftmax | n:  20
IntGELU    | n:  14
IntSoftmax | n:  20
IntGELU    | n:  14
IntSoftmax | n:  20
IntGELU    | n:  14
IntSoftmax | n:  20
IntGELU    | n:  14
IntSoftmax | n:  20
IntGELU    | n:  14
IntSoftmax | n:  20
IntGELU    | n:  14
IntSoftmax | n:  20
IntGELU    | n:  14
.calib done
Test: [  0/391]	Time  3.734 ( 3.734)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.84
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=20, intgelu_exp_n=15
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=20, intgelu_exp_n=15, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  20
IntGELU    | n:  15
IntSoftmax | n:  20
IntGELU    | n:  15
IntSoftmax | n:  20
IntGELU    | n:  15
IntSoftmax | n:  20
IntGELU    | n:  15
IntSoftmax | n:  20
IntGELU    | n:  15
IntSoftmax | n:  20
IntGELU    | n:  15
IntSoftmax | n:  20
IntGELU    | n:  15
IntSoftmax | n:  20
IntGELU    | n:  15
IntSoftmax | n:  20
IntGELU    | n:  15
IntSoftmax | n:  20
IntGELU    | n:  15
IntSoftmax | n:  20
IntGELU    | n:  15
IntSoftmax | n:  20
IntGELU    | n:  15
.calib done
Test: [  0/391]	Time  3.804 ( 3.804)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.87
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=20, intgelu_exp_n=16
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=20, intgelu_exp_n=16, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  20
IntGELU    | n:  16
IntSoftmax | n:  20
IntGELU    | n:  16
IntSoftmax | n:  20
IntGELU    | n:  16
IntSoftmax | n:  20
IntGELU    | n:  16
IntSoftmax | n:  20
IntGELU    | n:  16
IntSoftmax | n:  20
IntGELU    | n:  16
IntSoftmax | n:  20
IntGELU    | n:  16
IntSoftmax | n:  20
IntGELU    | n:  16
IntSoftmax | n:  20
IntGELU    | n:  16
IntSoftmax | n:  20
IntGELU    | n:  16
IntSoftmax | n:  20
IntGELU    | n:  16
IntSoftmax | n:  20
IntGELU    | n:  16
.calib done
Test: [  0/391]	Time  3.923 ( 3.923)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.00
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=20, intgelu_exp_n=17
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=20, intgelu_exp_n=17, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  20
IntGELU    | n:  17
IntSoftmax | n:  20
IntGELU    | n:  17
IntSoftmax | n:  20
IntGELU    | n:  17
IntSoftmax | n:  20
IntGELU    | n:  17
IntSoftmax | n:  20
IntGELU    | n:  17
IntSoftmax | n:  20
IntGELU    | n:  17
IntSoftmax | n:  20
IntGELU    | n:  17
IntSoftmax | n:  20
IntGELU    | n:  17
IntSoftmax | n:  20
IntGELU    | n:  17
IntSoftmax | n:  20
IntGELU    | n:  17
IntSoftmax | n:  20
IntGELU    | n:  17
IntSoftmax | n:  20
IntGELU    | n:  17
.calib done
Test: [  0/391]	Time  3.846 ( 3.846)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.89
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=20, intgelu_exp_n=18
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=20, intgelu_exp_n=18, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  20
IntGELU    | n:  18
IntSoftmax | n:  20
IntGELU    | n:  18
IntSoftmax | n:  20
IntGELU    | n:  18
IntSoftmax | n:  20
IntGELU    | n:  18
IntSoftmax | n:  20
IntGELU    | n:  18
IntSoftmax | n:  20
IntGELU    | n:  18
IntSoftmax | n:  20
IntGELU    | n:  18
IntSoftmax | n:  20
IntGELU    | n:  18
IntSoftmax | n:  20
IntGELU    | n:  18
IntSoftmax | n:  20
IntGELU    | n:  18
IntSoftmax | n:  20
IntGELU    | n:  18
IntSoftmax | n:  20
IntGELU    | n:  18
.calib done
Test: [  0/391]	Time  3.999 ( 3.999)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.09
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=20, intgelu_exp_n=19
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=20, intgelu_exp_n=19, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  20
IntGELU    | n:  19
IntSoftmax | n:  20
IntGELU    | n:  19
IntSoftmax | n:  20
IntGELU    | n:  19
IntSoftmax | n:  20
IntGELU    | n:  19
IntSoftmax | n:  20
IntGELU    | n:  19
IntSoftmax | n:  20
IntGELU    | n:  19
IntSoftmax | n:  20
IntGELU    | n:  19
IntSoftmax | n:  20
IntGELU    | n:  19
IntSoftmax | n:  20
IntGELU    | n:  19
IntSoftmax | n:  20
IntGELU    | n:  19
IntSoftmax | n:  20
IntGELU    | n:  19
IntSoftmax | n:  20
IntGELU    | n:  19
.calib done
Test: [  0/391]	Time  3.833 ( 3.833)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=20, intgelu_exp_n=20
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=20, intgelu_exp_n=20, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  20
IntGELU    | n:  20
IntSoftmax | n:  20
IntGELU    | n:  20
IntSoftmax | n:  20
IntGELU    | n:  20
IntSoftmax | n:  20
IntGELU    | n:  20
IntSoftmax | n:  20
IntGELU    | n:  20
IntSoftmax | n:  20
IntGELU    | n:  20
IntSoftmax | n:  20
IntGELU    | n:  20
IntSoftmax | n:  20
IntGELU    | n:  20
IntSoftmax | n:  20
IntGELU    | n:  20
IntSoftmax | n:  20
IntGELU    | n:  20
IntSoftmax | n:  20
IntGELU    | n:  20
IntSoftmax | n:  20
IntGELU    | n:  20
.calib done
Test: [  0/391]	Time  3.666 ( 3.666)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.70
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=20, intgelu_exp_n=21
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=20, intgelu_exp_n=21, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  20
IntGELU    | n:  21
IntSoftmax | n:  20
IntGELU    | n:  21
IntSoftmax | n:  20
IntGELU    | n:  21
IntSoftmax | n:  20
IntGELU    | n:  21
IntSoftmax | n:  20
IntGELU    | n:  21
IntSoftmax | n:  20
IntGELU    | n:  21
IntSoftmax | n:  20
IntGELU    | n:  21
IntSoftmax | n:  20
IntGELU    | n:  21
IntSoftmax | n:  20
IntGELU    | n:  21
IntSoftmax | n:  20
IntGELU    | n:  21
IntSoftmax | n:  20
IntGELU    | n:  21
IntSoftmax | n:  20
IntGELU    | n:  21
.calib done
Test: [  0/391]	Time  4.009 ( 4.009)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.01
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=20, intgelu_exp_n=22
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=20, intgelu_exp_n=22, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  20
IntGELU    | n:  22
IntSoftmax | n:  20
IntGELU    | n:  22
IntSoftmax | n:  20
IntGELU    | n:  22
IntSoftmax | n:  20
IntGELU    | n:  22
IntSoftmax | n:  20
IntGELU    | n:  22
IntSoftmax | n:  20
IntGELU    | n:  22
IntSoftmax | n:  20
IntGELU    | n:  22
IntSoftmax | n:  20
IntGELU    | n:  22
IntSoftmax | n:  20
IntGELU    | n:  22
IntSoftmax | n:  20
IntGELU    | n:  22
IntSoftmax | n:  20
IntGELU    | n:  22
IntSoftmax | n:  20
IntGELU    | n:  22
.calib done
Test: [  0/391]	Time  3.771 ( 3.771)	Acc@1   0.78 (  0.78)	Acc@5   1.56 (  1.56)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.781 Prec@5 1.562
Time: 8.85
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=20, intgelu_exp_n=23
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=20, intgelu_exp_n=23, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  20
IntGELU    | n:  23
IntSoftmax | n:  20
IntGELU    | n:  23
IntSoftmax | n:  20
IntGELU    | n:  23
IntSoftmax | n:  20
IntGELU    | n:  23
IntSoftmax | n:  20
IntGELU    | n:  23
IntSoftmax | n:  20
IntGELU    | n:  23
IntSoftmax | n:  20
IntGELU    | n:  23
IntSoftmax | n:  20
IntGELU    | n:  23
IntSoftmax | n:  20
IntGELU    | n:  23
IntSoftmax | n:  20
IntGELU    | n:  23
IntSoftmax | n:  20
IntGELU    | n:  23
IntSoftmax | n:  20
IntGELU    | n:  23
.calib done
Test: [  0/391]	Time  3.628 ( 3.628)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.73
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=20, intgelu_exp_n=24
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=20, intgelu_exp_n=24, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  20
IntGELU    | n:  24
IntSoftmax | n:  20
IntGELU    | n:  24
IntSoftmax | n:  20
IntGELU    | n:  24
IntSoftmax | n:  20
IntGELU    | n:  24
IntSoftmax | n:  20
IntGELU    | n:  24
IntSoftmax | n:  20
IntGELU    | n:  24
IntSoftmax | n:  20
IntGELU    | n:  24
IntSoftmax | n:  20
IntGELU    | n:  24
IntSoftmax | n:  20
IntGELU    | n:  24
IntSoftmax | n:  20
IntGELU    | n:  24
IntSoftmax | n:  20
IntGELU    | n:  24
IntSoftmax | n:  20
IntGELU    | n:  24
.calib done
Test: [  0/391]	Time  3.880 ( 3.880)	Acc@1   0.00 (  0.00)	Acc@5   0.78 (  0.78)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.781
Time: 8.92
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=20, intgelu_exp_n=25
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=20, intgelu_exp_n=25, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  20
IntGELU    | n:  25
IntSoftmax | n:  20
IntGELU    | n:  25
IntSoftmax | n:  20
IntGELU    | n:  25
IntSoftmax | n:  20
IntGELU    | n:  25
IntSoftmax | n:  20
IntGELU    | n:  25
IntSoftmax | n:  20
IntGELU    | n:  25
IntSoftmax | n:  20
IntGELU    | n:  25
IntSoftmax | n:  20
IntGELU    | n:  25
IntSoftmax | n:  20
IntGELU    | n:  25
IntSoftmax | n:  20
IntGELU    | n:  25
IntSoftmax | n:  20
IntGELU    | n:  25
IntSoftmax | n:  20
IntGELU    | n:  25
.calib done
Test: [  0/391]	Time  3.866 ( 3.866)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.89
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=20, intgelu_exp_n=26
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=20, intgelu_exp_n=26, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  20
IntGELU    | n:  26
IntSoftmax | n:  20
IntGELU    | n:  26
IntSoftmax | n:  20
IntGELU    | n:  26
IntSoftmax | n:  20
IntGELU    | n:  26
IntSoftmax | n:  20
IntGELU    | n:  26
IntSoftmax | n:  20
IntGELU    | n:  26
IntSoftmax | n:  20
IntGELU    | n:  26
IntSoftmax | n:  20
IntGELU    | n:  26
IntSoftmax | n:  20
IntGELU    | n:  26
IntSoftmax | n:  20
IntGELU    | n:  26
IntSoftmax | n:  20
IntGELU    | n:  26
IntSoftmax | n:  20
IntGELU    | n:  26
.calib done
Test: [  0/391]	Time  3.813 ( 3.813)	Acc@1   0.00 (  0.00)	Acc@5   0.78 (  0.78)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.781
Time: 8.88
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=20, intgelu_exp_n=27
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=20, intgelu_exp_n=27, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  20
IntGELU    | n:  27
IntSoftmax | n:  20
IntGELU    | n:  27
IntSoftmax | n:  20
IntGELU    | n:  27
IntSoftmax | n:  20
IntGELU    | n:  27
IntSoftmax | n:  20
IntGELU    | n:  27
IntSoftmax | n:  20
IntGELU    | n:  27
IntSoftmax | n:  20
IntGELU    | n:  27
IntSoftmax | n:  20
IntGELU    | n:  27
IntSoftmax | n:  20
IntGELU    | n:  27
IntSoftmax | n:  20
IntGELU    | n:  27
IntSoftmax | n:  20
IntGELU    | n:  27
IntSoftmax | n:  20
IntGELU    | n:  27
.calib done
Test: [  0/391]	Time  3.977 ( 3.977)	Acc@1   0.00 (  0.00)	Acc@5   3.12 (  3.12)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 3.125
Time: 9.03
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=20, intgelu_exp_n=28
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=20, intgelu_exp_n=28, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  20
IntGELU    | n:  28
IntSoftmax | n:  20
IntGELU    | n:  28
IntSoftmax | n:  20
IntGELU    | n:  28
IntSoftmax | n:  20
IntGELU    | n:  28
IntSoftmax | n:  20
IntGELU    | n:  28
IntSoftmax | n:  20
IntGELU    | n:  28
IntSoftmax | n:  20
IntGELU    | n:  28
IntSoftmax | n:  20
IntGELU    | n:  28
IntSoftmax | n:  20
IntGELU    | n:  28
IntSoftmax | n:  20
IntGELU    | n:  28
IntSoftmax | n:  20
IntGELU    | n:  28
IntSoftmax | n:  20
IntGELU    | n:  28
.calib done
Test: [  0/391]	Time  3.973 ( 3.973)	Acc@1   0.00 (  0.00)	Acc@5   0.78 (  0.78)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.781
Time: 9.07
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=20, intgelu_exp_n=29
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=20, intgelu_exp_n=29, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  20
IntGELU    | n:  29
IntSoftmax | n:  20
IntGELU    | n:  29
IntSoftmax | n:  20
IntGELU    | n:  29
IntSoftmax | n:  20
IntGELU    | n:  29
IntSoftmax | n:  20
IntGELU    | n:  29
IntSoftmax | n:  20
IntGELU    | n:  29
IntSoftmax | n:  20
IntGELU    | n:  29
IntSoftmax | n:  20
IntGELU    | n:  29
IntSoftmax | n:  20
IntGELU    | n:  29
IntSoftmax | n:  20
IntGELU    | n:  29
IntSoftmax | n:  20
IntGELU    | n:  29
IntSoftmax | n:  20
IntGELU    | n:  29
.calib done
Test: [  0/391]	Time  3.951 ( 3.951)	Acc@1   6.25 (  6.25)	Acc@5  16.41 ( 16.41)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 6.250 Prec@5 16.406
Time: 8.98
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=20, intgelu_exp_n=30
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=20, intgelu_exp_n=30, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  20
IntGELU    | n:  30
IntSoftmax | n:  20
IntGELU    | n:  30
IntSoftmax | n:  20
IntGELU    | n:  30
IntSoftmax | n:  20
IntGELU    | n:  30
IntSoftmax | n:  20
IntGELU    | n:  30
IntSoftmax | n:  20
IntGELU    | n:  30
IntSoftmax | n:  20
IntGELU    | n:  30
IntSoftmax | n:  20
IntGELU    | n:  30
IntSoftmax | n:  20
IntGELU    | n:  30
IntSoftmax | n:  20
IntGELU    | n:  30
IntSoftmax | n:  20
IntGELU    | n:  30
IntSoftmax | n:  20
IntGELU    | n:  30
.calib done
Test: [  0/391]	Time  3.819 ( 3.819)	Acc@1   0.78 (  0.78)	Acc@5   7.03 (  7.03)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.781 Prec@5 7.031
Time: 8.87
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=20, intgelu_exp_n=31
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=20, intgelu_exp_n=31, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  20
IntGELU    | n:  31
IntSoftmax | n:  20
IntGELU    | n:  31
IntSoftmax | n:  20
IntGELU    | n:  31
IntSoftmax | n:  20
IntGELU    | n:  31
IntSoftmax | n:  20
IntGELU    | n:  31
IntSoftmax | n:  20
IntGELU    | n:  31
IntSoftmax | n:  20
IntGELU    | n:  31
IntSoftmax | n:  20
IntGELU    | n:  31
IntSoftmax | n:  20
IntGELU    | n:  31
IntSoftmax | n:  20
IntGELU    | n:  31
IntSoftmax | n:  20
IntGELU    | n:  31
IntSoftmax | n:  20
IntGELU    | n:  31
.calib done
Test: [  0/391]	Time  4.012 ( 4.012)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.07
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=21, intgelu_exp_n=0
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=21, intgelu_exp_n=0, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  21
IntGELU    | n:  0
IntSoftmax | n:  21
IntGELU    | n:  0
IntSoftmax | n:  21
IntGELU    | n:  0
IntSoftmax | n:  21
IntGELU    | n:  0
IntSoftmax | n:  21
IntGELU    | n:  0
IntSoftmax | n:  21
IntGELU    | n:  0
IntSoftmax | n:  21
IntGELU    | n:  0
IntSoftmax | n:  21
IntGELU    | n:  0
IntSoftmax | n:  21
IntGELU    | n:  0
IntSoftmax | n:  21
IntGELU    | n:  0
IntSoftmax | n:  21
IntGELU    | n:  0
IntSoftmax | n:  21
IntGELU    | n:  0
.calib done
Test: [  0/391]	Time  3.949 ( 3.949)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.01
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=21, intgelu_exp_n=1
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=21, intgelu_exp_n=1, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  21
IntGELU    | n:  1
IntSoftmax | n:  21
IntGELU    | n:  1
IntSoftmax | n:  21
IntGELU    | n:  1
IntSoftmax | n:  21
IntGELU    | n:  1
IntSoftmax | n:  21
IntGELU    | n:  1
IntSoftmax | n:  21
IntGELU    | n:  1
IntSoftmax | n:  21
IntGELU    | n:  1
IntSoftmax | n:  21
IntGELU    | n:  1
IntSoftmax | n:  21
IntGELU    | n:  1
IntSoftmax | n:  21
IntGELU    | n:  1
IntSoftmax | n:  21
IntGELU    | n:  1
IntSoftmax | n:  21
IntGELU    | n:  1
.calib done
Test: [  0/391]	Time  3.836 ( 3.836)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.88
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=21, intgelu_exp_n=2
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=21, intgelu_exp_n=2, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  21
IntGELU    | n:  2
IntSoftmax | n:  21
IntGELU    | n:  2
IntSoftmax | n:  21
IntGELU    | n:  2
IntSoftmax | n:  21
IntGELU    | n:  2
IntSoftmax | n:  21
IntGELU    | n:  2
IntSoftmax | n:  21
IntGELU    | n:  2
IntSoftmax | n:  21
IntGELU    | n:  2
IntSoftmax | n:  21
IntGELU    | n:  2
IntSoftmax | n:  21
IntGELU    | n:  2
IntSoftmax | n:  21
IntGELU    | n:  2
IntSoftmax | n:  21
IntGELU    | n:  2
IntSoftmax | n:  21
IntGELU    | n:  2
.calib done
Test: [  0/391]	Time  3.888 ( 3.888)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=21, intgelu_exp_n=3
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=21, intgelu_exp_n=3, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  21
IntGELU    | n:  3
IntSoftmax | n:  21
IntGELU    | n:  3
IntSoftmax | n:  21
IntGELU    | n:  3
IntSoftmax | n:  21
IntGELU    | n:  3
IntSoftmax | n:  21
IntGELU    | n:  3
IntSoftmax | n:  21
IntGELU    | n:  3
IntSoftmax | n:  21
IntGELU    | n:  3
IntSoftmax | n:  21
IntGELU    | n:  3
IntSoftmax | n:  21
IntGELU    | n:  3
IntSoftmax | n:  21
IntGELU    | n:  3
IntSoftmax | n:  21
IntGELU    | n:  3
IntSoftmax | n:  21
IntGELU    | n:  3
.calib done
Test: [  0/391]	Time  3.892 ( 3.892)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.93
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=21, intgelu_exp_n=4
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=21, intgelu_exp_n=4, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  21
IntGELU    | n:  4
IntSoftmax | n:  21
IntGELU    | n:  4
IntSoftmax | n:  21
IntGELU    | n:  4
IntSoftmax | n:  21
IntGELU    | n:  4
IntSoftmax | n:  21
IntGELU    | n:  4
IntSoftmax | n:  21
IntGELU    | n:  4
IntSoftmax | n:  21
IntGELU    | n:  4
IntSoftmax | n:  21
IntGELU    | n:  4
IntSoftmax | n:  21
IntGELU    | n:  4
IntSoftmax | n:  21
IntGELU    | n:  4
IntSoftmax | n:  21
IntGELU    | n:  4
IntSoftmax | n:  21
IntGELU    | n:  4
.calib done
Test: [  0/391]	Time  3.911 ( 3.911)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.94
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=21, intgelu_exp_n=5
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=21, intgelu_exp_n=5, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  21
IntGELU    | n:  5
IntSoftmax | n:  21
IntGELU    | n:  5
IntSoftmax | n:  21
IntGELU    | n:  5
IntSoftmax | n:  21
IntGELU    | n:  5
IntSoftmax | n:  21
IntGELU    | n:  5
IntSoftmax | n:  21
IntGELU    | n:  5
IntSoftmax | n:  21
IntGELU    | n:  5
IntSoftmax | n:  21
IntGELU    | n:  5
IntSoftmax | n:  21
IntGELU    | n:  5
IntSoftmax | n:  21
IntGELU    | n:  5
IntSoftmax | n:  21
IntGELU    | n:  5
IntSoftmax | n:  21
IntGELU    | n:  5
.calib done
Test: [  0/391]	Time  3.653 ( 3.653)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.72
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=21, intgelu_exp_n=6
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=21, intgelu_exp_n=6, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  21
IntGELU    | n:  6
IntSoftmax | n:  21
IntGELU    | n:  6
IntSoftmax | n:  21
IntGELU    | n:  6
IntSoftmax | n:  21
IntGELU    | n:  6
IntSoftmax | n:  21
IntGELU    | n:  6
IntSoftmax | n:  21
IntGELU    | n:  6
IntSoftmax | n:  21
IntGELU    | n:  6
IntSoftmax | n:  21
IntGELU    | n:  6
IntSoftmax | n:  21
IntGELU    | n:  6
IntSoftmax | n:  21
IntGELU    | n:  6
IntSoftmax | n:  21
IntGELU    | n:  6
IntSoftmax | n:  21
IntGELU    | n:  6
.calib done
Test: [  0/391]	Time  3.823 ( 3.823)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.90
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=21, intgelu_exp_n=7
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=21, intgelu_exp_n=7, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  21
IntGELU    | n:  7
IntSoftmax | n:  21
IntGELU    | n:  7
IntSoftmax | n:  21
IntGELU    | n:  7
IntSoftmax | n:  21
IntGELU    | n:  7
IntSoftmax | n:  21
IntGELU    | n:  7
IntSoftmax | n:  21
IntGELU    | n:  7
IntSoftmax | n:  21
IntGELU    | n:  7
IntSoftmax | n:  21
IntGELU    | n:  7
IntSoftmax | n:  21
IntGELU    | n:  7
IntSoftmax | n:  21
IntGELU    | n:  7
IntSoftmax | n:  21
IntGELU    | n:  7
IntSoftmax | n:  21
IntGELU    | n:  7
.calib done
Test: [  0/391]	Time  3.689 ( 3.689)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.73
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=21, intgelu_exp_n=8
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=21, intgelu_exp_n=8, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  21
IntGELU    | n:  8
IntSoftmax | n:  21
IntGELU    | n:  8
IntSoftmax | n:  21
IntGELU    | n:  8
IntSoftmax | n:  21
IntGELU    | n:  8
IntSoftmax | n:  21
IntGELU    | n:  8
IntSoftmax | n:  21
IntGELU    | n:  8
IntSoftmax | n:  21
IntGELU    | n:  8
IntSoftmax | n:  21
IntGELU    | n:  8
IntSoftmax | n:  21
IntGELU    | n:  8
IntSoftmax | n:  21
IntGELU    | n:  8
IntSoftmax | n:  21
IntGELU    | n:  8
IntSoftmax | n:  21
IntGELU    | n:  8
.calib done
Test: [  0/391]	Time  3.923 ( 3.923)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.00
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=21, intgelu_exp_n=9
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=21, intgelu_exp_n=9, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  21
IntGELU    | n:  9
IntSoftmax | n:  21
IntGELU    | n:  9
IntSoftmax | n:  21
IntGELU    | n:  9
IntSoftmax | n:  21
IntGELU    | n:  9
IntSoftmax | n:  21
IntGELU    | n:  9
IntSoftmax | n:  21
IntGELU    | n:  9
IntSoftmax | n:  21
IntGELU    | n:  9
IntSoftmax | n:  21
IntGELU    | n:  9
IntSoftmax | n:  21
IntGELU    | n:  9
IntSoftmax | n:  21
IntGELU    | n:  9
IntSoftmax | n:  21
IntGELU    | n:  9
IntSoftmax | n:  21
IntGELU    | n:  9
.calib done
Test: [  0/391]	Time  3.952 ( 3.952)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=21, intgelu_exp_n=10
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=21, intgelu_exp_n=10, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  21
IntGELU    | n:  10
IntSoftmax | n:  21
IntGELU    | n:  10
IntSoftmax | n:  21
IntGELU    | n:  10
IntSoftmax | n:  21
IntGELU    | n:  10
IntSoftmax | n:  21
IntGELU    | n:  10
IntSoftmax | n:  21
IntGELU    | n:  10
IntSoftmax | n:  21
IntGELU    | n:  10
IntSoftmax | n:  21
IntGELU    | n:  10
IntSoftmax | n:  21
IntGELU    | n:  10
IntSoftmax | n:  21
IntGELU    | n:  10
IntSoftmax | n:  21
IntGELU    | n:  10
IntSoftmax | n:  21
IntGELU    | n:  10
.calib done
Test: [  0/391]	Time  4.016 ( 4.016)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.06
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=21, intgelu_exp_n=11
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=21, intgelu_exp_n=11, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  21
IntGELU    | n:  11
IntSoftmax | n:  21
IntGELU    | n:  11
IntSoftmax | n:  21
IntGELU    | n:  11
IntSoftmax | n:  21
IntGELU    | n:  11
IntSoftmax | n:  21
IntGELU    | n:  11
IntSoftmax | n:  21
IntGELU    | n:  11
IntSoftmax | n:  21
IntGELU    | n:  11
IntSoftmax | n:  21
IntGELU    | n:  11
IntSoftmax | n:  21
IntGELU    | n:  11
IntSoftmax | n:  21
IntGELU    | n:  11
IntSoftmax | n:  21
IntGELU    | n:  11
IntSoftmax | n:  21
IntGELU    | n:  11
.calib done
Test: [  0/391]	Time  3.944 ( 3.944)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.01
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=21, intgelu_exp_n=12
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=21, intgelu_exp_n=12, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  21
IntGELU    | n:  12
IntSoftmax | n:  21
IntGELU    | n:  12
IntSoftmax | n:  21
IntGELU    | n:  12
IntSoftmax | n:  21
IntGELU    | n:  12
IntSoftmax | n:  21
IntGELU    | n:  12
IntSoftmax | n:  21
IntGELU    | n:  12
IntSoftmax | n:  21
IntGELU    | n:  12
IntSoftmax | n:  21
IntGELU    | n:  12
IntSoftmax | n:  21
IntGELU    | n:  12
IntSoftmax | n:  21
IntGELU    | n:  12
IntSoftmax | n:  21
IntGELU    | n:  12
IntSoftmax | n:  21
IntGELU    | n:  12
.calib done
Test: [  0/391]	Time  3.936 ( 3.936)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=21, intgelu_exp_n=13
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=21, intgelu_exp_n=13, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  21
IntGELU    | n:  13
IntSoftmax | n:  21
IntGELU    | n:  13
IntSoftmax | n:  21
IntGELU    | n:  13
IntSoftmax | n:  21
IntGELU    | n:  13
IntSoftmax | n:  21
IntGELU    | n:  13
IntSoftmax | n:  21
IntGELU    | n:  13
IntSoftmax | n:  21
IntGELU    | n:  13
IntSoftmax | n:  21
IntGELU    | n:  13
IntSoftmax | n:  21
IntGELU    | n:  13
IntSoftmax | n:  21
IntGELU    | n:  13
IntSoftmax | n:  21
IntGELU    | n:  13
IntSoftmax | n:  21
IntGELU    | n:  13
.calib done
Test: [  0/391]	Time  3.922 ( 3.922)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=21, intgelu_exp_n=14
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=21, intgelu_exp_n=14, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  21
IntGELU    | n:  14
IntSoftmax | n:  21
IntGELU    | n:  14
IntSoftmax | n:  21
IntGELU    | n:  14
IntSoftmax | n:  21
IntGELU    | n:  14
IntSoftmax | n:  21
IntGELU    | n:  14
IntSoftmax | n:  21
IntGELU    | n:  14
IntSoftmax | n:  21
IntGELU    | n:  14
IntSoftmax | n:  21
IntGELU    | n:  14
IntSoftmax | n:  21
IntGELU    | n:  14
IntSoftmax | n:  21
IntGELU    | n:  14
IntSoftmax | n:  21
IntGELU    | n:  14
IntSoftmax | n:  21
IntGELU    | n:  14
.calib done
Test: [  0/391]	Time  3.919 ( 3.919)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.98
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=21, intgelu_exp_n=15
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=21, intgelu_exp_n=15, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  21
IntGELU    | n:  15
IntSoftmax | n:  21
IntGELU    | n:  15
IntSoftmax | n:  21
IntGELU    | n:  15
IntSoftmax | n:  21
IntGELU    | n:  15
IntSoftmax | n:  21
IntGELU    | n:  15
IntSoftmax | n:  21
IntGELU    | n:  15
IntSoftmax | n:  21
IntGELU    | n:  15
IntSoftmax | n:  21
IntGELU    | n:  15
IntSoftmax | n:  21
IntGELU    | n:  15
IntSoftmax | n:  21
IntGELU    | n:  15
IntSoftmax | n:  21
IntGELU    | n:  15
IntSoftmax | n:  21
IntGELU    | n:  15
.calib done
Test: [  0/391]	Time  3.820 ( 3.820)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.85
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=21, intgelu_exp_n=16
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=21, intgelu_exp_n=16, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  21
IntGELU    | n:  16
IntSoftmax | n:  21
IntGELU    | n:  16
IntSoftmax | n:  21
IntGELU    | n:  16
IntSoftmax | n:  21
IntGELU    | n:  16
IntSoftmax | n:  21
IntGELU    | n:  16
IntSoftmax | n:  21
IntGELU    | n:  16
IntSoftmax | n:  21
IntGELU    | n:  16
IntSoftmax | n:  21
IntGELU    | n:  16
IntSoftmax | n:  21
IntGELU    | n:  16
IntSoftmax | n:  21
IntGELU    | n:  16
IntSoftmax | n:  21
IntGELU    | n:  16
IntSoftmax | n:  21
IntGELU    | n:  16
.calib done
Test: [  0/391]	Time  3.944 ( 3.944)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.00
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=21, intgelu_exp_n=17
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=21, intgelu_exp_n=17, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  21
IntGELU    | n:  17
IntSoftmax | n:  21
IntGELU    | n:  17
IntSoftmax | n:  21
IntGELU    | n:  17
IntSoftmax | n:  21
IntGELU    | n:  17
IntSoftmax | n:  21
IntGELU    | n:  17
IntSoftmax | n:  21
IntGELU    | n:  17
IntSoftmax | n:  21
IntGELU    | n:  17
IntSoftmax | n:  21
IntGELU    | n:  17
IntSoftmax | n:  21
IntGELU    | n:  17
IntSoftmax | n:  21
IntGELU    | n:  17
IntSoftmax | n:  21
IntGELU    | n:  17
IntSoftmax | n:  21
IntGELU    | n:  17
.calib done
Test: [  0/391]	Time  3.832 ( 3.832)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.85
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=21, intgelu_exp_n=18
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=21, intgelu_exp_n=18, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  21
IntGELU    | n:  18
IntSoftmax | n:  21
IntGELU    | n:  18
IntSoftmax | n:  21
IntGELU    | n:  18
IntSoftmax | n:  21
IntGELU    | n:  18
IntSoftmax | n:  21
IntGELU    | n:  18
IntSoftmax | n:  21
IntGELU    | n:  18
IntSoftmax | n:  21
IntGELU    | n:  18
IntSoftmax | n:  21
IntGELU    | n:  18
IntSoftmax | n:  21
IntGELU    | n:  18
IntSoftmax | n:  21
IntGELU    | n:  18
IntSoftmax | n:  21
IntGELU    | n:  18
IntSoftmax | n:  21
IntGELU    | n:  18
.calib done
Test: [  0/391]	Time  3.906 ( 3.906)	Acc@1   0.00 (  0.00)	Acc@5   0.78 (  0.78)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.781
Time: 8.99
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=21, intgelu_exp_n=19
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=21, intgelu_exp_n=19, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  21
IntGELU    | n:  19
IntSoftmax | n:  21
IntGELU    | n:  19
IntSoftmax | n:  21
IntGELU    | n:  19
IntSoftmax | n:  21
IntGELU    | n:  19
IntSoftmax | n:  21
IntGELU    | n:  19
IntSoftmax | n:  21
IntGELU    | n:  19
IntSoftmax | n:  21
IntGELU    | n:  19
IntSoftmax | n:  21
IntGELU    | n:  19
IntSoftmax | n:  21
IntGELU    | n:  19
IntSoftmax | n:  21
IntGELU    | n:  19
IntSoftmax | n:  21
IntGELU    | n:  19
IntSoftmax | n:  21
IntGELU    | n:  19
.calib done
Test: [  0/391]	Time  4.031 ( 4.031)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.11
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=21, intgelu_exp_n=20
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=21, intgelu_exp_n=20, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  21
IntGELU    | n:  20
IntSoftmax | n:  21
IntGELU    | n:  20
IntSoftmax | n:  21
IntGELU    | n:  20
IntSoftmax | n:  21
IntGELU    | n:  20
IntSoftmax | n:  21
IntGELU    | n:  20
IntSoftmax | n:  21
IntGELU    | n:  20
IntSoftmax | n:  21
IntGELU    | n:  20
IntSoftmax | n:  21
IntGELU    | n:  20
IntSoftmax | n:  21
IntGELU    | n:  20
IntSoftmax | n:  21
IntGELU    | n:  20
IntSoftmax | n:  21
IntGELU    | n:  20
IntSoftmax | n:  21
IntGELU    | n:  20
.calib done
Test: [  0/391]	Time  3.750 ( 3.750)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.76
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=21, intgelu_exp_n=21
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=21, intgelu_exp_n=21, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  21
IntGELU    | n:  21
IntSoftmax | n:  21
IntGELU    | n:  21
IntSoftmax | n:  21
IntGELU    | n:  21
IntSoftmax | n:  21
IntGELU    | n:  21
IntSoftmax | n:  21
IntGELU    | n:  21
IntSoftmax | n:  21
IntGELU    | n:  21
IntSoftmax | n:  21
IntGELU    | n:  21
IntSoftmax | n:  21
IntGELU    | n:  21
IntSoftmax | n:  21
IntGELU    | n:  21
IntSoftmax | n:  21
IntGELU    | n:  21
IntSoftmax | n:  21
IntGELU    | n:  21
IntSoftmax | n:  21
IntGELU    | n:  21
.calib done
Test: [  0/391]	Time  4.087 ( 4.087)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.14
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=21, intgelu_exp_n=22
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=21, intgelu_exp_n=22, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  21
IntGELU    | n:  22
IntSoftmax | n:  21
IntGELU    | n:  22
IntSoftmax | n:  21
IntGELU    | n:  22
IntSoftmax | n:  21
IntGELU    | n:  22
IntSoftmax | n:  21
IntGELU    | n:  22
IntSoftmax | n:  21
IntGELU    | n:  22
IntSoftmax | n:  21
IntGELU    | n:  22
IntSoftmax | n:  21
IntGELU    | n:  22
IntSoftmax | n:  21
IntGELU    | n:  22
IntSoftmax | n:  21
IntGELU    | n:  22
IntSoftmax | n:  21
IntGELU    | n:  22
IntSoftmax | n:  21
IntGELU    | n:  22
.calib done
Test: [  0/391]	Time  3.944 ( 3.944)	Acc@1   0.78 (  0.78)	Acc@5   0.78 (  0.78)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.781 Prec@5 0.781
Time: 8.94
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=21, intgelu_exp_n=23
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=21, intgelu_exp_n=23, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  21
IntGELU    | n:  23
IntSoftmax | n:  21
IntGELU    | n:  23
IntSoftmax | n:  21
IntGELU    | n:  23
IntSoftmax | n:  21
IntGELU    | n:  23
IntSoftmax | n:  21
IntGELU    | n:  23
IntSoftmax | n:  21
IntGELU    | n:  23
IntSoftmax | n:  21
IntGELU    | n:  23
IntSoftmax | n:  21
IntGELU    | n:  23
IntSoftmax | n:  21
IntGELU    | n:  23
IntSoftmax | n:  21
IntGELU    | n:  23
IntSoftmax | n:  21
IntGELU    | n:  23
IntSoftmax | n:  21
IntGELU    | n:  23
.calib done
Test: [  0/391]	Time  3.888 ( 3.888)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.93
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=21, intgelu_exp_n=24
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=21, intgelu_exp_n=24, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  21
IntGELU    | n:  24
IntSoftmax | n:  21
IntGELU    | n:  24
IntSoftmax | n:  21
IntGELU    | n:  24
IntSoftmax | n:  21
IntGELU    | n:  24
IntSoftmax | n:  21
IntGELU    | n:  24
IntSoftmax | n:  21
IntGELU    | n:  24
IntSoftmax | n:  21
IntGELU    | n:  24
IntSoftmax | n:  21
IntGELU    | n:  24
IntSoftmax | n:  21
IntGELU    | n:  24
IntSoftmax | n:  21
IntGELU    | n:  24
IntSoftmax | n:  21
IntGELU    | n:  24
IntSoftmax | n:  21
IntGELU    | n:  24
.calib done
Test: [  0/391]	Time  3.886 ( 3.886)	Acc@1   0.78 (  0.78)	Acc@5   0.78 (  0.78)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.781 Prec@5 0.781
Time: 8.91
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=21, intgelu_exp_n=25
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=21, intgelu_exp_n=25, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  21
IntGELU    | n:  25
IntSoftmax | n:  21
IntGELU    | n:  25
IntSoftmax | n:  21
IntGELU    | n:  25
IntSoftmax | n:  21
IntGELU    | n:  25
IntSoftmax | n:  21
IntGELU    | n:  25
IntSoftmax | n:  21
IntGELU    | n:  25
IntSoftmax | n:  21
IntGELU    | n:  25
IntSoftmax | n:  21
IntGELU    | n:  25
IntSoftmax | n:  21
IntGELU    | n:  25
IntSoftmax | n:  21
IntGELU    | n:  25
IntSoftmax | n:  21
IntGELU    | n:  25
IntSoftmax | n:  21
IntGELU    | n:  25
.calib done
Test: [  0/391]	Time  3.767 ( 3.767)	Acc@1   0.00 (  0.00)	Acc@5   1.56 (  1.56)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 1.562
Time: 8.86
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=21, intgelu_exp_n=26
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=21, intgelu_exp_n=26, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  21
IntGELU    | n:  26
IntSoftmax | n:  21
IntGELU    | n:  26
IntSoftmax | n:  21
IntGELU    | n:  26
IntSoftmax | n:  21
IntGELU    | n:  26
IntSoftmax | n:  21
IntGELU    | n:  26
IntSoftmax | n:  21
IntGELU    | n:  26
IntSoftmax | n:  21
IntGELU    | n:  26
IntSoftmax | n:  21
IntGELU    | n:  26
IntSoftmax | n:  21
IntGELU    | n:  26
IntSoftmax | n:  21
IntGELU    | n:  26
IntSoftmax | n:  21
IntGELU    | n:  26
IntSoftmax | n:  21
IntGELU    | n:  26
.calib done
Test: [  0/391]	Time  3.858 ( 3.858)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.92
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=21, intgelu_exp_n=27
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=21, intgelu_exp_n=27, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  21
IntGELU    | n:  27
IntSoftmax | n:  21
IntGELU    | n:  27
IntSoftmax | n:  21
IntGELU    | n:  27
IntSoftmax | n:  21
IntGELU    | n:  27
IntSoftmax | n:  21
IntGELU    | n:  27
IntSoftmax | n:  21
IntGELU    | n:  27
IntSoftmax | n:  21
IntGELU    | n:  27
IntSoftmax | n:  21
IntGELU    | n:  27
IntSoftmax | n:  21
IntGELU    | n:  27
IntSoftmax | n:  21
IntGELU    | n:  27
IntSoftmax | n:  21
IntGELU    | n:  27
IntSoftmax | n:  21
IntGELU    | n:  27
.calib done
Test: [  0/391]	Time  3.821 ( 3.821)	Acc@1   0.00 (  0.00)	Acc@5   1.56 (  1.56)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 1.562
Time: 8.93
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=21, intgelu_exp_n=28
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=21, intgelu_exp_n=28, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  21
IntGELU    | n:  28
IntSoftmax | n:  21
IntGELU    | n:  28
IntSoftmax | n:  21
IntGELU    | n:  28
IntSoftmax | n:  21
IntGELU    | n:  28
IntSoftmax | n:  21
IntGELU    | n:  28
IntSoftmax | n:  21
IntGELU    | n:  28
IntSoftmax | n:  21
IntGELU    | n:  28
IntSoftmax | n:  21
IntGELU    | n:  28
IntSoftmax | n:  21
IntGELU    | n:  28
IntSoftmax | n:  21
IntGELU    | n:  28
IntSoftmax | n:  21
IntGELU    | n:  28
IntSoftmax | n:  21
IntGELU    | n:  28
.calib done
Test: [  0/391]	Time  3.993 ( 3.993)	Acc@1   0.00 (  0.00)	Acc@5   3.91 (  3.91)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 3.906
Time: 9.05
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=21, intgelu_exp_n=29
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=21, intgelu_exp_n=29, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  21
IntGELU    | n:  29
IntSoftmax | n:  21
IntGELU    | n:  29
IntSoftmax | n:  21
IntGELU    | n:  29
IntSoftmax | n:  21
IntGELU    | n:  29
IntSoftmax | n:  21
IntGELU    | n:  29
IntSoftmax | n:  21
IntGELU    | n:  29
IntSoftmax | n:  21
IntGELU    | n:  29
IntSoftmax | n:  21
IntGELU    | n:  29
IntSoftmax | n:  21
IntGELU    | n:  29
IntSoftmax | n:  21
IntGELU    | n:  29
IntSoftmax | n:  21
IntGELU    | n:  29
IntSoftmax | n:  21
IntGELU    | n:  29
.calib done
Test: [  0/391]	Time  3.787 ( 3.787)	Acc@1   2.34 (  2.34)	Acc@5   7.81 (  7.81)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 2.344 Prec@5 7.812
Time: 8.86
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=21, intgelu_exp_n=30
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=21, intgelu_exp_n=30, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  21
IntGELU    | n:  30
IntSoftmax | n:  21
IntGELU    | n:  30
IntSoftmax | n:  21
IntGELU    | n:  30
IntSoftmax | n:  21
IntGELU    | n:  30
IntSoftmax | n:  21
IntGELU    | n:  30
IntSoftmax | n:  21
IntGELU    | n:  30
IntSoftmax | n:  21
IntGELU    | n:  30
IntSoftmax | n:  21
IntGELU    | n:  30
IntSoftmax | n:  21
IntGELU    | n:  30
IntSoftmax | n:  21
IntGELU    | n:  30
IntSoftmax | n:  21
IntGELU    | n:  30
IntSoftmax | n:  21
IntGELU    | n:  30
.calib done
Test: [  0/391]	Time  3.994 ( 3.994)	Acc@1   4.69 (  4.69)	Acc@5  13.28 ( 13.28)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 4.688 Prec@5 13.281
Time: 8.98
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=21, intgelu_exp_n=31
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=21, intgelu_exp_n=31, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  21
IntGELU    | n:  31
IntSoftmax | n:  21
IntGELU    | n:  31
IntSoftmax | n:  21
IntGELU    | n:  31
IntSoftmax | n:  21
IntGELU    | n:  31
IntSoftmax | n:  21
IntGELU    | n:  31
IntSoftmax | n:  21
IntGELU    | n:  31
IntSoftmax | n:  21
IntGELU    | n:  31
IntSoftmax | n:  21
IntGELU    | n:  31
IntSoftmax | n:  21
IntGELU    | n:  31
IntSoftmax | n:  21
IntGELU    | n:  31
IntSoftmax | n:  21
IntGELU    | n:  31
IntSoftmax | n:  21
IntGELU    | n:  31
.calib done
Test: [  0/391]	Time  4.061 ( 4.061)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.09
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=22, intgelu_exp_n=0
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=22, intgelu_exp_n=0, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  22
IntGELU    | n:  0
IntSoftmax | n:  22
IntGELU    | n:  0
IntSoftmax | n:  22
IntGELU    | n:  0
IntSoftmax | n:  22
IntGELU    | n:  0
IntSoftmax | n:  22
IntGELU    | n:  0
IntSoftmax | n:  22
IntGELU    | n:  0
IntSoftmax | n:  22
IntGELU    | n:  0
IntSoftmax | n:  22
IntGELU    | n:  0
IntSoftmax | n:  22
IntGELU    | n:  0
IntSoftmax | n:  22
IntGELU    | n:  0
IntSoftmax | n:  22
IntGELU    | n:  0
IntSoftmax | n:  22
IntGELU    | n:  0
.calib done
Test: [  0/391]	Time  4.016 ( 4.016)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.04
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=22, intgelu_exp_n=1
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=22, intgelu_exp_n=1, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  22
IntGELU    | n:  1
IntSoftmax | n:  22
IntGELU    | n:  1
IntSoftmax | n:  22
IntGELU    | n:  1
IntSoftmax | n:  22
IntGELU    | n:  1
IntSoftmax | n:  22
IntGELU    | n:  1
IntSoftmax | n:  22
IntGELU    | n:  1
IntSoftmax | n:  22
IntGELU    | n:  1
IntSoftmax | n:  22
IntGELU    | n:  1
IntSoftmax | n:  22
IntGELU    | n:  1
IntSoftmax | n:  22
IntGELU    | n:  1
IntSoftmax | n:  22
IntGELU    | n:  1
IntSoftmax | n:  22
IntGELU    | n:  1
.calib done
Test: [  0/391]	Time  4.002 ( 4.002)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.04
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=22, intgelu_exp_n=2
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=22, intgelu_exp_n=2, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  22
IntGELU    | n:  2
IntSoftmax | n:  22
IntGELU    | n:  2
IntSoftmax | n:  22
IntGELU    | n:  2
IntSoftmax | n:  22
IntGELU    | n:  2
IntSoftmax | n:  22
IntGELU    | n:  2
IntSoftmax | n:  22
IntGELU    | n:  2
IntSoftmax | n:  22
IntGELU    | n:  2
IntSoftmax | n:  22
IntGELU    | n:  2
IntSoftmax | n:  22
IntGELU    | n:  2
IntSoftmax | n:  22
IntGELU    | n:  2
IntSoftmax | n:  22
IntGELU    | n:  2
IntSoftmax | n:  22
IntGELU    | n:  2
.calib done
Test: [  0/391]	Time  3.743 ( 3.743)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.80
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=22, intgelu_exp_n=3
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=22, intgelu_exp_n=3, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  22
IntGELU    | n:  3
IntSoftmax | n:  22
IntGELU    | n:  3
IntSoftmax | n:  22
IntGELU    | n:  3
IntSoftmax | n:  22
IntGELU    | n:  3
IntSoftmax | n:  22
IntGELU    | n:  3
IntSoftmax | n:  22
IntGELU    | n:  3
IntSoftmax | n:  22
IntGELU    | n:  3
IntSoftmax | n:  22
IntGELU    | n:  3
IntSoftmax | n:  22
IntGELU    | n:  3
IntSoftmax | n:  22
IntGELU    | n:  3
IntSoftmax | n:  22
IntGELU    | n:  3
IntSoftmax | n:  22
IntGELU    | n:  3
.calib done
Test: [  0/391]	Time  3.933 ( 3.933)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=22, intgelu_exp_n=4
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=22, intgelu_exp_n=4, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  22
IntGELU    | n:  4
IntSoftmax | n:  22
IntGELU    | n:  4
IntSoftmax | n:  22
IntGELU    | n:  4
IntSoftmax | n:  22
IntGELU    | n:  4
IntSoftmax | n:  22
IntGELU    | n:  4
IntSoftmax | n:  22
IntGELU    | n:  4
IntSoftmax | n:  22
IntGELU    | n:  4
IntSoftmax | n:  22
IntGELU    | n:  4
IntSoftmax | n:  22
IntGELU    | n:  4
IntSoftmax | n:  22
IntGELU    | n:  4
IntSoftmax | n:  22
IntGELU    | n:  4
IntSoftmax | n:  22
IntGELU    | n:  4
.calib done
Test: [  0/391]	Time  3.836 ( 3.836)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.85
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=22, intgelu_exp_n=5
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=22, intgelu_exp_n=5, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  22
IntGELU    | n:  5
IntSoftmax | n:  22
IntGELU    | n:  5
IntSoftmax | n:  22
IntGELU    | n:  5
IntSoftmax | n:  22
IntGELU    | n:  5
IntSoftmax | n:  22
IntGELU    | n:  5
IntSoftmax | n:  22
IntGELU    | n:  5
IntSoftmax | n:  22
IntGELU    | n:  5
IntSoftmax | n:  22
IntGELU    | n:  5
IntSoftmax | n:  22
IntGELU    | n:  5
IntSoftmax | n:  22
IntGELU    | n:  5
IntSoftmax | n:  22
IntGELU    | n:  5
IntSoftmax | n:  22
IntGELU    | n:  5
.calib done
Test: [  0/391]	Time  3.889 ( 3.889)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.90
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=22, intgelu_exp_n=6
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=22, intgelu_exp_n=6, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  22
IntGELU    | n:  6
IntSoftmax | n:  22
IntGELU    | n:  6
IntSoftmax | n:  22
IntGELU    | n:  6
IntSoftmax | n:  22
IntGELU    | n:  6
IntSoftmax | n:  22
IntGELU    | n:  6
IntSoftmax | n:  22
IntGELU    | n:  6
IntSoftmax | n:  22
IntGELU    | n:  6
IntSoftmax | n:  22
IntGELU    | n:  6
IntSoftmax | n:  22
IntGELU    | n:  6
IntSoftmax | n:  22
IntGELU    | n:  6
IntSoftmax | n:  22
IntGELU    | n:  6
IntSoftmax | n:  22
IntGELU    | n:  6
.calib done
Test: [  0/391]	Time  3.869 ( 3.869)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.92
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=22, intgelu_exp_n=7
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=22, intgelu_exp_n=7, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  22
IntGELU    | n:  7
IntSoftmax | n:  22
IntGELU    | n:  7
IntSoftmax | n:  22
IntGELU    | n:  7
IntSoftmax | n:  22
IntGELU    | n:  7
IntSoftmax | n:  22
IntGELU    | n:  7
IntSoftmax | n:  22
IntGELU    | n:  7
IntSoftmax | n:  22
IntGELU    | n:  7
IntSoftmax | n:  22
IntGELU    | n:  7
IntSoftmax | n:  22
IntGELU    | n:  7
IntSoftmax | n:  22
IntGELU    | n:  7
IntSoftmax | n:  22
IntGELU    | n:  7
IntSoftmax | n:  22
IntGELU    | n:  7
.calib done
Test: [  0/391]	Time  3.896 ( 3.896)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.98
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=22, intgelu_exp_n=8
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=22, intgelu_exp_n=8, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  22
IntGELU    | n:  8
IntSoftmax | n:  22
IntGELU    | n:  8
IntSoftmax | n:  22
IntGELU    | n:  8
IntSoftmax | n:  22
IntGELU    | n:  8
IntSoftmax | n:  22
IntGELU    | n:  8
IntSoftmax | n:  22
IntGELU    | n:  8
IntSoftmax | n:  22
IntGELU    | n:  8
IntSoftmax | n:  22
IntGELU    | n:  8
IntSoftmax | n:  22
IntGELU    | n:  8
IntSoftmax | n:  22
IntGELU    | n:  8
IntSoftmax | n:  22
IntGELU    | n:  8
IntSoftmax | n:  22
IntGELU    | n:  8
.calib done
Test: [  0/391]	Time  3.889 ( 3.889)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.98
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=22, intgelu_exp_n=9
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=22, intgelu_exp_n=9, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  22
IntGELU    | n:  9
IntSoftmax | n:  22
IntGELU    | n:  9
IntSoftmax | n:  22
IntGELU    | n:  9
IntSoftmax | n:  22
IntGELU    | n:  9
IntSoftmax | n:  22
IntGELU    | n:  9
IntSoftmax | n:  22
IntGELU    | n:  9
IntSoftmax | n:  22
IntGELU    | n:  9
IntSoftmax | n:  22
IntGELU    | n:  9
IntSoftmax | n:  22
IntGELU    | n:  9
IntSoftmax | n:  22
IntGELU    | n:  9
IntSoftmax | n:  22
IntGELU    | n:  9
IntSoftmax | n:  22
IntGELU    | n:  9
.calib done
Test: [  0/391]	Time  3.744 ( 3.744)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.76
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=22, intgelu_exp_n=10
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=22, intgelu_exp_n=10, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  22
IntGELU    | n:  10
IntSoftmax | n:  22
IntGELU    | n:  10
IntSoftmax | n:  22
IntGELU    | n:  10
IntSoftmax | n:  22
IntGELU    | n:  10
IntSoftmax | n:  22
IntGELU    | n:  10
IntSoftmax | n:  22
IntGELU    | n:  10
IntSoftmax | n:  22
IntGELU    | n:  10
IntSoftmax | n:  22
IntGELU    | n:  10
IntSoftmax | n:  22
IntGELU    | n:  10
IntSoftmax | n:  22
IntGELU    | n:  10
IntSoftmax | n:  22
IntGELU    | n:  10
IntSoftmax | n:  22
IntGELU    | n:  10
.calib done
Test: [  0/391]	Time  3.896 ( 3.896)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.00
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=22, intgelu_exp_n=11
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=22, intgelu_exp_n=11, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  22
IntGELU    | n:  11
IntSoftmax | n:  22
IntGELU    | n:  11
IntSoftmax | n:  22
IntGELU    | n:  11
IntSoftmax | n:  22
IntGELU    | n:  11
IntSoftmax | n:  22
IntGELU    | n:  11
IntSoftmax | n:  22
IntGELU    | n:  11
IntSoftmax | n:  22
IntGELU    | n:  11
IntSoftmax | n:  22
IntGELU    | n:  11
IntSoftmax | n:  22
IntGELU    | n:  11
IntSoftmax | n:  22
IntGELU    | n:  11
IntSoftmax | n:  22
IntGELU    | n:  11
IntSoftmax | n:  22
IntGELU    | n:  11
.calib done
Test: [  0/391]	Time  3.674 ( 3.674)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.68
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=22, intgelu_exp_n=12
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=22, intgelu_exp_n=12, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  22
IntGELU    | n:  12
IntSoftmax | n:  22
IntGELU    | n:  12
IntSoftmax | n:  22
IntGELU    | n:  12
IntSoftmax | n:  22
IntGELU    | n:  12
IntSoftmax | n:  22
IntGELU    | n:  12
IntSoftmax | n:  22
IntGELU    | n:  12
IntSoftmax | n:  22
IntGELU    | n:  12
IntSoftmax | n:  22
IntGELU    | n:  12
IntSoftmax | n:  22
IntGELU    | n:  12
IntSoftmax | n:  22
IntGELU    | n:  12
IntSoftmax | n:  22
IntGELU    | n:  12
IntSoftmax | n:  22
IntGELU    | n:  12
.calib done
Test: [  0/391]	Time  4.064 ( 4.064)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.05
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=22, intgelu_exp_n=13
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=22, intgelu_exp_n=13, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  22
IntGELU    | n:  13
IntSoftmax | n:  22
IntGELU    | n:  13
IntSoftmax | n:  22
IntGELU    | n:  13
IntSoftmax | n:  22
IntGELU    | n:  13
IntSoftmax | n:  22
IntGELU    | n:  13
IntSoftmax | n:  22
IntGELU    | n:  13
IntSoftmax | n:  22
IntGELU    | n:  13
IntSoftmax | n:  22
IntGELU    | n:  13
IntSoftmax | n:  22
IntGELU    | n:  13
IntSoftmax | n:  22
IntGELU    | n:  13
IntSoftmax | n:  22
IntGELU    | n:  13
IntSoftmax | n:  22
IntGELU    | n:  13
.calib done
Test: [  0/391]	Time  4.027 ( 4.027)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.06
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=22, intgelu_exp_n=14
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=22, intgelu_exp_n=14, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  22
IntGELU    | n:  14
IntSoftmax | n:  22
IntGELU    | n:  14
IntSoftmax | n:  22
IntGELU    | n:  14
IntSoftmax | n:  22
IntGELU    | n:  14
IntSoftmax | n:  22
IntGELU    | n:  14
IntSoftmax | n:  22
IntGELU    | n:  14
IntSoftmax | n:  22
IntGELU    | n:  14
IntSoftmax | n:  22
IntGELU    | n:  14
IntSoftmax | n:  22
IntGELU    | n:  14
IntSoftmax | n:  22
IntGELU    | n:  14
IntSoftmax | n:  22
IntGELU    | n:  14
IntSoftmax | n:  22
IntGELU    | n:  14
.calib done
Test: [  0/391]	Time  3.956 ( 3.956)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.01
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=22, intgelu_exp_n=15
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=22, intgelu_exp_n=15, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  22
IntGELU    | n:  15
IntSoftmax | n:  22
IntGELU    | n:  15
IntSoftmax | n:  22
IntGELU    | n:  15
IntSoftmax | n:  22
IntGELU    | n:  15
IntSoftmax | n:  22
IntGELU    | n:  15
IntSoftmax | n:  22
IntGELU    | n:  15
IntSoftmax | n:  22
IntGELU    | n:  15
IntSoftmax | n:  22
IntGELU    | n:  15
IntSoftmax | n:  22
IntGELU    | n:  15
IntSoftmax | n:  22
IntGELU    | n:  15
IntSoftmax | n:  22
IntGELU    | n:  15
IntSoftmax | n:  22
IntGELU    | n:  15
.calib done
Test: [  0/391]	Time  3.950 ( 3.950)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=22, intgelu_exp_n=16
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=22, intgelu_exp_n=16, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  22
IntGELU    | n:  16
IntSoftmax | n:  22
IntGELU    | n:  16
IntSoftmax | n:  22
IntGELU    | n:  16
IntSoftmax | n:  22
IntGELU    | n:  16
IntSoftmax | n:  22
IntGELU    | n:  16
IntSoftmax | n:  22
IntGELU    | n:  16
IntSoftmax | n:  22
IntGELU    | n:  16
IntSoftmax | n:  22
IntGELU    | n:  16
IntSoftmax | n:  22
IntGELU    | n:  16
IntSoftmax | n:  22
IntGELU    | n:  16
IntSoftmax | n:  22
IntGELU    | n:  16
IntSoftmax | n:  22
IntGELU    | n:  16
.calib done
Test: [  0/391]	Time  3.684 ( 3.684)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.72
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=22, intgelu_exp_n=17
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=22, intgelu_exp_n=17, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  22
IntGELU    | n:  17
IntSoftmax | n:  22
IntGELU    | n:  17
IntSoftmax | n:  22
IntGELU    | n:  17
IntSoftmax | n:  22
IntGELU    | n:  17
IntSoftmax | n:  22
IntGELU    | n:  17
IntSoftmax | n:  22
IntGELU    | n:  17
IntSoftmax | n:  22
IntGELU    | n:  17
IntSoftmax | n:  22
IntGELU    | n:  17
IntSoftmax | n:  22
IntGELU    | n:  17
IntSoftmax | n:  22
IntGELU    | n:  17
IntSoftmax | n:  22
IntGELU    | n:  17
IntSoftmax | n:  22
IntGELU    | n:  17
.calib done
Test: [  0/391]	Time  3.744 ( 3.744)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.75
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=22, intgelu_exp_n=18
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=22, intgelu_exp_n=18, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  22
IntGELU    | n:  18
IntSoftmax | n:  22
IntGELU    | n:  18
IntSoftmax | n:  22
IntGELU    | n:  18
IntSoftmax | n:  22
IntGELU    | n:  18
IntSoftmax | n:  22
IntGELU    | n:  18
IntSoftmax | n:  22
IntGELU    | n:  18
IntSoftmax | n:  22
IntGELU    | n:  18
IntSoftmax | n:  22
IntGELU    | n:  18
IntSoftmax | n:  22
IntGELU    | n:  18
IntSoftmax | n:  22
IntGELU    | n:  18
IntSoftmax | n:  22
IntGELU    | n:  18
IntSoftmax | n:  22
IntGELU    | n:  18
.calib done
Test: [  0/391]	Time  3.933 ( 3.933)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.00
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=22, intgelu_exp_n=19
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=22, intgelu_exp_n=19, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  22
IntGELU    | n:  19
IntSoftmax | n:  22
IntGELU    | n:  19
IntSoftmax | n:  22
IntGELU    | n:  19
IntSoftmax | n:  22
IntGELU    | n:  19
IntSoftmax | n:  22
IntGELU    | n:  19
IntSoftmax | n:  22
IntGELU    | n:  19
IntSoftmax | n:  22
IntGELU    | n:  19
IntSoftmax | n:  22
IntGELU    | n:  19
IntSoftmax | n:  22
IntGELU    | n:  19
IntSoftmax | n:  22
IntGELU    | n:  19
IntSoftmax | n:  22
IntGELU    | n:  19
IntSoftmax | n:  22
IntGELU    | n:  19
.calib done
Test: [  0/391]	Time  3.886 ( 3.886)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.91
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=22, intgelu_exp_n=20
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=22, intgelu_exp_n=20, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  22
IntGELU    | n:  20
IntSoftmax | n:  22
IntGELU    | n:  20
IntSoftmax | n:  22
IntGELU    | n:  20
IntSoftmax | n:  22
IntGELU    | n:  20
IntSoftmax | n:  22
IntGELU    | n:  20
IntSoftmax | n:  22
IntGELU    | n:  20
IntSoftmax | n:  22
IntGELU    | n:  20
IntSoftmax | n:  22
IntGELU    | n:  20
IntSoftmax | n:  22
IntGELU    | n:  20
IntSoftmax | n:  22
IntGELU    | n:  20
IntSoftmax | n:  22
IntGELU    | n:  20
IntSoftmax | n:  22
IntGELU    | n:  20
.calib done
Test: [  0/391]	Time  4.030 ( 4.030)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.08
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=22, intgelu_exp_n=21
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=22, intgelu_exp_n=21, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  22
IntGELU    | n:  21
IntSoftmax | n:  22
IntGELU    | n:  21
IntSoftmax | n:  22
IntGELU    | n:  21
IntSoftmax | n:  22
IntGELU    | n:  21
IntSoftmax | n:  22
IntGELU    | n:  21
IntSoftmax | n:  22
IntGELU    | n:  21
IntSoftmax | n:  22
IntGELU    | n:  21
IntSoftmax | n:  22
IntGELU    | n:  21
IntSoftmax | n:  22
IntGELU    | n:  21
IntSoftmax | n:  22
IntGELU    | n:  21
IntSoftmax | n:  22
IntGELU    | n:  21
IntSoftmax | n:  22
IntGELU    | n:  21
.calib done
Test: [  0/391]	Time  3.851 ( 3.851)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.93
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=22, intgelu_exp_n=22
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=22, intgelu_exp_n=22, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  22
IntGELU    | n:  22
IntSoftmax | n:  22
IntGELU    | n:  22
IntSoftmax | n:  22
IntGELU    | n:  22
IntSoftmax | n:  22
IntGELU    | n:  22
IntSoftmax | n:  22
IntGELU    | n:  22
IntSoftmax | n:  22
IntGELU    | n:  22
IntSoftmax | n:  22
IntGELU    | n:  22
IntSoftmax | n:  22
IntGELU    | n:  22
IntSoftmax | n:  22
IntGELU    | n:  22
IntSoftmax | n:  22
IntGELU    | n:  22
IntSoftmax | n:  22
IntGELU    | n:  22
IntSoftmax | n:  22
IntGELU    | n:  22
.calib done
Test: [  0/391]	Time  3.901 ( 3.901)	Acc@1   0.00 (  0.00)	Acc@5   0.78 (  0.78)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.781
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=22, intgelu_exp_n=23
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=22, intgelu_exp_n=23, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  22
IntGELU    | n:  23
IntSoftmax | n:  22
IntGELU    | n:  23
IntSoftmax | n:  22
IntGELU    | n:  23
IntSoftmax | n:  22
IntGELU    | n:  23
IntSoftmax | n:  22
IntGELU    | n:  23
IntSoftmax | n:  22
IntGELU    | n:  23
IntSoftmax | n:  22
IntGELU    | n:  23
IntSoftmax | n:  22
IntGELU    | n:  23
IntSoftmax | n:  22
IntGELU    | n:  23
IntSoftmax | n:  22
IntGELU    | n:  23
IntSoftmax | n:  22
IntGELU    | n:  23
IntSoftmax | n:  22
IntGELU    | n:  23
.calib done
Test: [  0/391]	Time  3.871 ( 3.871)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.92
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=22, intgelu_exp_n=24
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=22, intgelu_exp_n=24, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  22
IntGELU    | n:  24
IntSoftmax | n:  22
IntGELU    | n:  24
IntSoftmax | n:  22
IntGELU    | n:  24
IntSoftmax | n:  22
IntGELU    | n:  24
IntSoftmax | n:  22
IntGELU    | n:  24
IntSoftmax | n:  22
IntGELU    | n:  24
IntSoftmax | n:  22
IntGELU    | n:  24
IntSoftmax | n:  22
IntGELU    | n:  24
IntSoftmax | n:  22
IntGELU    | n:  24
IntSoftmax | n:  22
IntGELU    | n:  24
IntSoftmax | n:  22
IntGELU    | n:  24
IntSoftmax | n:  22
IntGELU    | n:  24
.calib done
Test: [  0/391]	Time  3.689 ( 3.689)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.80
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=22, intgelu_exp_n=25
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=22, intgelu_exp_n=25, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  22
IntGELU    | n:  25
IntSoftmax | n:  22
IntGELU    | n:  25
IntSoftmax | n:  22
IntGELU    | n:  25
IntSoftmax | n:  22
IntGELU    | n:  25
IntSoftmax | n:  22
IntGELU    | n:  25
IntSoftmax | n:  22
IntGELU    | n:  25
IntSoftmax | n:  22
IntGELU    | n:  25
IntSoftmax | n:  22
IntGELU    | n:  25
IntSoftmax | n:  22
IntGELU    | n:  25
IntSoftmax | n:  22
IntGELU    | n:  25
IntSoftmax | n:  22
IntGELU    | n:  25
IntSoftmax | n:  22
IntGELU    | n:  25
.calib done
Test: [  0/391]	Time  3.891 ( 3.891)	Acc@1   0.00 (  0.00)	Acc@5   0.78 (  0.78)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.781
Time: 8.90
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=22, intgelu_exp_n=26
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=22, intgelu_exp_n=26, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  22
IntGELU    | n:  26
IntSoftmax | n:  22
IntGELU    | n:  26
IntSoftmax | n:  22
IntGELU    | n:  26
IntSoftmax | n:  22
IntGELU    | n:  26
IntSoftmax | n:  22
IntGELU    | n:  26
IntSoftmax | n:  22
IntGELU    | n:  26
IntSoftmax | n:  22
IntGELU    | n:  26
IntSoftmax | n:  22
IntGELU    | n:  26
IntSoftmax | n:  22
IntGELU    | n:  26
IntSoftmax | n:  22
IntGELU    | n:  26
IntSoftmax | n:  22
IntGELU    | n:  26
IntSoftmax | n:  22
IntGELU    | n:  26
.calib done
Test: [  0/391]	Time  3.939 ( 3.939)	Acc@1   0.78 (  0.78)	Acc@5   1.56 (  1.56)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.781 Prec@5 1.562
Time: 8.99
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=22, intgelu_exp_n=27
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=22, intgelu_exp_n=27, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  22
IntGELU    | n:  27
IntSoftmax | n:  22
IntGELU    | n:  27
IntSoftmax | n:  22
IntGELU    | n:  27
IntSoftmax | n:  22
IntGELU    | n:  27
IntSoftmax | n:  22
IntGELU    | n:  27
IntSoftmax | n:  22
IntGELU    | n:  27
IntSoftmax | n:  22
IntGELU    | n:  27
IntSoftmax | n:  22
IntGELU    | n:  27
IntSoftmax | n:  22
IntGELU    | n:  27
IntSoftmax | n:  22
IntGELU    | n:  27
IntSoftmax | n:  22
IntGELU    | n:  27
IntSoftmax | n:  22
IntGELU    | n:  27
.calib done
Test: [  0/391]	Time  3.902 ( 3.902)	Acc@1   0.00 (  0.00)	Acc@5   0.78 (  0.78)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.781
Time: 8.93
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=22, intgelu_exp_n=28
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=22, intgelu_exp_n=28, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  22
IntGELU    | n:  28
IntSoftmax | n:  22
IntGELU    | n:  28
IntSoftmax | n:  22
IntGELU    | n:  28
IntSoftmax | n:  22
IntGELU    | n:  28
IntSoftmax | n:  22
IntGELU    | n:  28
IntSoftmax | n:  22
IntGELU    | n:  28
IntSoftmax | n:  22
IntGELU    | n:  28
IntSoftmax | n:  22
IntGELU    | n:  28
IntSoftmax | n:  22
IntGELU    | n:  28
IntSoftmax | n:  22
IntGELU    | n:  28
IntSoftmax | n:  22
IntGELU    | n:  28
IntSoftmax | n:  22
IntGELU    | n:  28
.calib done
Test: [  0/391]	Time  4.049 ( 4.049)	Acc@1   0.78 (  0.78)	Acc@5   0.78 (  0.78)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.781 Prec@5 0.781
Time: 9.09
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=22, intgelu_exp_n=29
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=22, intgelu_exp_n=29, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  22
IntGELU    | n:  29
IntSoftmax | n:  22
IntGELU    | n:  29
IntSoftmax | n:  22
IntGELU    | n:  29
IntSoftmax | n:  22
IntGELU    | n:  29
IntSoftmax | n:  22
IntGELU    | n:  29
IntSoftmax | n:  22
IntGELU    | n:  29
IntSoftmax | n:  22
IntGELU    | n:  29
IntSoftmax | n:  22
IntGELU    | n:  29
IntSoftmax | n:  22
IntGELU    | n:  29
IntSoftmax | n:  22
IntGELU    | n:  29
IntSoftmax | n:  22
IntGELU    | n:  29
IntSoftmax | n:  22
IntGELU    | n:  29
.calib done
Test: [  0/391]	Time  3.873 ( 3.873)	Acc@1   3.12 (  3.12)	Acc@5   8.59 (  8.59)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 3.125 Prec@5 8.594
Time: 8.88
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=22, intgelu_exp_n=30
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=22, intgelu_exp_n=30, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  22
IntGELU    | n:  30
IntSoftmax | n:  22
IntGELU    | n:  30
IntSoftmax | n:  22
IntGELU    | n:  30
IntSoftmax | n:  22
IntGELU    | n:  30
IntSoftmax | n:  22
IntGELU    | n:  30
IntSoftmax | n:  22
IntGELU    | n:  30
IntSoftmax | n:  22
IntGELU    | n:  30
IntSoftmax | n:  22
IntGELU    | n:  30
IntSoftmax | n:  22
IntGELU    | n:  30
IntSoftmax | n:  22
IntGELU    | n:  30
IntSoftmax | n:  22
IntGELU    | n:  30
IntSoftmax | n:  22
IntGELU    | n:  30
.calib done
Test: [  0/391]	Time  3.963 ( 3.963)	Acc@1   7.03 (  7.03)	Acc@5  18.75 ( 18.75)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 7.031 Prec@5 18.750
Time: 9.06
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=22, intgelu_exp_n=31
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=22, intgelu_exp_n=31, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  22
IntGELU    | n:  31
IntSoftmax | n:  22
IntGELU    | n:  31
IntSoftmax | n:  22
IntGELU    | n:  31
IntSoftmax | n:  22
IntGELU    | n:  31
IntSoftmax | n:  22
IntGELU    | n:  31
IntSoftmax | n:  22
IntGELU    | n:  31
IntSoftmax | n:  22
IntGELU    | n:  31
IntSoftmax | n:  22
IntGELU    | n:  31
IntSoftmax | n:  22
IntGELU    | n:  31
IntSoftmax | n:  22
IntGELU    | n:  31
IntSoftmax | n:  22
IntGELU    | n:  31
IntSoftmax | n:  22
IntGELU    | n:  31
.calib done
Test: [  0/391]	Time  3.907 ( 3.907)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.93
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=23, intgelu_exp_n=0
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=23, intgelu_exp_n=0, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  23
IntGELU    | n:  0
IntSoftmax | n:  23
IntGELU    | n:  0
IntSoftmax | n:  23
IntGELU    | n:  0
IntSoftmax | n:  23
IntGELU    | n:  0
IntSoftmax | n:  23
IntGELU    | n:  0
IntSoftmax | n:  23
IntGELU    | n:  0
IntSoftmax | n:  23
IntGELU    | n:  0
IntSoftmax | n:  23
IntGELU    | n:  0
IntSoftmax | n:  23
IntGELU    | n:  0
IntSoftmax | n:  23
IntGELU    | n:  0
IntSoftmax | n:  23
IntGELU    | n:  0
IntSoftmax | n:  23
IntGELU    | n:  0
.calib done
Test: [  0/391]	Time  3.900 ( 3.900)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.94
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=23, intgelu_exp_n=1
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=23, intgelu_exp_n=1, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  23
IntGELU    | n:  1
IntSoftmax | n:  23
IntGELU    | n:  1
IntSoftmax | n:  23
IntGELU    | n:  1
IntSoftmax | n:  23
IntGELU    | n:  1
IntSoftmax | n:  23
IntGELU    | n:  1
IntSoftmax | n:  23
IntGELU    | n:  1
IntSoftmax | n:  23
IntGELU    | n:  1
IntSoftmax | n:  23
IntGELU    | n:  1
IntSoftmax | n:  23
IntGELU    | n:  1
IntSoftmax | n:  23
IntGELU    | n:  1
IntSoftmax | n:  23
IntGELU    | n:  1
IntSoftmax | n:  23
IntGELU    | n:  1
.calib done
Test: [  0/391]	Time  3.904 ( 3.904)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.00
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=23, intgelu_exp_n=2
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=23, intgelu_exp_n=2, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  23
IntGELU    | n:  2
IntSoftmax | n:  23
IntGELU    | n:  2
IntSoftmax | n:  23
IntGELU    | n:  2
IntSoftmax | n:  23
IntGELU    | n:  2
IntSoftmax | n:  23
IntGELU    | n:  2
IntSoftmax | n:  23
IntGELU    | n:  2
IntSoftmax | n:  23
IntGELU    | n:  2
IntSoftmax | n:  23
IntGELU    | n:  2
IntSoftmax | n:  23
IntGELU    | n:  2
IntSoftmax | n:  23
IntGELU    | n:  2
IntSoftmax | n:  23
IntGELU    | n:  2
IntSoftmax | n:  23
IntGELU    | n:  2
.calib done
Test: [  0/391]	Time  3.918 ( 3.918)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.99
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=23, intgelu_exp_n=3
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=23, intgelu_exp_n=3, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  23
IntGELU    | n:  3
IntSoftmax | n:  23
IntGELU    | n:  3
IntSoftmax | n:  23
IntGELU    | n:  3
IntSoftmax | n:  23
IntGELU    | n:  3
IntSoftmax | n:  23
IntGELU    | n:  3
IntSoftmax | n:  23
IntGELU    | n:  3
IntSoftmax | n:  23
IntGELU    | n:  3
IntSoftmax | n:  23
IntGELU    | n:  3
IntSoftmax | n:  23
IntGELU    | n:  3
IntSoftmax | n:  23
IntGELU    | n:  3
IntSoftmax | n:  23
IntGELU    | n:  3
IntSoftmax | n:  23
IntGELU    | n:  3
.calib done
Test: [  0/391]	Time  3.786 ( 3.786)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.83
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=23, intgelu_exp_n=4
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=23, intgelu_exp_n=4, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  23
IntGELU    | n:  4
IntSoftmax | n:  23
IntGELU    | n:  4
IntSoftmax | n:  23
IntGELU    | n:  4
IntSoftmax | n:  23
IntGELU    | n:  4
IntSoftmax | n:  23
IntGELU    | n:  4
IntSoftmax | n:  23
IntGELU    | n:  4
IntSoftmax | n:  23
IntGELU    | n:  4
IntSoftmax | n:  23
IntGELU    | n:  4
IntSoftmax | n:  23
IntGELU    | n:  4
IntSoftmax | n:  23
IntGELU    | n:  4
IntSoftmax | n:  23
IntGELU    | n:  4
IntSoftmax | n:  23
IntGELU    | n:  4
.calib done
Test: [  0/391]	Time  3.845 ( 3.845)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.87
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=23, intgelu_exp_n=5
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=23, intgelu_exp_n=5, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  23
IntGELU    | n:  5
IntSoftmax | n:  23
IntGELU    | n:  5
IntSoftmax | n:  23
IntGELU    | n:  5
IntSoftmax | n:  23
IntGELU    | n:  5
IntSoftmax | n:  23
IntGELU    | n:  5
IntSoftmax | n:  23
IntGELU    | n:  5
IntSoftmax | n:  23
IntGELU    | n:  5
IntSoftmax | n:  23
IntGELU    | n:  5
IntSoftmax | n:  23
IntGELU    | n:  5
IntSoftmax | n:  23
IntGELU    | n:  5
IntSoftmax | n:  23
IntGELU    | n:  5
IntSoftmax | n:  23
IntGELU    | n:  5
.calib done
Test: [  0/391]	Time  3.785 ( 3.785)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.86
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=23, intgelu_exp_n=6
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=23, intgelu_exp_n=6, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  23
IntGELU    | n:  6
IntSoftmax | n:  23
IntGELU    | n:  6
IntSoftmax | n:  23
IntGELU    | n:  6
IntSoftmax | n:  23
IntGELU    | n:  6
IntSoftmax | n:  23
IntGELU    | n:  6
IntSoftmax | n:  23
IntGELU    | n:  6
IntSoftmax | n:  23
IntGELU    | n:  6
IntSoftmax | n:  23
IntGELU    | n:  6
IntSoftmax | n:  23
IntGELU    | n:  6
IntSoftmax | n:  23
IntGELU    | n:  6
IntSoftmax | n:  23
IntGELU    | n:  6
IntSoftmax | n:  23
IntGELU    | n:  6
.calib done
Test: [  0/391]	Time  3.806 ( 3.806)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.85
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=23, intgelu_exp_n=7
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=23, intgelu_exp_n=7, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  23
IntGELU    | n:  7
IntSoftmax | n:  23
IntGELU    | n:  7
IntSoftmax | n:  23
IntGELU    | n:  7
IntSoftmax | n:  23
IntGELU    | n:  7
IntSoftmax | n:  23
IntGELU    | n:  7
IntSoftmax | n:  23
IntGELU    | n:  7
IntSoftmax | n:  23
IntGELU    | n:  7
IntSoftmax | n:  23
IntGELU    | n:  7
IntSoftmax | n:  23
IntGELU    | n:  7
IntSoftmax | n:  23
IntGELU    | n:  7
IntSoftmax | n:  23
IntGELU    | n:  7
IntSoftmax | n:  23
IntGELU    | n:  7
.calib done
Test: [  0/391]	Time  3.675 ( 3.675)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.76
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=23, intgelu_exp_n=8
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=23, intgelu_exp_n=8, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  23
IntGELU    | n:  8
IntSoftmax | n:  23
IntGELU    | n:  8
IntSoftmax | n:  23
IntGELU    | n:  8
IntSoftmax | n:  23
IntGELU    | n:  8
IntSoftmax | n:  23
IntGELU    | n:  8
IntSoftmax | n:  23
IntGELU    | n:  8
IntSoftmax | n:  23
IntGELU    | n:  8
IntSoftmax | n:  23
IntGELU    | n:  8
IntSoftmax | n:  23
IntGELU    | n:  8
IntSoftmax | n:  23
IntGELU    | n:  8
IntSoftmax | n:  23
IntGELU    | n:  8
IntSoftmax | n:  23
IntGELU    | n:  8
.calib done
Test: [  0/391]	Time  3.848 ( 3.848)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.87
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=23, intgelu_exp_n=9
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=23, intgelu_exp_n=9, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  23
IntGELU    | n:  9
IntSoftmax | n:  23
IntGELU    | n:  9
IntSoftmax | n:  23
IntGELU    | n:  9
IntSoftmax | n:  23
IntGELU    | n:  9
IntSoftmax | n:  23
IntGELU    | n:  9
IntSoftmax | n:  23
IntGELU    | n:  9
IntSoftmax | n:  23
IntGELU    | n:  9
IntSoftmax | n:  23
IntGELU    | n:  9
IntSoftmax | n:  23
IntGELU    | n:  9
IntSoftmax | n:  23
IntGELU    | n:  9
IntSoftmax | n:  23
IntGELU    | n:  9
IntSoftmax | n:  23
IntGELU    | n:  9
.calib done
Test: [  0/391]	Time  3.907 ( 3.907)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=23, intgelu_exp_n=10
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=23, intgelu_exp_n=10, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  23
IntGELU    | n:  10
IntSoftmax | n:  23
IntGELU    | n:  10
IntSoftmax | n:  23
IntGELU    | n:  10
IntSoftmax | n:  23
IntGELU    | n:  10
IntSoftmax | n:  23
IntGELU    | n:  10
IntSoftmax | n:  23
IntGELU    | n:  10
IntSoftmax | n:  23
IntGELU    | n:  10
IntSoftmax | n:  23
IntGELU    | n:  10
IntSoftmax | n:  23
IntGELU    | n:  10
IntSoftmax | n:  23
IntGELU    | n:  10
IntSoftmax | n:  23
IntGELU    | n:  10
IntSoftmax | n:  23
IntGELU    | n:  10
.calib done
Test: [  0/391]	Time  3.719 ( 3.719)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.71
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=23, intgelu_exp_n=11
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=23, intgelu_exp_n=11, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  23
IntGELU    | n:  11
IntSoftmax | n:  23
IntGELU    | n:  11
IntSoftmax | n:  23
IntGELU    | n:  11
IntSoftmax | n:  23
IntGELU    | n:  11
IntSoftmax | n:  23
IntGELU    | n:  11
IntSoftmax | n:  23
IntGELU    | n:  11
IntSoftmax | n:  23
IntGELU    | n:  11
IntSoftmax | n:  23
IntGELU    | n:  11
IntSoftmax | n:  23
IntGELU    | n:  11
IntSoftmax | n:  23
IntGELU    | n:  11
IntSoftmax | n:  23
IntGELU    | n:  11
IntSoftmax | n:  23
IntGELU    | n:  11
.calib done
Test: [  0/391]	Time  4.028 ( 4.028)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.12
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=23, intgelu_exp_n=12
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=23, intgelu_exp_n=12, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  23
IntGELU    | n:  12
IntSoftmax | n:  23
IntGELU    | n:  12
IntSoftmax | n:  23
IntGELU    | n:  12
IntSoftmax | n:  23
IntGELU    | n:  12
IntSoftmax | n:  23
IntGELU    | n:  12
IntSoftmax | n:  23
IntGELU    | n:  12
IntSoftmax | n:  23
IntGELU    | n:  12
IntSoftmax | n:  23
IntGELU    | n:  12
IntSoftmax | n:  23
IntGELU    | n:  12
IntSoftmax | n:  23
IntGELU    | n:  12
IntSoftmax | n:  23
IntGELU    | n:  12
IntSoftmax | n:  23
IntGELU    | n:  12
.calib done
Test: [  0/391]	Time  3.804 ( 3.804)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.87
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=23, intgelu_exp_n=13
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=23, intgelu_exp_n=13, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  23
IntGELU    | n:  13
IntSoftmax | n:  23
IntGELU    | n:  13
IntSoftmax | n:  23
IntGELU    | n:  13
IntSoftmax | n:  23
IntGELU    | n:  13
IntSoftmax | n:  23
IntGELU    | n:  13
IntSoftmax | n:  23
IntGELU    | n:  13
IntSoftmax | n:  23
IntGELU    | n:  13
IntSoftmax | n:  23
IntGELU    | n:  13
IntSoftmax | n:  23
IntGELU    | n:  13
IntSoftmax | n:  23
IntGELU    | n:  13
IntSoftmax | n:  23
IntGELU    | n:  13
IntSoftmax | n:  23
IntGELU    | n:  13
.calib done
Test: [  0/391]	Time  3.657 ( 3.657)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.65
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=23, intgelu_exp_n=14
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=23, intgelu_exp_n=14, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  23
IntGELU    | n:  14
IntSoftmax | n:  23
IntGELU    | n:  14
IntSoftmax | n:  23
IntGELU    | n:  14
IntSoftmax | n:  23
IntGELU    | n:  14
IntSoftmax | n:  23
IntGELU    | n:  14
IntSoftmax | n:  23
IntGELU    | n:  14
IntSoftmax | n:  23
IntGELU    | n:  14
IntSoftmax | n:  23
IntGELU    | n:  14
IntSoftmax | n:  23
IntGELU    | n:  14
IntSoftmax | n:  23
IntGELU    | n:  14
IntSoftmax | n:  23
IntGELU    | n:  14
IntSoftmax | n:  23
IntGELU    | n:  14
.calib done
Test: [  0/391]	Time  3.806 ( 3.806)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.84
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=23, intgelu_exp_n=15
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=23, intgelu_exp_n=15, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  23
IntGELU    | n:  15
IntSoftmax | n:  23
IntGELU    | n:  15
IntSoftmax | n:  23
IntGELU    | n:  15
IntSoftmax | n:  23
IntGELU    | n:  15
IntSoftmax | n:  23
IntGELU    | n:  15
IntSoftmax | n:  23
IntGELU    | n:  15
IntSoftmax | n:  23
IntGELU    | n:  15
IntSoftmax | n:  23
IntGELU    | n:  15
IntSoftmax | n:  23
IntGELU    | n:  15
IntSoftmax | n:  23
IntGELU    | n:  15
IntSoftmax | n:  23
IntGELU    | n:  15
IntSoftmax | n:  23
IntGELU    | n:  15
.calib done
Test: [  0/391]	Time  3.727 ( 3.727)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.74
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=23, intgelu_exp_n=16
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=23, intgelu_exp_n=16, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  23
IntGELU    | n:  16
IntSoftmax | n:  23
IntGELU    | n:  16
IntSoftmax | n:  23
IntGELU    | n:  16
IntSoftmax | n:  23
IntGELU    | n:  16
IntSoftmax | n:  23
IntGELU    | n:  16
IntSoftmax | n:  23
IntGELU    | n:  16
IntSoftmax | n:  23
IntGELU    | n:  16
IntSoftmax | n:  23
IntGELU    | n:  16
IntSoftmax | n:  23
IntGELU    | n:  16
IntSoftmax | n:  23
IntGELU    | n:  16
IntSoftmax | n:  23
IntGELU    | n:  16
IntSoftmax | n:  23
IntGELU    | n:  16
.calib done
Test: [  0/391]	Time  3.870 ( 3.870)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.90
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=23, intgelu_exp_n=17
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=23, intgelu_exp_n=17, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  23
IntGELU    | n:  17
IntSoftmax | n:  23
IntGELU    | n:  17
IntSoftmax | n:  23
IntGELU    | n:  17
IntSoftmax | n:  23
IntGELU    | n:  17
IntSoftmax | n:  23
IntGELU    | n:  17
IntSoftmax | n:  23
IntGELU    | n:  17
IntSoftmax | n:  23
IntGELU    | n:  17
IntSoftmax | n:  23
IntGELU    | n:  17
IntSoftmax | n:  23
IntGELU    | n:  17
IntSoftmax | n:  23
IntGELU    | n:  17
IntSoftmax | n:  23
IntGELU    | n:  17
IntSoftmax | n:  23
IntGELU    | n:  17
.calib done
Test: [  0/391]	Time  4.024 ( 4.024)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.08
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=23, intgelu_exp_n=18
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=23, intgelu_exp_n=18, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  23
IntGELU    | n:  18
IntSoftmax | n:  23
IntGELU    | n:  18
IntSoftmax | n:  23
IntGELU    | n:  18
IntSoftmax | n:  23
IntGELU    | n:  18
IntSoftmax | n:  23
IntGELU    | n:  18
IntSoftmax | n:  23
IntGELU    | n:  18
IntSoftmax | n:  23
IntGELU    | n:  18
IntSoftmax | n:  23
IntGELU    | n:  18
IntSoftmax | n:  23
IntGELU    | n:  18
IntSoftmax | n:  23
IntGELU    | n:  18
IntSoftmax | n:  23
IntGELU    | n:  18
IntSoftmax | n:  23
IntGELU    | n:  18
.calib done
Test: [  0/391]	Time  3.914 ( 3.914)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.91
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=23, intgelu_exp_n=19
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=23, intgelu_exp_n=19, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  23
IntGELU    | n:  19
IntSoftmax | n:  23
IntGELU    | n:  19
IntSoftmax | n:  23
IntGELU    | n:  19
IntSoftmax | n:  23
IntGELU    | n:  19
IntSoftmax | n:  23
IntGELU    | n:  19
IntSoftmax | n:  23
IntGELU    | n:  19
IntSoftmax | n:  23
IntGELU    | n:  19
IntSoftmax | n:  23
IntGELU    | n:  19
IntSoftmax | n:  23
IntGELU    | n:  19
IntSoftmax | n:  23
IntGELU    | n:  19
IntSoftmax | n:  23
IntGELU    | n:  19
IntSoftmax | n:  23
IntGELU    | n:  19
.calib done
Test: [  0/391]	Time  3.933 ( 3.933)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=23, intgelu_exp_n=20
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=23, intgelu_exp_n=20, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  23
IntGELU    | n:  20
IntSoftmax | n:  23
IntGELU    | n:  20
IntSoftmax | n:  23
IntGELU    | n:  20
IntSoftmax | n:  23
IntGELU    | n:  20
IntSoftmax | n:  23
IntGELU    | n:  20
IntSoftmax | n:  23
IntGELU    | n:  20
IntSoftmax | n:  23
IntGELU    | n:  20
IntSoftmax | n:  23
IntGELU    | n:  20
IntSoftmax | n:  23
IntGELU    | n:  20
IntSoftmax | n:  23
IntGELU    | n:  20
IntSoftmax | n:  23
IntGELU    | n:  20
IntSoftmax | n:  23
IntGELU    | n:  20
.calib done
Test: [  0/391]	Time  3.922 ( 3.922)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=23, intgelu_exp_n=21
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=23, intgelu_exp_n=21, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  23
IntGELU    | n:  21
IntSoftmax | n:  23
IntGELU    | n:  21
IntSoftmax | n:  23
IntGELU    | n:  21
IntSoftmax | n:  23
IntGELU    | n:  21
IntSoftmax | n:  23
IntGELU    | n:  21
IntSoftmax | n:  23
IntGELU    | n:  21
IntSoftmax | n:  23
IntGELU    | n:  21
IntSoftmax | n:  23
IntGELU    | n:  21
IntSoftmax | n:  23
IntGELU    | n:  21
IntSoftmax | n:  23
IntGELU    | n:  21
IntSoftmax | n:  23
IntGELU    | n:  21
IntSoftmax | n:  23
IntGELU    | n:  21
.calib done
Test: [  0/391]	Time  3.967 ( 3.967)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.03
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=23, intgelu_exp_n=22
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=23, intgelu_exp_n=22, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  23
IntGELU    | n:  22
IntSoftmax | n:  23
IntGELU    | n:  22
IntSoftmax | n:  23
IntGELU    | n:  22
IntSoftmax | n:  23
IntGELU    | n:  22
IntSoftmax | n:  23
IntGELU    | n:  22
IntSoftmax | n:  23
IntGELU    | n:  22
IntSoftmax | n:  23
IntGELU    | n:  22
IntSoftmax | n:  23
IntGELU    | n:  22
IntSoftmax | n:  23
IntGELU    | n:  22
IntSoftmax | n:  23
IntGELU    | n:  22
IntSoftmax | n:  23
IntGELU    | n:  22
IntSoftmax | n:  23
IntGELU    | n:  22
.calib done
Test: [  0/391]	Time  3.674 ( 3.674)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.70
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=23, intgelu_exp_n=23
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=23, intgelu_exp_n=23, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  23
IntGELU    | n:  23
IntSoftmax | n:  23
IntGELU    | n:  23
IntSoftmax | n:  23
IntGELU    | n:  23
IntSoftmax | n:  23
IntGELU    | n:  23
IntSoftmax | n:  23
IntGELU    | n:  23
IntSoftmax | n:  23
IntGELU    | n:  23
IntSoftmax | n:  23
IntGELU    | n:  23
IntSoftmax | n:  23
IntGELU    | n:  23
IntSoftmax | n:  23
IntGELU    | n:  23
IntSoftmax | n:  23
IntGELU    | n:  23
IntSoftmax | n:  23
IntGELU    | n:  23
IntSoftmax | n:  23
IntGELU    | n:  23
.calib done
Test: [  0/391]	Time  3.912 ( 3.912)	Acc@1   0.00 (  0.00)	Acc@5   0.78 (  0.78)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.781
Time: 9.01
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=23, intgelu_exp_n=24
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=23, intgelu_exp_n=24, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  23
IntGELU    | n:  24
IntSoftmax | n:  23
IntGELU    | n:  24
IntSoftmax | n:  23
IntGELU    | n:  24
IntSoftmax | n:  23
IntGELU    | n:  24
IntSoftmax | n:  23
IntGELU    | n:  24
IntSoftmax | n:  23
IntGELU    | n:  24
IntSoftmax | n:  23
IntGELU    | n:  24
IntSoftmax | n:  23
IntGELU    | n:  24
IntSoftmax | n:  23
IntGELU    | n:  24
IntSoftmax | n:  23
IntGELU    | n:  24
IntSoftmax | n:  23
IntGELU    | n:  24
IntSoftmax | n:  23
IntGELU    | n:  24
.calib done
Test: [  0/391]	Time  3.951 ( 3.951)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=23, intgelu_exp_n=25
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=23, intgelu_exp_n=25, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  23
IntGELU    | n:  25
IntSoftmax | n:  23
IntGELU    | n:  25
IntSoftmax | n:  23
IntGELU    | n:  25
IntSoftmax | n:  23
IntGELU    | n:  25
IntSoftmax | n:  23
IntGELU    | n:  25
IntSoftmax | n:  23
IntGELU    | n:  25
IntSoftmax | n:  23
IntGELU    | n:  25
IntSoftmax | n:  23
IntGELU    | n:  25
IntSoftmax | n:  23
IntGELU    | n:  25
IntSoftmax | n:  23
IntGELU    | n:  25
IntSoftmax | n:  23
IntGELU    | n:  25
IntSoftmax | n:  23
IntGELU    | n:  25
.calib done
Test: [  0/391]	Time  4.290 ( 4.290)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.79
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=23, intgelu_exp_n=26
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=23, intgelu_exp_n=26, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  23
IntGELU    | n:  26
IntSoftmax | n:  23
IntGELU    | n:  26
IntSoftmax | n:  23
IntGELU    | n:  26
IntSoftmax | n:  23
IntGELU    | n:  26
IntSoftmax | n:  23
IntGELU    | n:  26
IntSoftmax | n:  23
IntGELU    | n:  26
IntSoftmax | n:  23
IntGELU    | n:  26
IntSoftmax | n:  23
IntGELU    | n:  26
IntSoftmax | n:  23
IntGELU    | n:  26
IntSoftmax | n:  23
IntGELU    | n:  26
IntSoftmax | n:  23
IntGELU    | n:  26
IntSoftmax | n:  23
IntGELU    | n:  26
.calib done
Test: [  0/391]	Time  3.936 ( 3.936)	Acc@1   0.78 (  0.78)	Acc@5   3.91 (  3.91)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.781 Prec@5 3.906
Time: 9.78
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=23, intgelu_exp_n=27
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=23, intgelu_exp_n=27, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  23
IntGELU    | n:  27
IntSoftmax | n:  23
IntGELU    | n:  27
IntSoftmax | n:  23
IntGELU    | n:  27
IntSoftmax | n:  23
IntGELU    | n:  27
IntSoftmax | n:  23
IntGELU    | n:  27
IntSoftmax | n:  23
IntGELU    | n:  27
IntSoftmax | n:  23
IntGELU    | n:  27
IntSoftmax | n:  23
IntGELU    | n:  27
IntSoftmax | n:  23
IntGELU    | n:  27
IntSoftmax | n:  23
IntGELU    | n:  27
IntSoftmax | n:  23
IntGELU    | n:  27
IntSoftmax | n:  23
IntGELU    | n:  27
.calib done
Test: [  0/391]	Time  3.962 ( 3.962)	Acc@1   0.00 (  0.00)	Acc@5   3.12 (  3.12)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 3.125
Time: 9.07
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=23, intgelu_exp_n=28
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=23, intgelu_exp_n=28, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  23
IntGELU    | n:  28
IntSoftmax | n:  23
IntGELU    | n:  28
IntSoftmax | n:  23
IntGELU    | n:  28
IntSoftmax | n:  23
IntGELU    | n:  28
IntSoftmax | n:  23
IntGELU    | n:  28
IntSoftmax | n:  23
IntGELU    | n:  28
IntSoftmax | n:  23
IntGELU    | n:  28
IntSoftmax | n:  23
IntGELU    | n:  28
IntSoftmax | n:  23
IntGELU    | n:  28
IntSoftmax | n:  23
IntGELU    | n:  28
IntSoftmax | n:  23
IntGELU    | n:  28
IntSoftmax | n:  23
IntGELU    | n:  28
.calib done
Test: [  0/391]	Time  4.006 ( 4.006)	Acc@1   0.00 (  0.00)	Acc@5   0.78 (  0.78)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.781
Time: 9.08
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=23, intgelu_exp_n=29
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=23, intgelu_exp_n=29, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  23
IntGELU    | n:  29
IntSoftmax | n:  23
IntGELU    | n:  29
IntSoftmax | n:  23
IntGELU    | n:  29
IntSoftmax | n:  23
IntGELU    | n:  29
IntSoftmax | n:  23
IntGELU    | n:  29
IntSoftmax | n:  23
IntGELU    | n:  29
IntSoftmax | n:  23
IntGELU    | n:  29
IntSoftmax | n:  23
IntGELU    | n:  29
IntSoftmax | n:  23
IntGELU    | n:  29
IntSoftmax | n:  23
IntGELU    | n:  29
IntSoftmax | n:  23
IntGELU    | n:  29
IntSoftmax | n:  23
IntGELU    | n:  29
.calib done
Test: [  0/391]	Time  3.664 ( 3.664)	Acc@1   3.91 (  3.91)	Acc@5   7.81 (  7.81)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 3.906 Prec@5 7.812
Time: 8.74
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=23, intgelu_exp_n=30
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=23, intgelu_exp_n=30, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  23
IntGELU    | n:  30
IntSoftmax | n:  23
IntGELU    | n:  30
IntSoftmax | n:  23
IntGELU    | n:  30
IntSoftmax | n:  23
IntGELU    | n:  30
IntSoftmax | n:  23
IntGELU    | n:  30
IntSoftmax | n:  23
IntGELU    | n:  30
IntSoftmax | n:  23
IntGELU    | n:  30
IntSoftmax | n:  23
IntGELU    | n:  30
IntSoftmax | n:  23
IntGELU    | n:  30
IntSoftmax | n:  23
IntGELU    | n:  30
IntSoftmax | n:  23
IntGELU    | n:  30
IntSoftmax | n:  23
IntGELU    | n:  30
.calib done
Test: [  0/391]	Time  3.964 ( 3.964)	Acc@1   1.56 (  1.56)	Acc@5   3.91 (  3.91)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 1.562 Prec@5 3.906
Time: 9.06
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=23, intgelu_exp_n=31
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=23, intgelu_exp_n=31, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  23
IntGELU    | n:  31
IntSoftmax | n:  23
IntGELU    | n:  31
IntSoftmax | n:  23
IntGELU    | n:  31
IntSoftmax | n:  23
IntGELU    | n:  31
IntSoftmax | n:  23
IntGELU    | n:  31
IntSoftmax | n:  23
IntGELU    | n:  31
IntSoftmax | n:  23
IntGELU    | n:  31
IntSoftmax | n:  23
IntGELU    | n:  31
IntSoftmax | n:  23
IntGELU    | n:  31
IntSoftmax | n:  23
IntGELU    | n:  31
IntSoftmax | n:  23
IntGELU    | n:  31
IntSoftmax | n:  23
IntGELU    | n:  31
.calib done
Test: [  0/391]	Time  3.796 ( 3.796)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.86
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=24, intgelu_exp_n=0
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=24, intgelu_exp_n=0, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  24
IntGELU    | n:  0
IntSoftmax | n:  24
IntGELU    | n:  0
IntSoftmax | n:  24
IntGELU    | n:  0
IntSoftmax | n:  24
IntGELU    | n:  0
IntSoftmax | n:  24
IntGELU    | n:  0
IntSoftmax | n:  24
IntGELU    | n:  0
IntSoftmax | n:  24
IntGELU    | n:  0
IntSoftmax | n:  24
IntGELU    | n:  0
IntSoftmax | n:  24
IntGELU    | n:  0
IntSoftmax | n:  24
IntGELU    | n:  0
IntSoftmax | n:  24
IntGELU    | n:  0
IntSoftmax | n:  24
IntGELU    | n:  0
.calib done
Test: [  0/391]	Time  3.901 ( 3.901)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.05
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=24, intgelu_exp_n=1
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=24, intgelu_exp_n=1, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  24
IntGELU    | n:  1
IntSoftmax | n:  24
IntGELU    | n:  1
IntSoftmax | n:  24
IntGELU    | n:  1
IntSoftmax | n:  24
IntGELU    | n:  1
IntSoftmax | n:  24
IntGELU    | n:  1
IntSoftmax | n:  24
IntGELU    | n:  1
IntSoftmax | n:  24
IntGELU    | n:  1
IntSoftmax | n:  24
IntGELU    | n:  1
IntSoftmax | n:  24
IntGELU    | n:  1
IntSoftmax | n:  24
IntGELU    | n:  1
IntSoftmax | n:  24
IntGELU    | n:  1
IntSoftmax | n:  24
IntGELU    | n:  1
.calib done
Test: [  0/391]	Time  3.915 ( 3.915)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.95
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=24, intgelu_exp_n=2
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=24, intgelu_exp_n=2, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  24
IntGELU    | n:  2
IntSoftmax | n:  24
IntGELU    | n:  2
IntSoftmax | n:  24
IntGELU    | n:  2
IntSoftmax | n:  24
IntGELU    | n:  2
IntSoftmax | n:  24
IntGELU    | n:  2
IntSoftmax | n:  24
IntGELU    | n:  2
IntSoftmax | n:  24
IntGELU    | n:  2
IntSoftmax | n:  24
IntGELU    | n:  2
IntSoftmax | n:  24
IntGELU    | n:  2
IntSoftmax | n:  24
IntGELU    | n:  2
IntSoftmax | n:  24
IntGELU    | n:  2
IntSoftmax | n:  24
IntGELU    | n:  2
.calib done
Test: [  0/391]	Time  3.696 ( 3.696)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.78
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=24, intgelu_exp_n=3
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=24, intgelu_exp_n=3, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  24
IntGELU    | n:  3
IntSoftmax | n:  24
IntGELU    | n:  3
IntSoftmax | n:  24
IntGELU    | n:  3
IntSoftmax | n:  24
IntGELU    | n:  3
IntSoftmax | n:  24
IntGELU    | n:  3
IntSoftmax | n:  24
IntGELU    | n:  3
IntSoftmax | n:  24
IntGELU    | n:  3
IntSoftmax | n:  24
IntGELU    | n:  3
IntSoftmax | n:  24
IntGELU    | n:  3
IntSoftmax | n:  24
IntGELU    | n:  3
IntSoftmax | n:  24
IntGELU    | n:  3
IntSoftmax | n:  24
IntGELU    | n:  3
.calib done
Test: [  0/391]	Time  4.051 ( 4.051)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.15
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=24, intgelu_exp_n=4
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=24, intgelu_exp_n=4, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  24
IntGELU    | n:  4
IntSoftmax | n:  24
IntGELU    | n:  4
IntSoftmax | n:  24
IntGELU    | n:  4
IntSoftmax | n:  24
IntGELU    | n:  4
IntSoftmax | n:  24
IntGELU    | n:  4
IntSoftmax | n:  24
IntGELU    | n:  4
IntSoftmax | n:  24
IntGELU    | n:  4
IntSoftmax | n:  24
IntGELU    | n:  4
IntSoftmax | n:  24
IntGELU    | n:  4
IntSoftmax | n:  24
IntGELU    | n:  4
IntSoftmax | n:  24
IntGELU    | n:  4
IntSoftmax | n:  24
IntGELU    | n:  4
.calib done
Test: [  0/391]	Time  3.991 ( 3.991)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.08
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=24, intgelu_exp_n=5
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=24, intgelu_exp_n=5, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  24
IntGELU    | n:  5
IntSoftmax | n:  24
IntGELU    | n:  5
IntSoftmax | n:  24
IntGELU    | n:  5
IntSoftmax | n:  24
IntGELU    | n:  5
IntSoftmax | n:  24
IntGELU    | n:  5
IntSoftmax | n:  24
IntGELU    | n:  5
IntSoftmax | n:  24
IntGELU    | n:  5
IntSoftmax | n:  24
IntGELU    | n:  5
IntSoftmax | n:  24
IntGELU    | n:  5
IntSoftmax | n:  24
IntGELU    | n:  5
IntSoftmax | n:  24
IntGELU    | n:  5
IntSoftmax | n:  24
IntGELU    | n:  5
.calib done
Test: [  0/391]	Time  3.954 ( 3.954)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.06
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=24, intgelu_exp_n=6
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=24, intgelu_exp_n=6, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  24
IntGELU    | n:  6
IntSoftmax | n:  24
IntGELU    | n:  6
IntSoftmax | n:  24
IntGELU    | n:  6
IntSoftmax | n:  24
IntGELU    | n:  6
IntSoftmax | n:  24
IntGELU    | n:  6
IntSoftmax | n:  24
IntGELU    | n:  6
IntSoftmax | n:  24
IntGELU    | n:  6
IntSoftmax | n:  24
IntGELU    | n:  6
IntSoftmax | n:  24
IntGELU    | n:  6
IntSoftmax | n:  24
IntGELU    | n:  6
IntSoftmax | n:  24
IntGELU    | n:  6
IntSoftmax | n:  24
IntGELU    | n:  6
.calib done
Test: [  0/391]	Time  4.006 ( 4.006)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.11
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=24, intgelu_exp_n=7
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=24, intgelu_exp_n=7, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  24
IntGELU    | n:  7
IntSoftmax | n:  24
IntGELU    | n:  7
IntSoftmax | n:  24
IntGELU    | n:  7
IntSoftmax | n:  24
IntGELU    | n:  7
IntSoftmax | n:  24
IntGELU    | n:  7
IntSoftmax | n:  24
IntGELU    | n:  7
IntSoftmax | n:  24
IntGELU    | n:  7
IntSoftmax | n:  24
IntGELU    | n:  7
IntSoftmax | n:  24
IntGELU    | n:  7
IntSoftmax | n:  24
IntGELU    | n:  7
IntSoftmax | n:  24
IntGELU    | n:  7
IntSoftmax | n:  24
IntGELU    | n:  7
.calib done
Test: [  0/391]	Time  3.951 ( 3.951)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.05
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=24, intgelu_exp_n=8
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=24, intgelu_exp_n=8, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  24
IntGELU    | n:  8
IntSoftmax | n:  24
IntGELU    | n:  8
IntSoftmax | n:  24
IntGELU    | n:  8
IntSoftmax | n:  24
IntGELU    | n:  8
IntSoftmax | n:  24
IntGELU    | n:  8
IntSoftmax | n:  24
IntGELU    | n:  8
IntSoftmax | n:  24
IntGELU    | n:  8
IntSoftmax | n:  24
IntGELU    | n:  8
IntSoftmax | n:  24
IntGELU    | n:  8
IntSoftmax | n:  24
IntGELU    | n:  8
IntSoftmax | n:  24
IntGELU    | n:  8
IntSoftmax | n:  24
IntGELU    | n:  8
.calib done
Test: [  0/391]	Time  3.703 ( 3.703)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.75
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=24, intgelu_exp_n=9
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=24, intgelu_exp_n=9, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  24
IntGELU    | n:  9
IntSoftmax | n:  24
IntGELU    | n:  9
IntSoftmax | n:  24
IntGELU    | n:  9
IntSoftmax | n:  24
IntGELU    | n:  9
IntSoftmax | n:  24
IntGELU    | n:  9
IntSoftmax | n:  24
IntGELU    | n:  9
IntSoftmax | n:  24
IntGELU    | n:  9
IntSoftmax | n:  24
IntGELU    | n:  9
IntSoftmax | n:  24
IntGELU    | n:  9
IntSoftmax | n:  24
IntGELU    | n:  9
IntSoftmax | n:  24
IntGELU    | n:  9
IntSoftmax | n:  24
IntGELU    | n:  9
.calib done
Test: [  0/391]	Time  4.003 ( 4.003)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.09
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=24, intgelu_exp_n=10
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=24, intgelu_exp_n=10, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  24
IntGELU    | n:  10
IntSoftmax | n:  24
IntGELU    | n:  10
IntSoftmax | n:  24
IntGELU    | n:  10
IntSoftmax | n:  24
IntGELU    | n:  10
IntSoftmax | n:  24
IntGELU    | n:  10
IntSoftmax | n:  24
IntGELU    | n:  10
IntSoftmax | n:  24
IntGELU    | n:  10
IntSoftmax | n:  24
IntGELU    | n:  10
IntSoftmax | n:  24
IntGELU    | n:  10
IntSoftmax | n:  24
IntGELU    | n:  10
IntSoftmax | n:  24
IntGELU    | n:  10
IntSoftmax | n:  24
IntGELU    | n:  10
.calib done
Test: [  0/391]	Time  3.794 ( 3.794)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.83
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=24, intgelu_exp_n=11
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=24, intgelu_exp_n=11, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  24
IntGELU    | n:  11
IntSoftmax | n:  24
IntGELU    | n:  11
IntSoftmax | n:  24
IntGELU    | n:  11
IntSoftmax | n:  24
IntGELU    | n:  11
IntSoftmax | n:  24
IntGELU    | n:  11
IntSoftmax | n:  24
IntGELU    | n:  11
IntSoftmax | n:  24
IntGELU    | n:  11
IntSoftmax | n:  24
IntGELU    | n:  11
IntSoftmax | n:  24
IntGELU    | n:  11
IntSoftmax | n:  24
IntGELU    | n:  11
IntSoftmax | n:  24
IntGELU    | n:  11
IntSoftmax | n:  24
IntGELU    | n:  11
.calib done
Test: [  0/391]	Time  3.830 ( 3.830)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.94
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=24, intgelu_exp_n=12
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=24, intgelu_exp_n=12, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  24
IntGELU    | n:  12
IntSoftmax | n:  24
IntGELU    | n:  12
IntSoftmax | n:  24
IntGELU    | n:  12
IntSoftmax | n:  24
IntGELU    | n:  12
IntSoftmax | n:  24
IntGELU    | n:  12
IntSoftmax | n:  24
IntGELU    | n:  12
IntSoftmax | n:  24
IntGELU    | n:  12
IntSoftmax | n:  24
IntGELU    | n:  12
IntSoftmax | n:  24
IntGELU    | n:  12
IntSoftmax | n:  24
IntGELU    | n:  12
IntSoftmax | n:  24
IntGELU    | n:  12
IntSoftmax | n:  24
IntGELU    | n:  12
.calib done
Test: [  0/391]	Time  3.888 ( 3.888)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.05
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=24, intgelu_exp_n=13
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=24, intgelu_exp_n=13, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  24
IntGELU    | n:  13
IntSoftmax | n:  24
IntGELU    | n:  13
IntSoftmax | n:  24
IntGELU    | n:  13
IntSoftmax | n:  24
IntGELU    | n:  13
IntSoftmax | n:  24
IntGELU    | n:  13
IntSoftmax | n:  24
IntGELU    | n:  13
IntSoftmax | n:  24
IntGELU    | n:  13
IntSoftmax | n:  24
IntGELU    | n:  13
IntSoftmax | n:  24
IntGELU    | n:  13
IntSoftmax | n:  24
IntGELU    | n:  13
IntSoftmax | n:  24
IntGELU    | n:  13
IntSoftmax | n:  24
IntGELU    | n:  13
.calib done
Test: [  0/391]	Time  4.000 ( 4.000)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.07
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=24, intgelu_exp_n=14
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=24, intgelu_exp_n=14, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  24
IntGELU    | n:  14
IntSoftmax | n:  24
IntGELU    | n:  14
IntSoftmax | n:  24
IntGELU    | n:  14
IntSoftmax | n:  24
IntGELU    | n:  14
IntSoftmax | n:  24
IntGELU    | n:  14
IntSoftmax | n:  24
IntGELU    | n:  14
IntSoftmax | n:  24
IntGELU    | n:  14
IntSoftmax | n:  24
IntGELU    | n:  14
IntSoftmax | n:  24
IntGELU    | n:  14
IntSoftmax | n:  24
IntGELU    | n:  14
IntSoftmax | n:  24
IntGELU    | n:  14
IntSoftmax | n:  24
IntGELU    | n:  14
.calib done
Test: [  0/391]	Time  3.887 ( 3.887)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.07
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=24, intgelu_exp_n=15
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=24, intgelu_exp_n=15, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  24
IntGELU    | n:  15
IntSoftmax | n:  24
IntGELU    | n:  15
IntSoftmax | n:  24
IntGELU    | n:  15
IntSoftmax | n:  24
IntGELU    | n:  15
IntSoftmax | n:  24
IntGELU    | n:  15
IntSoftmax | n:  24
IntGELU    | n:  15
IntSoftmax | n:  24
IntGELU    | n:  15
IntSoftmax | n:  24
IntGELU    | n:  15
IntSoftmax | n:  24
IntGELU    | n:  15
IntSoftmax | n:  24
IntGELU    | n:  15
IntSoftmax | n:  24
IntGELU    | n:  15
IntSoftmax | n:  24
IntGELU    | n:  15
.calib done
Test: [  0/391]	Time  3.964 ( 3.964)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.08
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=24, intgelu_exp_n=16
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=24, intgelu_exp_n=16, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  24
IntGELU    | n:  16
IntSoftmax | n:  24
IntGELU    | n:  16
IntSoftmax | n:  24
IntGELU    | n:  16
IntSoftmax | n:  24
IntGELU    | n:  16
IntSoftmax | n:  24
IntGELU    | n:  16
IntSoftmax | n:  24
IntGELU    | n:  16
IntSoftmax | n:  24
IntGELU    | n:  16
IntSoftmax | n:  24
IntGELU    | n:  16
IntSoftmax | n:  24
IntGELU    | n:  16
IntSoftmax | n:  24
IntGELU    | n:  16
IntSoftmax | n:  24
IntGELU    | n:  16
IntSoftmax | n:  24
IntGELU    | n:  16
.calib done
Test: [  0/391]	Time  3.910 ( 3.910)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.95
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=24, intgelu_exp_n=17
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=24, intgelu_exp_n=17, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  24
IntGELU    | n:  17
IntSoftmax | n:  24
IntGELU    | n:  17
IntSoftmax | n:  24
IntGELU    | n:  17
IntSoftmax | n:  24
IntGELU    | n:  17
IntSoftmax | n:  24
IntGELU    | n:  17
IntSoftmax | n:  24
IntGELU    | n:  17
IntSoftmax | n:  24
IntGELU    | n:  17
IntSoftmax | n:  24
IntGELU    | n:  17
IntSoftmax | n:  24
IntGELU    | n:  17
IntSoftmax | n:  24
IntGELU    | n:  17
IntSoftmax | n:  24
IntGELU    | n:  17
IntSoftmax | n:  24
IntGELU    | n:  17
.calib done
Test: [  0/391]	Time  3.938 ( 3.938)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=24, intgelu_exp_n=18
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=24, intgelu_exp_n=18, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  24
IntGELU    | n:  18
IntSoftmax | n:  24
IntGELU    | n:  18
IntSoftmax | n:  24
IntGELU    | n:  18
IntSoftmax | n:  24
IntGELU    | n:  18
IntSoftmax | n:  24
IntGELU    | n:  18
IntSoftmax | n:  24
IntGELU    | n:  18
IntSoftmax | n:  24
IntGELU    | n:  18
IntSoftmax | n:  24
IntGELU    | n:  18
IntSoftmax | n:  24
IntGELU    | n:  18
IntSoftmax | n:  24
IntGELU    | n:  18
IntSoftmax | n:  24
IntGELU    | n:  18
IntSoftmax | n:  24
IntGELU    | n:  18
.calib done
Test: [  0/391]	Time  3.919 ( 3.919)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.95
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=24, intgelu_exp_n=19
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=24, intgelu_exp_n=19, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  24
IntGELU    | n:  19
IntSoftmax | n:  24
IntGELU    | n:  19
IntSoftmax | n:  24
IntGELU    | n:  19
IntSoftmax | n:  24
IntGELU    | n:  19
IntSoftmax | n:  24
IntGELU    | n:  19
IntSoftmax | n:  24
IntGELU    | n:  19
IntSoftmax | n:  24
IntGELU    | n:  19
IntSoftmax | n:  24
IntGELU    | n:  19
IntSoftmax | n:  24
IntGELU    | n:  19
IntSoftmax | n:  24
IntGELU    | n:  19
IntSoftmax | n:  24
IntGELU    | n:  19
IntSoftmax | n:  24
IntGELU    | n:  19
.calib done
Test: [  0/391]	Time  3.800 ( 3.800)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.92
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=24, intgelu_exp_n=20
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=24, intgelu_exp_n=20, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  24
IntGELU    | n:  20
IntSoftmax | n:  24
IntGELU    | n:  20
IntSoftmax | n:  24
IntGELU    | n:  20
IntSoftmax | n:  24
IntGELU    | n:  20
IntSoftmax | n:  24
IntGELU    | n:  20
IntSoftmax | n:  24
IntGELU    | n:  20
IntSoftmax | n:  24
IntGELU    | n:  20
IntSoftmax | n:  24
IntGELU    | n:  20
IntSoftmax | n:  24
IntGELU    | n:  20
IntSoftmax | n:  24
IntGELU    | n:  20
IntSoftmax | n:  24
IntGELU    | n:  20
IntSoftmax | n:  24
IntGELU    | n:  20
.calib done
Test: [  0/391]	Time  4.006 ( 4.006)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.01
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=24, intgelu_exp_n=21
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=24, intgelu_exp_n=21, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  24
IntGELU    | n:  21
IntSoftmax | n:  24
IntGELU    | n:  21
IntSoftmax | n:  24
IntGELU    | n:  21
IntSoftmax | n:  24
IntGELU    | n:  21
IntSoftmax | n:  24
IntGELU    | n:  21
IntSoftmax | n:  24
IntGELU    | n:  21
IntSoftmax | n:  24
IntGELU    | n:  21
IntSoftmax | n:  24
IntGELU    | n:  21
IntSoftmax | n:  24
IntGELU    | n:  21
IntSoftmax | n:  24
IntGELU    | n:  21
IntSoftmax | n:  24
IntGELU    | n:  21
IntSoftmax | n:  24
IntGELU    | n:  21
.calib done
Test: [  0/391]	Time  3.716 ( 3.716)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.78
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=24, intgelu_exp_n=22
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=24, intgelu_exp_n=22, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  24
IntGELU    | n:  22
IntSoftmax | n:  24
IntGELU    | n:  22
IntSoftmax | n:  24
IntGELU    | n:  22
IntSoftmax | n:  24
IntGELU    | n:  22
IntSoftmax | n:  24
IntGELU    | n:  22
IntSoftmax | n:  24
IntGELU    | n:  22
IntSoftmax | n:  24
IntGELU    | n:  22
IntSoftmax | n:  24
IntGELU    | n:  22
IntSoftmax | n:  24
IntGELU    | n:  22
IntSoftmax | n:  24
IntGELU    | n:  22
IntSoftmax | n:  24
IntGELU    | n:  22
IntSoftmax | n:  24
IntGELU    | n:  22
.calib done
Test: [  0/391]	Time  3.857 ( 3.857)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.93
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=24, intgelu_exp_n=23
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=24, intgelu_exp_n=23, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  24
IntGELU    | n:  23
IntSoftmax | n:  24
IntGELU    | n:  23
IntSoftmax | n:  24
IntGELU    | n:  23
IntSoftmax | n:  24
IntGELU    | n:  23
IntSoftmax | n:  24
IntGELU    | n:  23
IntSoftmax | n:  24
IntGELU    | n:  23
IntSoftmax | n:  24
IntGELU    | n:  23
IntSoftmax | n:  24
IntGELU    | n:  23
IntSoftmax | n:  24
IntGELU    | n:  23
IntSoftmax | n:  24
IntGELU    | n:  23
IntSoftmax | n:  24
IntGELU    | n:  23
IntSoftmax | n:  24
IntGELU    | n:  23
.calib done
Test: [  0/391]	Time  3.691 ( 3.691)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.77
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=24, intgelu_exp_n=24
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=24, intgelu_exp_n=24, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  24
IntGELU    | n:  24
IntSoftmax | n:  24
IntGELU    | n:  24
IntSoftmax | n:  24
IntGELU    | n:  24
IntSoftmax | n:  24
IntGELU    | n:  24
IntSoftmax | n:  24
IntGELU    | n:  24
IntSoftmax | n:  24
IntGELU    | n:  24
IntSoftmax | n:  24
IntGELU    | n:  24
IntSoftmax | n:  24
IntGELU    | n:  24
IntSoftmax | n:  24
IntGELU    | n:  24
IntSoftmax | n:  24
IntGELU    | n:  24
IntSoftmax | n:  24
IntGELU    | n:  24
IntSoftmax | n:  24
IntGELU    | n:  24
.calib done
Test: [  0/391]	Time  3.844 ( 3.844)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.90
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=24, intgelu_exp_n=25
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=24, intgelu_exp_n=25, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  24
IntGELU    | n:  25
IntSoftmax | n:  24
IntGELU    | n:  25
IntSoftmax | n:  24
IntGELU    | n:  25
IntSoftmax | n:  24
IntGELU    | n:  25
IntSoftmax | n:  24
IntGELU    | n:  25
IntSoftmax | n:  24
IntGELU    | n:  25
IntSoftmax | n:  24
IntGELU    | n:  25
IntSoftmax | n:  24
IntGELU    | n:  25
IntSoftmax | n:  24
IntGELU    | n:  25
IntSoftmax | n:  24
IntGELU    | n:  25
IntSoftmax | n:  24
IntGELU    | n:  25
IntSoftmax | n:  24
IntGELU    | n:  25
.calib done
Test: [  0/391]	Time  3.720 ( 3.720)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.77
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=24, intgelu_exp_n=26
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=24, intgelu_exp_n=26, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  24
IntGELU    | n:  26
IntSoftmax | n:  24
IntGELU    | n:  26
IntSoftmax | n:  24
IntGELU    | n:  26
IntSoftmax | n:  24
IntGELU    | n:  26
IntSoftmax | n:  24
IntGELU    | n:  26
IntSoftmax | n:  24
IntGELU    | n:  26
IntSoftmax | n:  24
IntGELU    | n:  26
IntSoftmax | n:  24
IntGELU    | n:  26
IntSoftmax | n:  24
IntGELU    | n:  26
IntSoftmax | n:  24
IntGELU    | n:  26
IntSoftmax | n:  24
IntGELU    | n:  26
IntSoftmax | n:  24
IntGELU    | n:  26
.calib done
Test: [  0/391]	Time  3.986 ( 3.986)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.05
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=24, intgelu_exp_n=27
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=24, intgelu_exp_n=27, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  24
IntGELU    | n:  27
IntSoftmax | n:  24
IntGELU    | n:  27
IntSoftmax | n:  24
IntGELU    | n:  27
IntSoftmax | n:  24
IntGELU    | n:  27
IntSoftmax | n:  24
IntGELU    | n:  27
IntSoftmax | n:  24
IntGELU    | n:  27
IntSoftmax | n:  24
IntGELU    | n:  27
IntSoftmax | n:  24
IntGELU    | n:  27
IntSoftmax | n:  24
IntGELU    | n:  27
IntSoftmax | n:  24
IntGELU    | n:  27
IntSoftmax | n:  24
IntGELU    | n:  27
IntSoftmax | n:  24
IntGELU    | n:  27
.calib done
Test: [  0/391]	Time  3.643 ( 3.643)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.67
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=24, intgelu_exp_n=28
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=24, intgelu_exp_n=28, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  24
IntGELU    | n:  28
IntSoftmax | n:  24
IntGELU    | n:  28
IntSoftmax | n:  24
IntGELU    | n:  28
IntSoftmax | n:  24
IntGELU    | n:  28
IntSoftmax | n:  24
IntGELU    | n:  28
IntSoftmax | n:  24
IntGELU    | n:  28
IntSoftmax | n:  24
IntGELU    | n:  28
IntSoftmax | n:  24
IntGELU    | n:  28
IntSoftmax | n:  24
IntGELU    | n:  28
IntSoftmax | n:  24
IntGELU    | n:  28
IntSoftmax | n:  24
IntGELU    | n:  28
IntSoftmax | n:  24
IntGELU    | n:  28
.calib done
Test: [  0/391]	Time  3.656 ( 3.656)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.69
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=24, intgelu_exp_n=29
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=24, intgelu_exp_n=29, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  24
IntGELU    | n:  29
IntSoftmax | n:  24
IntGELU    | n:  29
IntSoftmax | n:  24
IntGELU    | n:  29
IntSoftmax | n:  24
IntGELU    | n:  29
IntSoftmax | n:  24
IntGELU    | n:  29
IntSoftmax | n:  24
IntGELU    | n:  29
IntSoftmax | n:  24
IntGELU    | n:  29
IntSoftmax | n:  24
IntGELU    | n:  29
IntSoftmax | n:  24
IntGELU    | n:  29
IntSoftmax | n:  24
IntGELU    | n:  29
IntSoftmax | n:  24
IntGELU    | n:  29
IntSoftmax | n:  24
IntGELU    | n:  29
.calib done
Test: [  0/391]	Time  3.905 ( 3.905)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=24, intgelu_exp_n=30
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=24, intgelu_exp_n=30, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  24
IntGELU    | n:  30
IntSoftmax | n:  24
IntGELU    | n:  30
IntSoftmax | n:  24
IntGELU    | n:  30
IntSoftmax | n:  24
IntGELU    | n:  30
IntSoftmax | n:  24
IntGELU    | n:  30
IntSoftmax | n:  24
IntGELU    | n:  30
IntSoftmax | n:  24
IntGELU    | n:  30
IntSoftmax | n:  24
IntGELU    | n:  30
IntSoftmax | n:  24
IntGELU    | n:  30
IntSoftmax | n:  24
IntGELU    | n:  30
IntSoftmax | n:  24
IntGELU    | n:  30
IntSoftmax | n:  24
IntGELU    | n:  30
.calib done
Test: [  0/391]	Time  4.019 ( 4.019)	Acc@1   0.78 (  0.78)	Acc@5   2.34 (  2.34)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.781 Prec@5 2.344
Time: 9.06
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=24, intgelu_exp_n=31
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=24, intgelu_exp_n=31, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  24
IntGELU    | n:  31
IntSoftmax | n:  24
IntGELU    | n:  31
IntSoftmax | n:  24
IntGELU    | n:  31
IntSoftmax | n:  24
IntGELU    | n:  31
IntSoftmax | n:  24
IntGELU    | n:  31
IntSoftmax | n:  24
IntGELU    | n:  31
IntSoftmax | n:  24
IntGELU    | n:  31
IntSoftmax | n:  24
IntGELU    | n:  31
IntSoftmax | n:  24
IntGELU    | n:  31
IntSoftmax | n:  24
IntGELU    | n:  31
IntSoftmax | n:  24
IntGELU    | n:  31
IntSoftmax | n:  24
IntGELU    | n:  31
.calib done
Test: [  0/391]	Time  3.795 ( 3.795)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.79
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=25, intgelu_exp_n=0
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=25, intgelu_exp_n=0, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  25
IntGELU    | n:  0
IntSoftmax | n:  25
IntGELU    | n:  0
IntSoftmax | n:  25
IntGELU    | n:  0
IntSoftmax | n:  25
IntGELU    | n:  0
IntSoftmax | n:  25
IntGELU    | n:  0
IntSoftmax | n:  25
IntGELU    | n:  0
IntSoftmax | n:  25
IntGELU    | n:  0
IntSoftmax | n:  25
IntGELU    | n:  0
IntSoftmax | n:  25
IntGELU    | n:  0
IntSoftmax | n:  25
IntGELU    | n:  0
IntSoftmax | n:  25
IntGELU    | n:  0
IntSoftmax | n:  25
IntGELU    | n:  0
.calib done
Test: [  0/391]	Time  3.932 ( 3.932)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.99
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=25, intgelu_exp_n=1
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=25, intgelu_exp_n=1, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  25
IntGELU    | n:  1
IntSoftmax | n:  25
IntGELU    | n:  1
IntSoftmax | n:  25
IntGELU    | n:  1
IntSoftmax | n:  25
IntGELU    | n:  1
IntSoftmax | n:  25
IntGELU    | n:  1
IntSoftmax | n:  25
IntGELU    | n:  1
IntSoftmax | n:  25
IntGELU    | n:  1
IntSoftmax | n:  25
IntGELU    | n:  1
IntSoftmax | n:  25
IntGELU    | n:  1
IntSoftmax | n:  25
IntGELU    | n:  1
IntSoftmax | n:  25
IntGELU    | n:  1
IntSoftmax | n:  25
IntGELU    | n:  1
.calib done
Test: [  0/391]	Time  3.673 ( 3.673)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.72
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=25, intgelu_exp_n=2
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=25, intgelu_exp_n=2, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  25
IntGELU    | n:  2
IntSoftmax | n:  25
IntGELU    | n:  2
IntSoftmax | n:  25
IntGELU    | n:  2
IntSoftmax | n:  25
IntGELU    | n:  2
IntSoftmax | n:  25
IntGELU    | n:  2
IntSoftmax | n:  25
IntGELU    | n:  2
IntSoftmax | n:  25
IntGELU    | n:  2
IntSoftmax | n:  25
IntGELU    | n:  2
IntSoftmax | n:  25
IntGELU    | n:  2
IntSoftmax | n:  25
IntGELU    | n:  2
IntSoftmax | n:  25
IntGELU    | n:  2
IntSoftmax | n:  25
IntGELU    | n:  2
.calib done
Test: [  0/391]	Time  3.963 ( 3.963)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.00
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=25, intgelu_exp_n=3
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=25, intgelu_exp_n=3, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  25
IntGELU    | n:  3
IntSoftmax | n:  25
IntGELU    | n:  3
IntSoftmax | n:  25
IntGELU    | n:  3
IntSoftmax | n:  25
IntGELU    | n:  3
IntSoftmax | n:  25
IntGELU    | n:  3
IntSoftmax | n:  25
IntGELU    | n:  3
IntSoftmax | n:  25
IntGELU    | n:  3
IntSoftmax | n:  25
IntGELU    | n:  3
IntSoftmax | n:  25
IntGELU    | n:  3
IntSoftmax | n:  25
IntGELU    | n:  3
IntSoftmax | n:  25
IntGELU    | n:  3
IntSoftmax | n:  25
IntGELU    | n:  3
.calib done
Test: [  0/391]	Time  3.701 ( 3.701)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.78
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=25, intgelu_exp_n=4
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=25, intgelu_exp_n=4, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  25
IntGELU    | n:  4
IntSoftmax | n:  25
IntGELU    | n:  4
IntSoftmax | n:  25
IntGELU    | n:  4
IntSoftmax | n:  25
IntGELU    | n:  4
IntSoftmax | n:  25
IntGELU    | n:  4
IntSoftmax | n:  25
IntGELU    | n:  4
IntSoftmax | n:  25
IntGELU    | n:  4
IntSoftmax | n:  25
IntGELU    | n:  4
IntSoftmax | n:  25
IntGELU    | n:  4
IntSoftmax | n:  25
IntGELU    | n:  4
IntSoftmax | n:  25
IntGELU    | n:  4
IntSoftmax | n:  25
IntGELU    | n:  4
.calib done
Test: [  0/391]	Time  3.793 ( 3.793)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.81
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=25, intgelu_exp_n=5
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=25, intgelu_exp_n=5, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  25
IntGELU    | n:  5
IntSoftmax | n:  25
IntGELU    | n:  5
IntSoftmax | n:  25
IntGELU    | n:  5
IntSoftmax | n:  25
IntGELU    | n:  5
IntSoftmax | n:  25
IntGELU    | n:  5
IntSoftmax | n:  25
IntGELU    | n:  5
IntSoftmax | n:  25
IntGELU    | n:  5
IntSoftmax | n:  25
IntGELU    | n:  5
IntSoftmax | n:  25
IntGELU    | n:  5
IntSoftmax | n:  25
IntGELU    | n:  5
IntSoftmax | n:  25
IntGELU    | n:  5
IntSoftmax | n:  25
IntGELU    | n:  5
.calib done
Test: [  0/391]	Time  3.971 ( 3.971)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=25, intgelu_exp_n=6
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=25, intgelu_exp_n=6, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  25
IntGELU    | n:  6
IntSoftmax | n:  25
IntGELU    | n:  6
IntSoftmax | n:  25
IntGELU    | n:  6
IntSoftmax | n:  25
IntGELU    | n:  6
IntSoftmax | n:  25
IntGELU    | n:  6
IntSoftmax | n:  25
IntGELU    | n:  6
IntSoftmax | n:  25
IntGELU    | n:  6
IntSoftmax | n:  25
IntGELU    | n:  6
IntSoftmax | n:  25
IntGELU    | n:  6
IntSoftmax | n:  25
IntGELU    | n:  6
IntSoftmax | n:  25
IntGELU    | n:  6
IntSoftmax | n:  25
IntGELU    | n:  6
.calib done
Test: [  0/391]	Time  3.875 ( 3.875)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.86
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=25, intgelu_exp_n=7
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=25, intgelu_exp_n=7, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  25
IntGELU    | n:  7
IntSoftmax | n:  25
IntGELU    | n:  7
IntSoftmax | n:  25
IntGELU    | n:  7
IntSoftmax | n:  25
IntGELU    | n:  7
IntSoftmax | n:  25
IntGELU    | n:  7
IntSoftmax | n:  25
IntGELU    | n:  7
IntSoftmax | n:  25
IntGELU    | n:  7
IntSoftmax | n:  25
IntGELU    | n:  7
IntSoftmax | n:  25
IntGELU    | n:  7
IntSoftmax | n:  25
IntGELU    | n:  7
IntSoftmax | n:  25
IntGELU    | n:  7
IntSoftmax | n:  25
IntGELU    | n:  7
.calib done
Test: [  0/391]	Time  3.820 ( 3.820)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.85
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=25, intgelu_exp_n=8
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=25, intgelu_exp_n=8, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  25
IntGELU    | n:  8
IntSoftmax | n:  25
IntGELU    | n:  8
IntSoftmax | n:  25
IntGELU    | n:  8
IntSoftmax | n:  25
IntGELU    | n:  8
IntSoftmax | n:  25
IntGELU    | n:  8
IntSoftmax | n:  25
IntGELU    | n:  8
IntSoftmax | n:  25
IntGELU    | n:  8
IntSoftmax | n:  25
IntGELU    | n:  8
IntSoftmax | n:  25
IntGELU    | n:  8
IntSoftmax | n:  25
IntGELU    | n:  8
IntSoftmax | n:  25
IntGELU    | n:  8
IntSoftmax | n:  25
IntGELU    | n:  8
.calib done
Test: [  0/391]	Time  3.792 ( 3.792)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.86
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=25, intgelu_exp_n=9
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=25, intgelu_exp_n=9, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  25
IntGELU    | n:  9
IntSoftmax | n:  25
IntGELU    | n:  9
IntSoftmax | n:  25
IntGELU    | n:  9
IntSoftmax | n:  25
IntGELU    | n:  9
IntSoftmax | n:  25
IntGELU    | n:  9
IntSoftmax | n:  25
IntGELU    | n:  9
IntSoftmax | n:  25
IntGELU    | n:  9
IntSoftmax | n:  25
IntGELU    | n:  9
IntSoftmax | n:  25
IntGELU    | n:  9
IntSoftmax | n:  25
IntGELU    | n:  9
IntSoftmax | n:  25
IntGELU    | n:  9
IntSoftmax | n:  25
IntGELU    | n:  9
.calib done
Test: [  0/391]	Time  3.932 ( 3.932)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.98
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=25, intgelu_exp_n=10
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=25, intgelu_exp_n=10, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  25
IntGELU    | n:  10
IntSoftmax | n:  25
IntGELU    | n:  10
IntSoftmax | n:  25
IntGELU    | n:  10
IntSoftmax | n:  25
IntGELU    | n:  10
IntSoftmax | n:  25
IntGELU    | n:  10
IntSoftmax | n:  25
IntGELU    | n:  10
IntSoftmax | n:  25
IntGELU    | n:  10
IntSoftmax | n:  25
IntGELU    | n:  10
IntSoftmax | n:  25
IntGELU    | n:  10
IntSoftmax | n:  25
IntGELU    | n:  10
IntSoftmax | n:  25
IntGELU    | n:  10
IntSoftmax | n:  25
IntGELU    | n:  10
.calib done
Test: [  0/391]	Time  3.775 ( 3.775)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.79
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=25, intgelu_exp_n=11
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=25, intgelu_exp_n=11, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  25
IntGELU    | n:  11
IntSoftmax | n:  25
IntGELU    | n:  11
IntSoftmax | n:  25
IntGELU    | n:  11
IntSoftmax | n:  25
IntGELU    | n:  11
IntSoftmax | n:  25
IntGELU    | n:  11
IntSoftmax | n:  25
IntGELU    | n:  11
IntSoftmax | n:  25
IntGELU    | n:  11
IntSoftmax | n:  25
IntGELU    | n:  11
IntSoftmax | n:  25
IntGELU    | n:  11
IntSoftmax | n:  25
IntGELU    | n:  11
IntSoftmax | n:  25
IntGELU    | n:  11
IntSoftmax | n:  25
IntGELU    | n:  11
.calib done
Test: [  0/391]	Time  3.854 ( 3.854)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.91
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=25, intgelu_exp_n=12
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=25, intgelu_exp_n=12, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  25
IntGELU    | n:  12
IntSoftmax | n:  25
IntGELU    | n:  12
IntSoftmax | n:  25
IntGELU    | n:  12
IntSoftmax | n:  25
IntGELU    | n:  12
IntSoftmax | n:  25
IntGELU    | n:  12
IntSoftmax | n:  25
IntGELU    | n:  12
IntSoftmax | n:  25
IntGELU    | n:  12
IntSoftmax | n:  25
IntGELU    | n:  12
IntSoftmax | n:  25
IntGELU    | n:  12
IntSoftmax | n:  25
IntGELU    | n:  12
IntSoftmax | n:  25
IntGELU    | n:  12
IntSoftmax | n:  25
IntGELU    | n:  12
.calib done
Test: [  0/391]	Time  3.657 ( 3.657)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.73
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=25, intgelu_exp_n=13
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=25, intgelu_exp_n=13, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  25
IntGELU    | n:  13
IntSoftmax | n:  25
IntGELU    | n:  13
IntSoftmax | n:  25
IntGELU    | n:  13
IntSoftmax | n:  25
IntGELU    | n:  13
IntSoftmax | n:  25
IntGELU    | n:  13
IntSoftmax | n:  25
IntGELU    | n:  13
IntSoftmax | n:  25
IntGELU    | n:  13
IntSoftmax | n:  25
IntGELU    | n:  13
IntSoftmax | n:  25
IntGELU    | n:  13
IntSoftmax | n:  25
IntGELU    | n:  13
IntSoftmax | n:  25
IntGELU    | n:  13
IntSoftmax | n:  25
IntGELU    | n:  13
.calib done
Test: [  0/391]	Time  4.035 ( 4.035)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.06
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=25, intgelu_exp_n=14
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=25, intgelu_exp_n=14, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  25
IntGELU    | n:  14
IntSoftmax | n:  25
IntGELU    | n:  14
IntSoftmax | n:  25
IntGELU    | n:  14
IntSoftmax | n:  25
IntGELU    | n:  14
IntSoftmax | n:  25
IntGELU    | n:  14
IntSoftmax | n:  25
IntGELU    | n:  14
IntSoftmax | n:  25
IntGELU    | n:  14
IntSoftmax | n:  25
IntGELU    | n:  14
IntSoftmax | n:  25
IntGELU    | n:  14
IntSoftmax | n:  25
IntGELU    | n:  14
IntSoftmax | n:  25
IntGELU    | n:  14
IntSoftmax | n:  25
IntGELU    | n:  14
.calib done
Test: [  0/391]	Time  3.973 ( 3.973)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.06
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=25, intgelu_exp_n=15
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=25, intgelu_exp_n=15, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  25
IntGELU    | n:  15
IntSoftmax | n:  25
IntGELU    | n:  15
IntSoftmax | n:  25
IntGELU    | n:  15
IntSoftmax | n:  25
IntGELU    | n:  15
IntSoftmax | n:  25
IntGELU    | n:  15
IntSoftmax | n:  25
IntGELU    | n:  15
IntSoftmax | n:  25
IntGELU    | n:  15
IntSoftmax | n:  25
IntGELU    | n:  15
IntSoftmax | n:  25
IntGELU    | n:  15
IntSoftmax | n:  25
IntGELU    | n:  15
IntSoftmax | n:  25
IntGELU    | n:  15
IntSoftmax | n:  25
IntGELU    | n:  15
.calib done
Test: [  0/391]	Time  3.917 ( 3.917)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.00
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=25, intgelu_exp_n=16
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=25, intgelu_exp_n=16, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  25
IntGELU    | n:  16
IntSoftmax | n:  25
IntGELU    | n:  16
IntSoftmax | n:  25
IntGELU    | n:  16
IntSoftmax | n:  25
IntGELU    | n:  16
IntSoftmax | n:  25
IntGELU    | n:  16
IntSoftmax | n:  25
IntGELU    | n:  16
IntSoftmax | n:  25
IntGELU    | n:  16
IntSoftmax | n:  25
IntGELU    | n:  16
IntSoftmax | n:  25
IntGELU    | n:  16
IntSoftmax | n:  25
IntGELU    | n:  16
IntSoftmax | n:  25
IntGELU    | n:  16
IntSoftmax | n:  25
IntGELU    | n:  16
.calib done
Test: [  0/391]	Time  3.925 ( 3.925)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.98
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=25, intgelu_exp_n=17
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=25, intgelu_exp_n=17, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  25
IntGELU    | n:  17
IntSoftmax | n:  25
IntGELU    | n:  17
IntSoftmax | n:  25
IntGELU    | n:  17
IntSoftmax | n:  25
IntGELU    | n:  17
IntSoftmax | n:  25
IntGELU    | n:  17
IntSoftmax | n:  25
IntGELU    | n:  17
IntSoftmax | n:  25
IntGELU    | n:  17
IntSoftmax | n:  25
IntGELU    | n:  17
IntSoftmax | n:  25
IntGELU    | n:  17
IntSoftmax | n:  25
IntGELU    | n:  17
IntSoftmax | n:  25
IntGELU    | n:  17
IntSoftmax | n:  25
IntGELU    | n:  17
.calib done
Test: [  0/391]	Time  3.849 ( 3.849)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.88
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=25, intgelu_exp_n=18
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=25, intgelu_exp_n=18, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  25
IntGELU    | n:  18
IntSoftmax | n:  25
IntGELU    | n:  18
IntSoftmax | n:  25
IntGELU    | n:  18
IntSoftmax | n:  25
IntGELU    | n:  18
IntSoftmax | n:  25
IntGELU    | n:  18
IntSoftmax | n:  25
IntGELU    | n:  18
IntSoftmax | n:  25
IntGELU    | n:  18
IntSoftmax | n:  25
IntGELU    | n:  18
IntSoftmax | n:  25
IntGELU    | n:  18
IntSoftmax | n:  25
IntGELU    | n:  18
IntSoftmax | n:  25
IntGELU    | n:  18
IntSoftmax | n:  25
IntGELU    | n:  18
.calib done
Test: [  0/391]	Time  3.741 ( 3.741)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.78
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=25, intgelu_exp_n=19
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=25, intgelu_exp_n=19, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  25
IntGELU    | n:  19
IntSoftmax | n:  25
IntGELU    | n:  19
IntSoftmax | n:  25
IntGELU    | n:  19
IntSoftmax | n:  25
IntGELU    | n:  19
IntSoftmax | n:  25
IntGELU    | n:  19
IntSoftmax | n:  25
IntGELU    | n:  19
IntSoftmax | n:  25
IntGELU    | n:  19
IntSoftmax | n:  25
IntGELU    | n:  19
IntSoftmax | n:  25
IntGELU    | n:  19
IntSoftmax | n:  25
IntGELU    | n:  19
IntSoftmax | n:  25
IntGELU    | n:  19
IntSoftmax | n:  25
IntGELU    | n:  19
.calib done
Test: [  0/391]	Time  3.995 ( 3.995)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.07
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=25, intgelu_exp_n=20
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=25, intgelu_exp_n=20, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  25
IntGELU    | n:  20
IntSoftmax | n:  25
IntGELU    | n:  20
IntSoftmax | n:  25
IntGELU    | n:  20
IntSoftmax | n:  25
IntGELU    | n:  20
IntSoftmax | n:  25
IntGELU    | n:  20
IntSoftmax | n:  25
IntGELU    | n:  20
IntSoftmax | n:  25
IntGELU    | n:  20
IntSoftmax | n:  25
IntGELU    | n:  20
IntSoftmax | n:  25
IntGELU    | n:  20
IntSoftmax | n:  25
IntGELU    | n:  20
IntSoftmax | n:  25
IntGELU    | n:  20
IntSoftmax | n:  25
IntGELU    | n:  20
.calib done
Test: [  0/391]	Time  3.916 ( 3.916)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=25, intgelu_exp_n=21
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=25, intgelu_exp_n=21, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  25
IntGELU    | n:  21
IntSoftmax | n:  25
IntGELU    | n:  21
IntSoftmax | n:  25
IntGELU    | n:  21
IntSoftmax | n:  25
IntGELU    | n:  21
IntSoftmax | n:  25
IntGELU    | n:  21
IntSoftmax | n:  25
IntGELU    | n:  21
IntSoftmax | n:  25
IntGELU    | n:  21
IntSoftmax | n:  25
IntGELU    | n:  21
IntSoftmax | n:  25
IntGELU    | n:  21
IntSoftmax | n:  25
IntGELU    | n:  21
IntSoftmax | n:  25
IntGELU    | n:  21
IntSoftmax | n:  25
IntGELU    | n:  21
.calib done
Test: [  0/391]	Time  3.879 ( 3.879)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.93
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=25, intgelu_exp_n=22
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=25, intgelu_exp_n=22, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  25
IntGELU    | n:  22
IntSoftmax | n:  25
IntGELU    | n:  22
IntSoftmax | n:  25
IntGELU    | n:  22
IntSoftmax | n:  25
IntGELU    | n:  22
IntSoftmax | n:  25
IntGELU    | n:  22
IntSoftmax | n:  25
IntGELU    | n:  22
IntSoftmax | n:  25
IntGELU    | n:  22
IntSoftmax | n:  25
IntGELU    | n:  22
IntSoftmax | n:  25
IntGELU    | n:  22
IntSoftmax | n:  25
IntGELU    | n:  22
IntSoftmax | n:  25
IntGELU    | n:  22
IntSoftmax | n:  25
IntGELU    | n:  22
.calib done
Test: [  0/391]	Time  3.689 ( 3.689)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.75
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=25, intgelu_exp_n=23
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=25, intgelu_exp_n=23, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  25
IntGELU    | n:  23
IntSoftmax | n:  25
IntGELU    | n:  23
IntSoftmax | n:  25
IntGELU    | n:  23
IntSoftmax | n:  25
IntGELU    | n:  23
IntSoftmax | n:  25
IntGELU    | n:  23
IntSoftmax | n:  25
IntGELU    | n:  23
IntSoftmax | n:  25
IntGELU    | n:  23
IntSoftmax | n:  25
IntGELU    | n:  23
IntSoftmax | n:  25
IntGELU    | n:  23
IntSoftmax | n:  25
IntGELU    | n:  23
IntSoftmax | n:  25
IntGELU    | n:  23
IntSoftmax | n:  25
IntGELU    | n:  23
.calib done
Test: [  0/391]	Time  3.870 ( 3.870)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=25, intgelu_exp_n=24
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=25, intgelu_exp_n=24, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  25
IntGELU    | n:  24
IntSoftmax | n:  25
IntGELU    | n:  24
IntSoftmax | n:  25
IntGELU    | n:  24
IntSoftmax | n:  25
IntGELU    | n:  24
IntSoftmax | n:  25
IntGELU    | n:  24
IntSoftmax | n:  25
IntGELU    | n:  24
IntSoftmax | n:  25
IntGELU    | n:  24
IntSoftmax | n:  25
IntGELU    | n:  24
IntSoftmax | n:  25
IntGELU    | n:  24
IntSoftmax | n:  25
IntGELU    | n:  24
IntSoftmax | n:  25
IntGELU    | n:  24
IntSoftmax | n:  25
IntGELU    | n:  24
.calib done
Test: [  0/391]	Time  3.874 ( 3.874)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.99
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=25, intgelu_exp_n=25
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=25, intgelu_exp_n=25, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  25
IntGELU    | n:  25
IntSoftmax | n:  25
IntGELU    | n:  25
IntSoftmax | n:  25
IntGELU    | n:  25
IntSoftmax | n:  25
IntGELU    | n:  25
IntSoftmax | n:  25
IntGELU    | n:  25
IntSoftmax | n:  25
IntGELU    | n:  25
IntSoftmax | n:  25
IntGELU    | n:  25
IntSoftmax | n:  25
IntGELU    | n:  25
IntSoftmax | n:  25
IntGELU    | n:  25
IntSoftmax | n:  25
IntGELU    | n:  25
IntSoftmax | n:  25
IntGELU    | n:  25
IntSoftmax | n:  25
IntGELU    | n:  25
.calib done
Test: [  0/391]	Time  3.825 ( 3.825)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.93
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=25, intgelu_exp_n=26
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=25, intgelu_exp_n=26, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  25
IntGELU    | n:  26
IntSoftmax | n:  25
IntGELU    | n:  26
IntSoftmax | n:  25
IntGELU    | n:  26
IntSoftmax | n:  25
IntGELU    | n:  26
IntSoftmax | n:  25
IntGELU    | n:  26
IntSoftmax | n:  25
IntGELU    | n:  26
IntSoftmax | n:  25
IntGELU    | n:  26
IntSoftmax | n:  25
IntGELU    | n:  26
IntSoftmax | n:  25
IntGELU    | n:  26
IntSoftmax | n:  25
IntGELU    | n:  26
IntSoftmax | n:  25
IntGELU    | n:  26
IntSoftmax | n:  25
IntGELU    | n:  26
.calib done
Test: [  0/391]	Time  3.881 ( 3.881)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.93
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=25, intgelu_exp_n=27
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=25, intgelu_exp_n=27, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  25
IntGELU    | n:  27
IntSoftmax | n:  25
IntGELU    | n:  27
IntSoftmax | n:  25
IntGELU    | n:  27
IntSoftmax | n:  25
IntGELU    | n:  27
IntSoftmax | n:  25
IntGELU    | n:  27
IntSoftmax | n:  25
IntGELU    | n:  27
IntSoftmax | n:  25
IntGELU    | n:  27
IntSoftmax | n:  25
IntGELU    | n:  27
IntSoftmax | n:  25
IntGELU    | n:  27
IntSoftmax | n:  25
IntGELU    | n:  27
IntSoftmax | n:  25
IntGELU    | n:  27
IntSoftmax | n:  25
IntGELU    | n:  27
.calib done
Test: [  0/391]	Time  3.793 ( 3.793)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.85
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=25, intgelu_exp_n=28
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=25, intgelu_exp_n=28, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  25
IntGELU    | n:  28
IntSoftmax | n:  25
IntGELU    | n:  28
IntSoftmax | n:  25
IntGELU    | n:  28
IntSoftmax | n:  25
IntGELU    | n:  28
IntSoftmax | n:  25
IntGELU    | n:  28
IntSoftmax | n:  25
IntGELU    | n:  28
IntSoftmax | n:  25
IntGELU    | n:  28
IntSoftmax | n:  25
IntGELU    | n:  28
IntSoftmax | n:  25
IntGELU    | n:  28
IntSoftmax | n:  25
IntGELU    | n:  28
IntSoftmax | n:  25
IntGELU    | n:  28
IntSoftmax | n:  25
IntGELU    | n:  28
.calib done
Test: [  0/391]	Time  3.926 ( 3.926)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.95
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=25, intgelu_exp_n=29
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=25, intgelu_exp_n=29, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  25
IntGELU    | n:  29
IntSoftmax | n:  25
IntGELU    | n:  29
IntSoftmax | n:  25
IntGELU    | n:  29
IntSoftmax | n:  25
IntGELU    | n:  29
IntSoftmax | n:  25
IntGELU    | n:  29
IntSoftmax | n:  25
IntGELU    | n:  29
IntSoftmax | n:  25
IntGELU    | n:  29
IntSoftmax | n:  25
IntGELU    | n:  29
IntSoftmax | n:  25
IntGELU    | n:  29
IntSoftmax | n:  25
IntGELU    | n:  29
IntSoftmax | n:  25
IntGELU    | n:  29
IntSoftmax | n:  25
IntGELU    | n:  29
.calib done
Test: [  0/391]	Time  3.907 ( 3.907)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.99
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=25, intgelu_exp_n=30
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=25, intgelu_exp_n=30, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  25
IntGELU    | n:  30
IntSoftmax | n:  25
IntGELU    | n:  30
IntSoftmax | n:  25
IntGELU    | n:  30
IntSoftmax | n:  25
IntGELU    | n:  30
IntSoftmax | n:  25
IntGELU    | n:  30
IntSoftmax | n:  25
IntGELU    | n:  30
IntSoftmax | n:  25
IntGELU    | n:  30
IntSoftmax | n:  25
IntGELU    | n:  30
IntSoftmax | n:  25
IntGELU    | n:  30
IntSoftmax | n:  25
IntGELU    | n:  30
IntSoftmax | n:  25
IntGELU    | n:  30
IntSoftmax | n:  25
IntGELU    | n:  30
.calib done
Test: [  0/391]	Time  3.780 ( 3.780)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.77
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=25, intgelu_exp_n=31
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=25, intgelu_exp_n=31, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  25
IntGELU    | n:  31
IntSoftmax | n:  25
IntGELU    | n:  31
IntSoftmax | n:  25
IntGELU    | n:  31
IntSoftmax | n:  25
IntGELU    | n:  31
IntSoftmax | n:  25
IntGELU    | n:  31
IntSoftmax | n:  25
IntGELU    | n:  31
IntSoftmax | n:  25
IntGELU    | n:  31
IntSoftmax | n:  25
IntGELU    | n:  31
IntSoftmax | n:  25
IntGELU    | n:  31
IntSoftmax | n:  25
IntGELU    | n:  31
IntSoftmax | n:  25
IntGELU    | n:  31
IntSoftmax | n:  25
IntGELU    | n:  31
.calib done
Test: [  0/391]	Time  3.983 ( 3.983)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.03
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=26, intgelu_exp_n=0
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=26, intgelu_exp_n=0, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  26
IntGELU    | n:  0
IntSoftmax | n:  26
IntGELU    | n:  0
IntSoftmax | n:  26
IntGELU    | n:  0
IntSoftmax | n:  26
IntGELU    | n:  0
IntSoftmax | n:  26
IntGELU    | n:  0
IntSoftmax | n:  26
IntGELU    | n:  0
IntSoftmax | n:  26
IntGELU    | n:  0
IntSoftmax | n:  26
IntGELU    | n:  0
IntSoftmax | n:  26
IntGELU    | n:  0
IntSoftmax | n:  26
IntGELU    | n:  0
IntSoftmax | n:  26
IntGELU    | n:  0
IntSoftmax | n:  26
IntGELU    | n:  0
.calib done
Test: [  0/391]	Time  3.735 ( 3.735)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.79
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=26, intgelu_exp_n=1
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=26, intgelu_exp_n=1, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  26
IntGELU    | n:  1
IntSoftmax | n:  26
IntGELU    | n:  1
IntSoftmax | n:  26
IntGELU    | n:  1
IntSoftmax | n:  26
IntGELU    | n:  1
IntSoftmax | n:  26
IntGELU    | n:  1
IntSoftmax | n:  26
IntGELU    | n:  1
IntSoftmax | n:  26
IntGELU    | n:  1
IntSoftmax | n:  26
IntGELU    | n:  1
IntSoftmax | n:  26
IntGELU    | n:  1
IntSoftmax | n:  26
IntGELU    | n:  1
IntSoftmax | n:  26
IntGELU    | n:  1
IntSoftmax | n:  26
IntGELU    | n:  1
.calib done
Test: [  0/391]	Time  3.675 ( 3.675)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.75
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=26, intgelu_exp_n=2
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=26, intgelu_exp_n=2, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  26
IntGELU    | n:  2
IntSoftmax | n:  26
IntGELU    | n:  2
IntSoftmax | n:  26
IntGELU    | n:  2
IntSoftmax | n:  26
IntGELU    | n:  2
IntSoftmax | n:  26
IntGELU    | n:  2
IntSoftmax | n:  26
IntGELU    | n:  2
IntSoftmax | n:  26
IntGELU    | n:  2
IntSoftmax | n:  26
IntGELU    | n:  2
IntSoftmax | n:  26
IntGELU    | n:  2
IntSoftmax | n:  26
IntGELU    | n:  2
IntSoftmax | n:  26
IntGELU    | n:  2
IntSoftmax | n:  26
IntGELU    | n:  2
.calib done
Test: [  0/391]	Time  4.017 ( 4.017)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.11
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=26, intgelu_exp_n=3
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=26, intgelu_exp_n=3, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  26
IntGELU    | n:  3
IntSoftmax | n:  26
IntGELU    | n:  3
IntSoftmax | n:  26
IntGELU    | n:  3
IntSoftmax | n:  26
IntGELU    | n:  3
IntSoftmax | n:  26
IntGELU    | n:  3
IntSoftmax | n:  26
IntGELU    | n:  3
IntSoftmax | n:  26
IntGELU    | n:  3
IntSoftmax | n:  26
IntGELU    | n:  3
IntSoftmax | n:  26
IntGELU    | n:  3
IntSoftmax | n:  26
IntGELU    | n:  3
IntSoftmax | n:  26
IntGELU    | n:  3
IntSoftmax | n:  26
IntGELU    | n:  3
.calib done
Test: [  0/391]	Time  3.863 ( 3.863)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.84
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=26, intgelu_exp_n=4
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=26, intgelu_exp_n=4, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  26
IntGELU    | n:  4
IntSoftmax | n:  26
IntGELU    | n:  4
IntSoftmax | n:  26
IntGELU    | n:  4
IntSoftmax | n:  26
IntGELU    | n:  4
IntSoftmax | n:  26
IntGELU    | n:  4
IntSoftmax | n:  26
IntGELU    | n:  4
IntSoftmax | n:  26
IntGELU    | n:  4
IntSoftmax | n:  26
IntGELU    | n:  4
IntSoftmax | n:  26
IntGELU    | n:  4
IntSoftmax | n:  26
IntGELU    | n:  4
IntSoftmax | n:  26
IntGELU    | n:  4
IntSoftmax | n:  26
IntGELU    | n:  4
.calib done
Test: [  0/391]	Time  3.879 ( 3.879)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.93
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=26, intgelu_exp_n=5
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=26, intgelu_exp_n=5, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  26
IntGELU    | n:  5
IntSoftmax | n:  26
IntGELU    | n:  5
IntSoftmax | n:  26
IntGELU    | n:  5
IntSoftmax | n:  26
IntGELU    | n:  5
IntSoftmax | n:  26
IntGELU    | n:  5
IntSoftmax | n:  26
IntGELU    | n:  5
IntSoftmax | n:  26
IntGELU    | n:  5
IntSoftmax | n:  26
IntGELU    | n:  5
IntSoftmax | n:  26
IntGELU    | n:  5
IntSoftmax | n:  26
IntGELU    | n:  5
IntSoftmax | n:  26
IntGELU    | n:  5
IntSoftmax | n:  26
IntGELU    | n:  5
.calib done
Test: [  0/391]	Time  3.837 ( 3.837)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.90
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=26, intgelu_exp_n=6
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=26, intgelu_exp_n=6, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  26
IntGELU    | n:  6
IntSoftmax | n:  26
IntGELU    | n:  6
IntSoftmax | n:  26
IntGELU    | n:  6
IntSoftmax | n:  26
IntGELU    | n:  6
IntSoftmax | n:  26
IntGELU    | n:  6
IntSoftmax | n:  26
IntGELU    | n:  6
IntSoftmax | n:  26
IntGELU    | n:  6
IntSoftmax | n:  26
IntGELU    | n:  6
IntSoftmax | n:  26
IntGELU    | n:  6
IntSoftmax | n:  26
IntGELU    | n:  6
IntSoftmax | n:  26
IntGELU    | n:  6
IntSoftmax | n:  26
IntGELU    | n:  6
.calib done
Test: [  0/391]	Time  3.781 ( 3.781)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.83
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=26, intgelu_exp_n=7
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=26, intgelu_exp_n=7, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  26
IntGELU    | n:  7
IntSoftmax | n:  26
IntGELU    | n:  7
IntSoftmax | n:  26
IntGELU    | n:  7
IntSoftmax | n:  26
IntGELU    | n:  7
IntSoftmax | n:  26
IntGELU    | n:  7
IntSoftmax | n:  26
IntGELU    | n:  7
IntSoftmax | n:  26
IntGELU    | n:  7
IntSoftmax | n:  26
IntGELU    | n:  7
IntSoftmax | n:  26
IntGELU    | n:  7
IntSoftmax | n:  26
IntGELU    | n:  7
IntSoftmax | n:  26
IntGELU    | n:  7
IntSoftmax | n:  26
IntGELU    | n:  7
.calib done
Test: [  0/391]	Time  3.787 ( 3.787)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.85
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=26, intgelu_exp_n=8
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=26, intgelu_exp_n=8, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  26
IntGELU    | n:  8
IntSoftmax | n:  26
IntGELU    | n:  8
IntSoftmax | n:  26
IntGELU    | n:  8
IntSoftmax | n:  26
IntGELU    | n:  8
IntSoftmax | n:  26
IntGELU    | n:  8
IntSoftmax | n:  26
IntGELU    | n:  8
IntSoftmax | n:  26
IntGELU    | n:  8
IntSoftmax | n:  26
IntGELU    | n:  8
IntSoftmax | n:  26
IntGELU    | n:  8
IntSoftmax | n:  26
IntGELU    | n:  8
IntSoftmax | n:  26
IntGELU    | n:  8
IntSoftmax | n:  26
IntGELU    | n:  8
.calib done
Test: [  0/391]	Time  3.853 ( 3.853)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.94
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=26, intgelu_exp_n=9
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=26, intgelu_exp_n=9, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  26
IntGELU    | n:  9
IntSoftmax | n:  26
IntGELU    | n:  9
IntSoftmax | n:  26
IntGELU    | n:  9
IntSoftmax | n:  26
IntGELU    | n:  9
IntSoftmax | n:  26
IntGELU    | n:  9
IntSoftmax | n:  26
IntGELU    | n:  9
IntSoftmax | n:  26
IntGELU    | n:  9
IntSoftmax | n:  26
IntGELU    | n:  9
IntSoftmax | n:  26
IntGELU    | n:  9
IntSoftmax | n:  26
IntGELU    | n:  9
IntSoftmax | n:  26
IntGELU    | n:  9
IntSoftmax | n:  26
IntGELU    | n:  9
.calib done
Test: [  0/391]	Time  3.865 ( 3.865)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.92
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=26, intgelu_exp_n=10
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=26, intgelu_exp_n=10, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  26
IntGELU    | n:  10
IntSoftmax | n:  26
IntGELU    | n:  10
IntSoftmax | n:  26
IntGELU    | n:  10
IntSoftmax | n:  26
IntGELU    | n:  10
IntSoftmax | n:  26
IntGELU    | n:  10
IntSoftmax | n:  26
IntGELU    | n:  10
IntSoftmax | n:  26
IntGELU    | n:  10
IntSoftmax | n:  26
IntGELU    | n:  10
IntSoftmax | n:  26
IntGELU    | n:  10
IntSoftmax | n:  26
IntGELU    | n:  10
IntSoftmax | n:  26
IntGELU    | n:  10
IntSoftmax | n:  26
IntGELU    | n:  10
.calib done
Test: [  0/391]	Time  3.843 ( 3.843)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.90
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=26, intgelu_exp_n=11
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=26, intgelu_exp_n=11, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  26
IntGELU    | n:  11
IntSoftmax | n:  26
IntGELU    | n:  11
IntSoftmax | n:  26
IntGELU    | n:  11
IntSoftmax | n:  26
IntGELU    | n:  11
IntSoftmax | n:  26
IntGELU    | n:  11
IntSoftmax | n:  26
IntGELU    | n:  11
IntSoftmax | n:  26
IntGELU    | n:  11
IntSoftmax | n:  26
IntGELU    | n:  11
IntSoftmax | n:  26
IntGELU    | n:  11
IntSoftmax | n:  26
IntGELU    | n:  11
IntSoftmax | n:  26
IntGELU    | n:  11
IntSoftmax | n:  26
IntGELU    | n:  11
.calib done
Test: [  0/391]	Time  3.917 ( 3.917)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=26, intgelu_exp_n=12
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=26, intgelu_exp_n=12, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  26
IntGELU    | n:  12
IntSoftmax | n:  26
IntGELU    | n:  12
IntSoftmax | n:  26
IntGELU    | n:  12
IntSoftmax | n:  26
IntGELU    | n:  12
IntSoftmax | n:  26
IntGELU    | n:  12
IntSoftmax | n:  26
IntGELU    | n:  12
IntSoftmax | n:  26
IntGELU    | n:  12
IntSoftmax | n:  26
IntGELU    | n:  12
IntSoftmax | n:  26
IntGELU    | n:  12
IntSoftmax | n:  26
IntGELU    | n:  12
IntSoftmax | n:  26
IntGELU    | n:  12
IntSoftmax | n:  26
IntGELU    | n:  12
.calib done
Test: [  0/391]	Time  3.821 ( 3.821)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.89
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=26, intgelu_exp_n=13
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=26, intgelu_exp_n=13, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  26
IntGELU    | n:  13
IntSoftmax | n:  26
IntGELU    | n:  13
IntSoftmax | n:  26
IntGELU    | n:  13
IntSoftmax | n:  26
IntGELU    | n:  13
IntSoftmax | n:  26
IntGELU    | n:  13
IntSoftmax | n:  26
IntGELU    | n:  13
IntSoftmax | n:  26
IntGELU    | n:  13
IntSoftmax | n:  26
IntGELU    | n:  13
IntSoftmax | n:  26
IntGELU    | n:  13
IntSoftmax | n:  26
IntGELU    | n:  13
IntSoftmax | n:  26
IntGELU    | n:  13
IntSoftmax | n:  26
IntGELU    | n:  13
.calib done
Test: [  0/391]	Time  3.693 ( 3.693)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.75
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=26, intgelu_exp_n=14
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=26, intgelu_exp_n=14, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  26
IntGELU    | n:  14
IntSoftmax | n:  26
IntGELU    | n:  14
IntSoftmax | n:  26
IntGELU    | n:  14
IntSoftmax | n:  26
IntGELU    | n:  14
IntSoftmax | n:  26
IntGELU    | n:  14
IntSoftmax | n:  26
IntGELU    | n:  14
IntSoftmax | n:  26
IntGELU    | n:  14
IntSoftmax | n:  26
IntGELU    | n:  14
IntSoftmax | n:  26
IntGELU    | n:  14
IntSoftmax | n:  26
IntGELU    | n:  14
IntSoftmax | n:  26
IntGELU    | n:  14
IntSoftmax | n:  26
IntGELU    | n:  14
.calib done
Test: [  0/391]	Time  3.879 ( 3.879)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.90
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=26, intgelu_exp_n=15
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=26, intgelu_exp_n=15, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  26
IntGELU    | n:  15
IntSoftmax | n:  26
IntGELU    | n:  15
IntSoftmax | n:  26
IntGELU    | n:  15
IntSoftmax | n:  26
IntGELU    | n:  15
IntSoftmax | n:  26
IntGELU    | n:  15
IntSoftmax | n:  26
IntGELU    | n:  15
IntSoftmax | n:  26
IntGELU    | n:  15
IntSoftmax | n:  26
IntGELU    | n:  15
IntSoftmax | n:  26
IntGELU    | n:  15
IntSoftmax | n:  26
IntGELU    | n:  15
IntSoftmax | n:  26
IntGELU    | n:  15
IntSoftmax | n:  26
IntGELU    | n:  15
.calib done
Test: [  0/391]	Time  4.025 ( 4.025)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.05
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=26, intgelu_exp_n=16
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=26, intgelu_exp_n=16, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  26
IntGELU    | n:  16
IntSoftmax | n:  26
IntGELU    | n:  16
IntSoftmax | n:  26
IntGELU    | n:  16
IntSoftmax | n:  26
IntGELU    | n:  16
IntSoftmax | n:  26
IntGELU    | n:  16
IntSoftmax | n:  26
IntGELU    | n:  16
IntSoftmax | n:  26
IntGELU    | n:  16
IntSoftmax | n:  26
IntGELU    | n:  16
IntSoftmax | n:  26
IntGELU    | n:  16
IntSoftmax | n:  26
IntGELU    | n:  16
IntSoftmax | n:  26
IntGELU    | n:  16
IntSoftmax | n:  26
IntGELU    | n:  16
.calib done
Test: [  0/391]	Time  3.952 ( 3.952)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.99
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=26, intgelu_exp_n=17
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=26, intgelu_exp_n=17, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  26
IntGELU    | n:  17
IntSoftmax | n:  26
IntGELU    | n:  17
IntSoftmax | n:  26
IntGELU    | n:  17
IntSoftmax | n:  26
IntGELU    | n:  17
IntSoftmax | n:  26
IntGELU    | n:  17
IntSoftmax | n:  26
IntGELU    | n:  17
IntSoftmax | n:  26
IntGELU    | n:  17
IntSoftmax | n:  26
IntGELU    | n:  17
IntSoftmax | n:  26
IntGELU    | n:  17
IntSoftmax | n:  26
IntGELU    | n:  17
IntSoftmax | n:  26
IntGELU    | n:  17
IntSoftmax | n:  26
IntGELU    | n:  17
.calib done
Test: [  0/391]	Time  4.004 ( 4.004)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.03
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=26, intgelu_exp_n=18
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=26, intgelu_exp_n=18, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  26
IntGELU    | n:  18
IntSoftmax | n:  26
IntGELU    | n:  18
IntSoftmax | n:  26
IntGELU    | n:  18
IntSoftmax | n:  26
IntGELU    | n:  18
IntSoftmax | n:  26
IntGELU    | n:  18
IntSoftmax | n:  26
IntGELU    | n:  18
IntSoftmax | n:  26
IntGELU    | n:  18
IntSoftmax | n:  26
IntGELU    | n:  18
IntSoftmax | n:  26
IntGELU    | n:  18
IntSoftmax | n:  26
IntGELU    | n:  18
IntSoftmax | n:  26
IntGELU    | n:  18
IntSoftmax | n:  26
IntGELU    | n:  18
.calib done
Test: [  0/391]	Time  3.908 ( 3.908)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.95
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=26, intgelu_exp_n=19
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=26, intgelu_exp_n=19, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  26
IntGELU    | n:  19
IntSoftmax | n:  26
IntGELU    | n:  19
IntSoftmax | n:  26
IntGELU    | n:  19
IntSoftmax | n:  26
IntGELU    | n:  19
IntSoftmax | n:  26
IntGELU    | n:  19
IntSoftmax | n:  26
IntGELU    | n:  19
IntSoftmax | n:  26
IntGELU    | n:  19
IntSoftmax | n:  26
IntGELU    | n:  19
IntSoftmax | n:  26
IntGELU    | n:  19
IntSoftmax | n:  26
IntGELU    | n:  19
IntSoftmax | n:  26
IntGELU    | n:  19
IntSoftmax | n:  26
IntGELU    | n:  19
.calib done
Test: [  0/391]	Time  4.075 ( 4.075)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.16
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=26, intgelu_exp_n=20
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=26, intgelu_exp_n=20, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  26
IntGELU    | n:  20
IntSoftmax | n:  26
IntGELU    | n:  20
IntSoftmax | n:  26
IntGELU    | n:  20
IntSoftmax | n:  26
IntGELU    | n:  20
IntSoftmax | n:  26
IntGELU    | n:  20
IntSoftmax | n:  26
IntGELU    | n:  20
IntSoftmax | n:  26
IntGELU    | n:  20
IntSoftmax | n:  26
IntGELU    | n:  20
IntSoftmax | n:  26
IntGELU    | n:  20
IntSoftmax | n:  26
IntGELU    | n:  20
IntSoftmax | n:  26
IntGELU    | n:  20
IntSoftmax | n:  26
IntGELU    | n:  20
.calib done
Test: [  0/391]	Time  3.961 ( 3.961)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.03
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=26, intgelu_exp_n=21
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=26, intgelu_exp_n=21, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  26
IntGELU    | n:  21
IntSoftmax | n:  26
IntGELU    | n:  21
IntSoftmax | n:  26
IntGELU    | n:  21
IntSoftmax | n:  26
IntGELU    | n:  21
IntSoftmax | n:  26
IntGELU    | n:  21
IntSoftmax | n:  26
IntGELU    | n:  21
IntSoftmax | n:  26
IntGELU    | n:  21
IntSoftmax | n:  26
IntGELU    | n:  21
IntSoftmax | n:  26
IntGELU    | n:  21
IntSoftmax | n:  26
IntGELU    | n:  21
IntSoftmax | n:  26
IntGELU    | n:  21
IntSoftmax | n:  26
IntGELU    | n:  21
.calib done
Test: [  0/391]	Time  4.070 ( 4.070)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.10
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=26, intgelu_exp_n=22
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=26, intgelu_exp_n=22, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  26
IntGELU    | n:  22
IntSoftmax | n:  26
IntGELU    | n:  22
IntSoftmax | n:  26
IntGELU    | n:  22
IntSoftmax | n:  26
IntGELU    | n:  22
IntSoftmax | n:  26
IntGELU    | n:  22
IntSoftmax | n:  26
IntGELU    | n:  22
IntSoftmax | n:  26
IntGELU    | n:  22
IntSoftmax | n:  26
IntGELU    | n:  22
IntSoftmax | n:  26
IntGELU    | n:  22
IntSoftmax | n:  26
IntGELU    | n:  22
IntSoftmax | n:  26
IntGELU    | n:  22
IntSoftmax | n:  26
IntGELU    | n:  22
.calib done
Test: [  0/391]	Time  3.745 ( 3.745)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.79
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=26, intgelu_exp_n=23
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=26, intgelu_exp_n=23, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  26
IntGELU    | n:  23
IntSoftmax | n:  26
IntGELU    | n:  23
IntSoftmax | n:  26
IntGELU    | n:  23
IntSoftmax | n:  26
IntGELU    | n:  23
IntSoftmax | n:  26
IntGELU    | n:  23
IntSoftmax | n:  26
IntGELU    | n:  23
IntSoftmax | n:  26
IntGELU    | n:  23
IntSoftmax | n:  26
IntGELU    | n:  23
IntSoftmax | n:  26
IntGELU    | n:  23
IntSoftmax | n:  26
IntGELU    | n:  23
IntSoftmax | n:  26
IntGELU    | n:  23
IntSoftmax | n:  26
IntGELU    | n:  23
.calib done
Test: [  0/391]	Time  3.869 ( 3.869)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.93
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=26, intgelu_exp_n=24
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=26, intgelu_exp_n=24, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  26
IntGELU    | n:  24
IntSoftmax | n:  26
IntGELU    | n:  24
IntSoftmax | n:  26
IntGELU    | n:  24
IntSoftmax | n:  26
IntGELU    | n:  24
IntSoftmax | n:  26
IntGELU    | n:  24
IntSoftmax | n:  26
IntGELU    | n:  24
IntSoftmax | n:  26
IntGELU    | n:  24
IntSoftmax | n:  26
IntGELU    | n:  24
IntSoftmax | n:  26
IntGELU    | n:  24
IntSoftmax | n:  26
IntGELU    | n:  24
IntSoftmax | n:  26
IntGELU    | n:  24
IntSoftmax | n:  26
IntGELU    | n:  24
.calib done
Test: [  0/391]	Time  4.058 ( 4.058)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.13
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=26, intgelu_exp_n=25
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=26, intgelu_exp_n=25, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  26
IntGELU    | n:  25
IntSoftmax | n:  26
IntGELU    | n:  25
IntSoftmax | n:  26
IntGELU    | n:  25
IntSoftmax | n:  26
IntGELU    | n:  25
IntSoftmax | n:  26
IntGELU    | n:  25
IntSoftmax | n:  26
IntGELU    | n:  25
IntSoftmax | n:  26
IntGELU    | n:  25
IntSoftmax | n:  26
IntGELU    | n:  25
IntSoftmax | n:  26
IntGELU    | n:  25
IntSoftmax | n:  26
IntGELU    | n:  25
IntSoftmax | n:  26
IntGELU    | n:  25
IntSoftmax | n:  26
IntGELU    | n:  25
.calib done
Test: [  0/391]	Time  3.872 ( 3.872)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=26, intgelu_exp_n=26
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=26, intgelu_exp_n=26, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  26
IntGELU    | n:  26
IntSoftmax | n:  26
IntGELU    | n:  26
IntSoftmax | n:  26
IntGELU    | n:  26
IntSoftmax | n:  26
IntGELU    | n:  26
IntSoftmax | n:  26
IntGELU    | n:  26
IntSoftmax | n:  26
IntGELU    | n:  26
IntSoftmax | n:  26
IntGELU    | n:  26
IntSoftmax | n:  26
IntGELU    | n:  26
IntSoftmax | n:  26
IntGELU    | n:  26
IntSoftmax | n:  26
IntGELU    | n:  26
IntSoftmax | n:  26
IntGELU    | n:  26
IntSoftmax | n:  26
IntGELU    | n:  26
.calib done
Test: [  0/391]	Time  3.872 ( 3.872)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.93
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=26, intgelu_exp_n=27
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=26, intgelu_exp_n=27, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  26
IntGELU    | n:  27
IntSoftmax | n:  26
IntGELU    | n:  27
IntSoftmax | n:  26
IntGELU    | n:  27
IntSoftmax | n:  26
IntGELU    | n:  27
IntSoftmax | n:  26
IntGELU    | n:  27
IntSoftmax | n:  26
IntGELU    | n:  27
IntSoftmax | n:  26
IntGELU    | n:  27
IntSoftmax | n:  26
IntGELU    | n:  27
IntSoftmax | n:  26
IntGELU    | n:  27
IntSoftmax | n:  26
IntGELU    | n:  27
IntSoftmax | n:  26
IntGELU    | n:  27
IntSoftmax | n:  26
IntGELU    | n:  27
.calib done
Test: [  0/391]	Time  3.882 ( 3.882)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.91
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=26, intgelu_exp_n=28
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=26, intgelu_exp_n=28, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  26
IntGELU    | n:  28
IntSoftmax | n:  26
IntGELU    | n:  28
IntSoftmax | n:  26
IntGELU    | n:  28
IntSoftmax | n:  26
IntGELU    | n:  28
IntSoftmax | n:  26
IntGELU    | n:  28
IntSoftmax | n:  26
IntGELU    | n:  28
IntSoftmax | n:  26
IntGELU    | n:  28
IntSoftmax | n:  26
IntGELU    | n:  28
IntSoftmax | n:  26
IntGELU    | n:  28
IntSoftmax | n:  26
IntGELU    | n:  28
IntSoftmax | n:  26
IntGELU    | n:  28
IntSoftmax | n:  26
IntGELU    | n:  28
.calib done
Test: [  0/391]	Time  3.799 ( 3.799)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.84
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=26, intgelu_exp_n=29
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=26, intgelu_exp_n=29, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  26
IntGELU    | n:  29
IntSoftmax | n:  26
IntGELU    | n:  29
IntSoftmax | n:  26
IntGELU    | n:  29
IntSoftmax | n:  26
IntGELU    | n:  29
IntSoftmax | n:  26
IntGELU    | n:  29
IntSoftmax | n:  26
IntGELU    | n:  29
IntSoftmax | n:  26
IntGELU    | n:  29
IntSoftmax | n:  26
IntGELU    | n:  29
IntSoftmax | n:  26
IntGELU    | n:  29
IntSoftmax | n:  26
IntGELU    | n:  29
IntSoftmax | n:  26
IntGELU    | n:  29
IntSoftmax | n:  26
IntGELU    | n:  29
.calib done
Test: [  0/391]	Time  3.811 ( 3.811)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.91
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=26, intgelu_exp_n=30
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=26, intgelu_exp_n=30, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  26
IntGELU    | n:  30
IntSoftmax | n:  26
IntGELU    | n:  30
IntSoftmax | n:  26
IntGELU    | n:  30
IntSoftmax | n:  26
IntGELU    | n:  30
IntSoftmax | n:  26
IntGELU    | n:  30
IntSoftmax | n:  26
IntGELU    | n:  30
IntSoftmax | n:  26
IntGELU    | n:  30
IntSoftmax | n:  26
IntGELU    | n:  30
IntSoftmax | n:  26
IntGELU    | n:  30
IntSoftmax | n:  26
IntGELU    | n:  30
IntSoftmax | n:  26
IntGELU    | n:  30
IntSoftmax | n:  26
IntGELU    | n:  30
.calib done
Test: [  0/391]	Time  3.982 ( 3.982)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.03
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=26, intgelu_exp_n=31
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=26, intgelu_exp_n=31, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  26
IntGELU    | n:  31
IntSoftmax | n:  26
IntGELU    | n:  31
IntSoftmax | n:  26
IntGELU    | n:  31
IntSoftmax | n:  26
IntGELU    | n:  31
IntSoftmax | n:  26
IntGELU    | n:  31
IntSoftmax | n:  26
IntGELU    | n:  31
IntSoftmax | n:  26
IntGELU    | n:  31
IntSoftmax | n:  26
IntGELU    | n:  31
IntSoftmax | n:  26
IntGELU    | n:  31
IntSoftmax | n:  26
IntGELU    | n:  31
IntSoftmax | n:  26
IntGELU    | n:  31
IntSoftmax | n:  26
IntGELU    | n:  31
.calib done
Test: [  0/391]	Time  3.739 ( 3.739)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.78
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=27, intgelu_exp_n=0
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=27, intgelu_exp_n=0, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  27
IntGELU    | n:  0
IntSoftmax | n:  27
IntGELU    | n:  0
IntSoftmax | n:  27
IntGELU    | n:  0
IntSoftmax | n:  27
IntGELU    | n:  0
IntSoftmax | n:  27
IntGELU    | n:  0
IntSoftmax | n:  27
IntGELU    | n:  0
IntSoftmax | n:  27
IntGELU    | n:  0
IntSoftmax | n:  27
IntGELU    | n:  0
IntSoftmax | n:  27
IntGELU    | n:  0
IntSoftmax | n:  27
IntGELU    | n:  0
IntSoftmax | n:  27
IntGELU    | n:  0
IntSoftmax | n:  27
IntGELU    | n:  0
.calib done
Test: [  0/391]	Time  3.951 ( 3.951)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.03
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=27, intgelu_exp_n=1
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=27, intgelu_exp_n=1, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  27
IntGELU    | n:  1
IntSoftmax | n:  27
IntGELU    | n:  1
IntSoftmax | n:  27
IntGELU    | n:  1
IntSoftmax | n:  27
IntGELU    | n:  1
IntSoftmax | n:  27
IntGELU    | n:  1
IntSoftmax | n:  27
IntGELU    | n:  1
IntSoftmax | n:  27
IntGELU    | n:  1
IntSoftmax | n:  27
IntGELU    | n:  1
IntSoftmax | n:  27
IntGELU    | n:  1
IntSoftmax | n:  27
IntGELU    | n:  1
IntSoftmax | n:  27
IntGELU    | n:  1
IntSoftmax | n:  27
IntGELU    | n:  1
.calib done
Test: [  0/391]	Time  3.914 ( 3.914)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.00
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=27, intgelu_exp_n=2
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=27, intgelu_exp_n=2, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  27
IntGELU    | n:  2
IntSoftmax | n:  27
IntGELU    | n:  2
IntSoftmax | n:  27
IntGELU    | n:  2
IntSoftmax | n:  27
IntGELU    | n:  2
IntSoftmax | n:  27
IntGELU    | n:  2
IntSoftmax | n:  27
IntGELU    | n:  2
IntSoftmax | n:  27
IntGELU    | n:  2
IntSoftmax | n:  27
IntGELU    | n:  2
IntSoftmax | n:  27
IntGELU    | n:  2
IntSoftmax | n:  27
IntGELU    | n:  2
IntSoftmax | n:  27
IntGELU    | n:  2
IntSoftmax | n:  27
IntGELU    | n:  2
.calib done
Test: [  0/391]	Time  3.916 ( 3.916)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.94
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=27, intgelu_exp_n=3
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=27, intgelu_exp_n=3, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  27
IntGELU    | n:  3
IntSoftmax | n:  27
IntGELU    | n:  3
IntSoftmax | n:  27
IntGELU    | n:  3
IntSoftmax | n:  27
IntGELU    | n:  3
IntSoftmax | n:  27
IntGELU    | n:  3
IntSoftmax | n:  27
IntGELU    | n:  3
IntSoftmax | n:  27
IntGELU    | n:  3
IntSoftmax | n:  27
IntGELU    | n:  3
IntSoftmax | n:  27
IntGELU    | n:  3
IntSoftmax | n:  27
IntGELU    | n:  3
IntSoftmax | n:  27
IntGELU    | n:  3
IntSoftmax | n:  27
IntGELU    | n:  3
.calib done
Test: [  0/391]	Time  3.796 ( 3.796)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.84
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=27, intgelu_exp_n=4
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=27, intgelu_exp_n=4, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  27
IntGELU    | n:  4
IntSoftmax | n:  27
IntGELU    | n:  4
IntSoftmax | n:  27
IntGELU    | n:  4
IntSoftmax | n:  27
IntGELU    | n:  4
IntSoftmax | n:  27
IntGELU    | n:  4
IntSoftmax | n:  27
IntGELU    | n:  4
IntSoftmax | n:  27
IntGELU    | n:  4
IntSoftmax | n:  27
IntGELU    | n:  4
IntSoftmax | n:  27
IntGELU    | n:  4
IntSoftmax | n:  27
IntGELU    | n:  4
IntSoftmax | n:  27
IntGELU    | n:  4
IntSoftmax | n:  27
IntGELU    | n:  4
.calib done
Test: [  0/391]	Time  3.899 ( 3.899)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.94
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=27, intgelu_exp_n=5
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=27, intgelu_exp_n=5, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  27
IntGELU    | n:  5
IntSoftmax | n:  27
IntGELU    | n:  5
IntSoftmax | n:  27
IntGELU    | n:  5
IntSoftmax | n:  27
IntGELU    | n:  5
IntSoftmax | n:  27
IntGELU    | n:  5
IntSoftmax | n:  27
IntGELU    | n:  5
IntSoftmax | n:  27
IntGELU    | n:  5
IntSoftmax | n:  27
IntGELU    | n:  5
IntSoftmax | n:  27
IntGELU    | n:  5
IntSoftmax | n:  27
IntGELU    | n:  5
IntSoftmax | n:  27
IntGELU    | n:  5
IntSoftmax | n:  27
IntGELU    | n:  5
.calib done
Test: [  0/391]	Time  3.862 ( 3.862)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.90
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=27, intgelu_exp_n=6
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=27, intgelu_exp_n=6, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  27
IntGELU    | n:  6
IntSoftmax | n:  27
IntGELU    | n:  6
IntSoftmax | n:  27
IntGELU    | n:  6
IntSoftmax | n:  27
IntGELU    | n:  6
IntSoftmax | n:  27
IntGELU    | n:  6
IntSoftmax | n:  27
IntGELU    | n:  6
IntSoftmax | n:  27
IntGELU    | n:  6
IntSoftmax | n:  27
IntGELU    | n:  6
IntSoftmax | n:  27
IntGELU    | n:  6
IntSoftmax | n:  27
IntGELU    | n:  6
IntSoftmax | n:  27
IntGELU    | n:  6
IntSoftmax | n:  27
IntGELU    | n:  6
.calib done
Test: [  0/391]	Time  3.880 ( 3.880)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.95
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=27, intgelu_exp_n=7
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=27, intgelu_exp_n=7, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  27
IntGELU    | n:  7
IntSoftmax | n:  27
IntGELU    | n:  7
IntSoftmax | n:  27
IntGELU    | n:  7
IntSoftmax | n:  27
IntGELU    | n:  7
IntSoftmax | n:  27
IntGELU    | n:  7
IntSoftmax | n:  27
IntGELU    | n:  7
IntSoftmax | n:  27
IntGELU    | n:  7
IntSoftmax | n:  27
IntGELU    | n:  7
IntSoftmax | n:  27
IntGELU    | n:  7
IntSoftmax | n:  27
IntGELU    | n:  7
IntSoftmax | n:  27
IntGELU    | n:  7
IntSoftmax | n:  27
IntGELU    | n:  7
.calib done
Test: [  0/391]	Time  3.966 ( 3.966)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.01
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=27, intgelu_exp_n=8
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=27, intgelu_exp_n=8, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  27
IntGELU    | n:  8
IntSoftmax | n:  27
IntGELU    | n:  8
IntSoftmax | n:  27
IntGELU    | n:  8
IntSoftmax | n:  27
IntGELU    | n:  8
IntSoftmax | n:  27
IntGELU    | n:  8
IntSoftmax | n:  27
IntGELU    | n:  8
IntSoftmax | n:  27
IntGELU    | n:  8
IntSoftmax | n:  27
IntGELU    | n:  8
IntSoftmax | n:  27
IntGELU    | n:  8
IntSoftmax | n:  27
IntGELU    | n:  8
IntSoftmax | n:  27
IntGELU    | n:  8
IntSoftmax | n:  27
IntGELU    | n:  8
.calib done
Test: [  0/391]	Time  3.859 ( 3.859)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.90
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=27, intgelu_exp_n=9
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=27, intgelu_exp_n=9, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  27
IntGELU    | n:  9
IntSoftmax | n:  27
IntGELU    | n:  9
IntSoftmax | n:  27
IntGELU    | n:  9
IntSoftmax | n:  27
IntGELU    | n:  9
IntSoftmax | n:  27
IntGELU    | n:  9
IntSoftmax | n:  27
IntGELU    | n:  9
IntSoftmax | n:  27
IntGELU    | n:  9
IntSoftmax | n:  27
IntGELU    | n:  9
IntSoftmax | n:  27
IntGELU    | n:  9
IntSoftmax | n:  27
IntGELU    | n:  9
IntSoftmax | n:  27
IntGELU    | n:  9
IntSoftmax | n:  27
IntGELU    | n:  9
.calib done
Test: [  0/391]	Time  4.009 ( 4.009)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.03
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=27, intgelu_exp_n=10
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=27, intgelu_exp_n=10, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  27
IntGELU    | n:  10
IntSoftmax | n:  27
IntGELU    | n:  10
IntSoftmax | n:  27
IntGELU    | n:  10
IntSoftmax | n:  27
IntGELU    | n:  10
IntSoftmax | n:  27
IntGELU    | n:  10
IntSoftmax | n:  27
IntGELU    | n:  10
IntSoftmax | n:  27
IntGELU    | n:  10
IntSoftmax | n:  27
IntGELU    | n:  10
IntSoftmax | n:  27
IntGELU    | n:  10
IntSoftmax | n:  27
IntGELU    | n:  10
IntSoftmax | n:  27
IntGELU    | n:  10
IntSoftmax | n:  27
IntGELU    | n:  10
.calib done
Test: [  0/391]	Time  3.837 ( 3.837)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.91
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=27, intgelu_exp_n=11
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=27, intgelu_exp_n=11, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  27
IntGELU    | n:  11
IntSoftmax | n:  27
IntGELU    | n:  11
IntSoftmax | n:  27
IntGELU    | n:  11
IntSoftmax | n:  27
IntGELU    | n:  11
IntSoftmax | n:  27
IntGELU    | n:  11
IntSoftmax | n:  27
IntGELU    | n:  11
IntSoftmax | n:  27
IntGELU    | n:  11
IntSoftmax | n:  27
IntGELU    | n:  11
IntSoftmax | n:  27
IntGELU    | n:  11
IntSoftmax | n:  27
IntGELU    | n:  11
IntSoftmax | n:  27
IntGELU    | n:  11
IntSoftmax | n:  27
IntGELU    | n:  11
.calib done
Test: [  0/391]	Time  3.783 ( 3.783)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.84
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=27, intgelu_exp_n=12
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=27, intgelu_exp_n=12, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  27
IntGELU    | n:  12
IntSoftmax | n:  27
IntGELU    | n:  12
IntSoftmax | n:  27
IntGELU    | n:  12
IntSoftmax | n:  27
IntGELU    | n:  12
IntSoftmax | n:  27
IntGELU    | n:  12
IntSoftmax | n:  27
IntGELU    | n:  12
IntSoftmax | n:  27
IntGELU    | n:  12
IntSoftmax | n:  27
IntGELU    | n:  12
IntSoftmax | n:  27
IntGELU    | n:  12
IntSoftmax | n:  27
IntGELU    | n:  12
IntSoftmax | n:  27
IntGELU    | n:  12
IntSoftmax | n:  27
IntGELU    | n:  12
.calib done
Test: [  0/391]	Time  3.879 ( 3.879)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.93
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=27, intgelu_exp_n=13
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=27, intgelu_exp_n=13, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  27
IntGELU    | n:  13
IntSoftmax | n:  27
IntGELU    | n:  13
IntSoftmax | n:  27
IntGELU    | n:  13
IntSoftmax | n:  27
IntGELU    | n:  13
IntSoftmax | n:  27
IntGELU    | n:  13
IntSoftmax | n:  27
IntGELU    | n:  13
IntSoftmax | n:  27
IntGELU    | n:  13
IntSoftmax | n:  27
IntGELU    | n:  13
IntSoftmax | n:  27
IntGELU    | n:  13
IntSoftmax | n:  27
IntGELU    | n:  13
IntSoftmax | n:  27
IntGELU    | n:  13
IntSoftmax | n:  27
IntGELU    | n:  13
.calib done
Test: [  0/391]	Time  4.026 ( 4.026)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.04
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=27, intgelu_exp_n=14
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=27, intgelu_exp_n=14, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  27
IntGELU    | n:  14
IntSoftmax | n:  27
IntGELU    | n:  14
IntSoftmax | n:  27
IntGELU    | n:  14
IntSoftmax | n:  27
IntGELU    | n:  14
IntSoftmax | n:  27
IntGELU    | n:  14
IntSoftmax | n:  27
IntGELU    | n:  14
IntSoftmax | n:  27
IntGELU    | n:  14
IntSoftmax | n:  27
IntGELU    | n:  14
IntSoftmax | n:  27
IntGELU    | n:  14
IntSoftmax | n:  27
IntGELU    | n:  14
IntSoftmax | n:  27
IntGELU    | n:  14
IntSoftmax | n:  27
IntGELU    | n:  14
.calib done
Test: [  0/391]	Time  3.735 ( 3.735)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.84
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=27, intgelu_exp_n=15
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=27, intgelu_exp_n=15, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  27
IntGELU    | n:  15
IntSoftmax | n:  27
IntGELU    | n:  15
IntSoftmax | n:  27
IntGELU    | n:  15
IntSoftmax | n:  27
IntGELU    | n:  15
IntSoftmax | n:  27
IntGELU    | n:  15
IntSoftmax | n:  27
IntGELU    | n:  15
IntSoftmax | n:  27
IntGELU    | n:  15
IntSoftmax | n:  27
IntGELU    | n:  15
IntSoftmax | n:  27
IntGELU    | n:  15
IntSoftmax | n:  27
IntGELU    | n:  15
IntSoftmax | n:  27
IntGELU    | n:  15
IntSoftmax | n:  27
IntGELU    | n:  15
.calib done
Test: [  0/391]	Time  3.843 ( 3.843)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.86
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=27, intgelu_exp_n=16
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=27, intgelu_exp_n=16, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  27
IntGELU    | n:  16
IntSoftmax | n:  27
IntGELU    | n:  16
IntSoftmax | n:  27
IntGELU    | n:  16
IntSoftmax | n:  27
IntGELU    | n:  16
IntSoftmax | n:  27
IntGELU    | n:  16
IntSoftmax | n:  27
IntGELU    | n:  16
IntSoftmax | n:  27
IntGELU    | n:  16
IntSoftmax | n:  27
IntGELU    | n:  16
IntSoftmax | n:  27
IntGELU    | n:  16
IntSoftmax | n:  27
IntGELU    | n:  16
IntSoftmax | n:  27
IntGELU    | n:  16
IntSoftmax | n:  27
IntGELU    | n:  16
.calib done
Test: [  0/391]	Time  3.965 ( 3.965)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=27, intgelu_exp_n=17
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=27, intgelu_exp_n=17, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  27
IntGELU    | n:  17
IntSoftmax | n:  27
IntGELU    | n:  17
IntSoftmax | n:  27
IntGELU    | n:  17
IntSoftmax | n:  27
IntGELU    | n:  17
IntSoftmax | n:  27
IntGELU    | n:  17
IntSoftmax | n:  27
IntGELU    | n:  17
IntSoftmax | n:  27
IntGELU    | n:  17
IntSoftmax | n:  27
IntGELU    | n:  17
IntSoftmax | n:  27
IntGELU    | n:  17
IntSoftmax | n:  27
IntGELU    | n:  17
IntSoftmax | n:  27
IntGELU    | n:  17
IntSoftmax | n:  27
IntGELU    | n:  17
.calib done
Test: [  0/391]	Time  3.680 ( 3.680)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.70
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=27, intgelu_exp_n=18
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=27, intgelu_exp_n=18, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  27
IntGELU    | n:  18
IntSoftmax | n:  27
IntGELU    | n:  18
IntSoftmax | n:  27
IntGELU    | n:  18
IntSoftmax | n:  27
IntGELU    | n:  18
IntSoftmax | n:  27
IntGELU    | n:  18
IntSoftmax | n:  27
IntGELU    | n:  18
IntSoftmax | n:  27
IntGELU    | n:  18
IntSoftmax | n:  27
IntGELU    | n:  18
IntSoftmax | n:  27
IntGELU    | n:  18
IntSoftmax | n:  27
IntGELU    | n:  18
IntSoftmax | n:  27
IntGELU    | n:  18
IntSoftmax | n:  27
IntGELU    | n:  18
.calib done
Test: [  0/391]	Time  4.003 ( 4.003)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.09
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=27, intgelu_exp_n=19
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=27, intgelu_exp_n=19, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  27
IntGELU    | n:  19
IntSoftmax | n:  27
IntGELU    | n:  19
IntSoftmax | n:  27
IntGELU    | n:  19
IntSoftmax | n:  27
IntGELU    | n:  19
IntSoftmax | n:  27
IntGELU    | n:  19
IntSoftmax | n:  27
IntGELU    | n:  19
IntSoftmax | n:  27
IntGELU    | n:  19
IntSoftmax | n:  27
IntGELU    | n:  19
IntSoftmax | n:  27
IntGELU    | n:  19
IntSoftmax | n:  27
IntGELU    | n:  19
IntSoftmax | n:  27
IntGELU    | n:  19
IntSoftmax | n:  27
IntGELU    | n:  19
.calib done
Test: [  0/391]	Time  3.880 ( 3.880)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.94
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=27, intgelu_exp_n=20
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=27, intgelu_exp_n=20, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  27
IntGELU    | n:  20
IntSoftmax | n:  27
IntGELU    | n:  20
IntSoftmax | n:  27
IntGELU    | n:  20
IntSoftmax | n:  27
IntGELU    | n:  20
IntSoftmax | n:  27
IntGELU    | n:  20
IntSoftmax | n:  27
IntGELU    | n:  20
IntSoftmax | n:  27
IntGELU    | n:  20
IntSoftmax | n:  27
IntGELU    | n:  20
IntSoftmax | n:  27
IntGELU    | n:  20
IntSoftmax | n:  27
IntGELU    | n:  20
IntSoftmax | n:  27
IntGELU    | n:  20
IntSoftmax | n:  27
IntGELU    | n:  20
.calib done
Test: [  0/391]	Time  4.001 ( 4.001)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.03
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=27, intgelu_exp_n=21
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=27, intgelu_exp_n=21, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  27
IntGELU    | n:  21
IntSoftmax | n:  27
IntGELU    | n:  21
IntSoftmax | n:  27
IntGELU    | n:  21
IntSoftmax | n:  27
IntGELU    | n:  21
IntSoftmax | n:  27
IntGELU    | n:  21
IntSoftmax | n:  27
IntGELU    | n:  21
IntSoftmax | n:  27
IntGELU    | n:  21
IntSoftmax | n:  27
IntGELU    | n:  21
IntSoftmax | n:  27
IntGELU    | n:  21
IntSoftmax | n:  27
IntGELU    | n:  21
IntSoftmax | n:  27
IntGELU    | n:  21
IntSoftmax | n:  27
IntGELU    | n:  21
.calib done
Test: [  0/391]	Time  3.896 ( 3.896)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=27, intgelu_exp_n=22
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=27, intgelu_exp_n=22, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  27
IntGELU    | n:  22
IntSoftmax | n:  27
IntGELU    | n:  22
IntSoftmax | n:  27
IntGELU    | n:  22
IntSoftmax | n:  27
IntGELU    | n:  22
IntSoftmax | n:  27
IntGELU    | n:  22
IntSoftmax | n:  27
IntGELU    | n:  22
IntSoftmax | n:  27
IntGELU    | n:  22
IntSoftmax | n:  27
IntGELU    | n:  22
IntSoftmax | n:  27
IntGELU    | n:  22
IntSoftmax | n:  27
IntGELU    | n:  22
IntSoftmax | n:  27
IntGELU    | n:  22
IntSoftmax | n:  27
IntGELU    | n:  22
.calib done
Test: [  0/391]	Time  3.887 ( 3.887)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.89
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=27, intgelu_exp_n=23
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=27, intgelu_exp_n=23, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  27
IntGELU    | n:  23
IntSoftmax | n:  27
IntGELU    | n:  23
IntSoftmax | n:  27
IntGELU    | n:  23
IntSoftmax | n:  27
IntGELU    | n:  23
IntSoftmax | n:  27
IntGELU    | n:  23
IntSoftmax | n:  27
IntGELU    | n:  23
IntSoftmax | n:  27
IntGELU    | n:  23
IntSoftmax | n:  27
IntGELU    | n:  23
IntSoftmax | n:  27
IntGELU    | n:  23
IntSoftmax | n:  27
IntGELU    | n:  23
IntSoftmax | n:  27
IntGELU    | n:  23
IntSoftmax | n:  27
IntGELU    | n:  23
.calib done
Test: [  0/391]	Time  3.853 ( 3.853)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.94
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=27, intgelu_exp_n=24
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=27, intgelu_exp_n=24, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  27
IntGELU    | n:  24
IntSoftmax | n:  27
IntGELU    | n:  24
IntSoftmax | n:  27
IntGELU    | n:  24
IntSoftmax | n:  27
IntGELU    | n:  24
IntSoftmax | n:  27
IntGELU    | n:  24
IntSoftmax | n:  27
IntGELU    | n:  24
IntSoftmax | n:  27
IntGELU    | n:  24
IntSoftmax | n:  27
IntGELU    | n:  24
IntSoftmax | n:  27
IntGELU    | n:  24
IntSoftmax | n:  27
IntGELU    | n:  24
IntSoftmax | n:  27
IntGELU    | n:  24
IntSoftmax | n:  27
IntGELU    | n:  24
.calib done
Test: [  0/391]	Time  3.847 ( 3.847)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.88
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=27, intgelu_exp_n=25
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=27, intgelu_exp_n=25, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  27
IntGELU    | n:  25
IntSoftmax | n:  27
IntGELU    | n:  25
IntSoftmax | n:  27
IntGELU    | n:  25
IntSoftmax | n:  27
IntGELU    | n:  25
IntSoftmax | n:  27
IntGELU    | n:  25
IntSoftmax | n:  27
IntGELU    | n:  25
IntSoftmax | n:  27
IntGELU    | n:  25
IntSoftmax | n:  27
IntGELU    | n:  25
IntSoftmax | n:  27
IntGELU    | n:  25
IntSoftmax | n:  27
IntGELU    | n:  25
IntSoftmax | n:  27
IntGELU    | n:  25
IntSoftmax | n:  27
IntGELU    | n:  25
.calib done
Test: [  0/391]	Time  3.867 ( 3.867)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=27, intgelu_exp_n=26
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=27, intgelu_exp_n=26, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  27
IntGELU    | n:  26
IntSoftmax | n:  27
IntGELU    | n:  26
IntSoftmax | n:  27
IntGELU    | n:  26
IntSoftmax | n:  27
IntGELU    | n:  26
IntSoftmax | n:  27
IntGELU    | n:  26
IntSoftmax | n:  27
IntGELU    | n:  26
IntSoftmax | n:  27
IntGELU    | n:  26
IntSoftmax | n:  27
IntGELU    | n:  26
IntSoftmax | n:  27
IntGELU    | n:  26
IntSoftmax | n:  27
IntGELU    | n:  26
IntSoftmax | n:  27
IntGELU    | n:  26
IntSoftmax | n:  27
IntGELU    | n:  26
.calib done
Test: [  0/391]	Time  3.970 ( 3.970)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.04
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=27, intgelu_exp_n=27
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=27, intgelu_exp_n=27, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  27
IntGELU    | n:  27
IntSoftmax | n:  27
IntGELU    | n:  27
IntSoftmax | n:  27
IntGELU    | n:  27
IntSoftmax | n:  27
IntGELU    | n:  27
IntSoftmax | n:  27
IntGELU    | n:  27
IntSoftmax | n:  27
IntGELU    | n:  27
IntSoftmax | n:  27
IntGELU    | n:  27
IntSoftmax | n:  27
IntGELU    | n:  27
IntSoftmax | n:  27
IntGELU    | n:  27
IntSoftmax | n:  27
IntGELU    | n:  27
IntSoftmax | n:  27
IntGELU    | n:  27
IntSoftmax | n:  27
IntGELU    | n:  27
.calib done
Test: [  0/391]	Time  3.963 ( 3.963)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.02
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=27, intgelu_exp_n=28
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=27, intgelu_exp_n=28, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  27
IntGELU    | n:  28
IntSoftmax | n:  27
IntGELU    | n:  28
IntSoftmax | n:  27
IntGELU    | n:  28
IntSoftmax | n:  27
IntGELU    | n:  28
IntSoftmax | n:  27
IntGELU    | n:  28
IntSoftmax | n:  27
IntGELU    | n:  28
IntSoftmax | n:  27
IntGELU    | n:  28
IntSoftmax | n:  27
IntGELU    | n:  28
IntSoftmax | n:  27
IntGELU    | n:  28
IntSoftmax | n:  27
IntGELU    | n:  28
IntSoftmax | n:  27
IntGELU    | n:  28
IntSoftmax | n:  27
IntGELU    | n:  28
.calib done
Test: [  0/391]	Time  3.930 ( 3.930)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=27, intgelu_exp_n=29
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=27, intgelu_exp_n=29, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  27
IntGELU    | n:  29
IntSoftmax | n:  27
IntGELU    | n:  29
IntSoftmax | n:  27
IntGELU    | n:  29
IntSoftmax | n:  27
IntGELU    | n:  29
IntSoftmax | n:  27
IntGELU    | n:  29
IntSoftmax | n:  27
IntGELU    | n:  29
IntSoftmax | n:  27
IntGELU    | n:  29
IntSoftmax | n:  27
IntGELU    | n:  29
IntSoftmax | n:  27
IntGELU    | n:  29
IntSoftmax | n:  27
IntGELU    | n:  29
IntSoftmax | n:  27
IntGELU    | n:  29
IntSoftmax | n:  27
IntGELU    | n:  29
.calib done
Test: [  0/391]	Time  3.890 ( 3.890)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.99
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=27, intgelu_exp_n=30
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=27, intgelu_exp_n=30, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  27
IntGELU    | n:  30
IntSoftmax | n:  27
IntGELU    | n:  30
IntSoftmax | n:  27
IntGELU    | n:  30
IntSoftmax | n:  27
IntGELU    | n:  30
IntSoftmax | n:  27
IntGELU    | n:  30
IntSoftmax | n:  27
IntGELU    | n:  30
IntSoftmax | n:  27
IntGELU    | n:  30
IntSoftmax | n:  27
IntGELU    | n:  30
IntSoftmax | n:  27
IntGELU    | n:  30
IntSoftmax | n:  27
IntGELU    | n:  30
IntSoftmax | n:  27
IntGELU    | n:  30
IntSoftmax | n:  27
IntGELU    | n:  30
.calib done
Test: [  0/391]	Time  3.683 ( 3.683)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.77
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=27, intgelu_exp_n=31
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=27, intgelu_exp_n=31, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  27
IntGELU    | n:  31
IntSoftmax | n:  27
IntGELU    | n:  31
IntSoftmax | n:  27
IntGELU    | n:  31
IntSoftmax | n:  27
IntGELU    | n:  31
IntSoftmax | n:  27
IntGELU    | n:  31
IntSoftmax | n:  27
IntGELU    | n:  31
IntSoftmax | n:  27
IntGELU    | n:  31
IntSoftmax | n:  27
IntGELU    | n:  31
IntSoftmax | n:  27
IntGELU    | n:  31
IntSoftmax | n:  27
IntGELU    | n:  31
IntSoftmax | n:  27
IntGELU    | n:  31
IntSoftmax | n:  27
IntGELU    | n:  31
.calib done
Test: [  0/391]	Time  3.935 ( 3.935)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.99
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=28, intgelu_exp_n=0
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=28, intgelu_exp_n=0, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  28
IntGELU    | n:  0
IntSoftmax | n:  28
IntGELU    | n:  0
IntSoftmax | n:  28
IntGELU    | n:  0
IntSoftmax | n:  28
IntGELU    | n:  0
IntSoftmax | n:  28
IntGELU    | n:  0
IntSoftmax | n:  28
IntGELU    | n:  0
IntSoftmax | n:  28
IntGELU    | n:  0
IntSoftmax | n:  28
IntGELU    | n:  0
IntSoftmax | n:  28
IntGELU    | n:  0
IntSoftmax | n:  28
IntGELU    | n:  0
IntSoftmax | n:  28
IntGELU    | n:  0
IntSoftmax | n:  28
IntGELU    | n:  0
.calib done
Test: [  0/391]	Time  3.860 ( 3.860)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.95
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=28, intgelu_exp_n=1
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=28, intgelu_exp_n=1, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  28
IntGELU    | n:  1
IntSoftmax | n:  28
IntGELU    | n:  1
IntSoftmax | n:  28
IntGELU    | n:  1
IntSoftmax | n:  28
IntGELU    | n:  1
IntSoftmax | n:  28
IntGELU    | n:  1
IntSoftmax | n:  28
IntGELU    | n:  1
IntSoftmax | n:  28
IntGELU    | n:  1
IntSoftmax | n:  28
IntGELU    | n:  1
IntSoftmax | n:  28
IntGELU    | n:  1
IntSoftmax | n:  28
IntGELU    | n:  1
IntSoftmax | n:  28
IntGELU    | n:  1
IntSoftmax | n:  28
IntGELU    | n:  1
.calib done
Test: [  0/391]	Time  3.904 ( 3.904)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.89
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=28, intgelu_exp_n=2
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=28, intgelu_exp_n=2, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  28
IntGELU    | n:  2
IntSoftmax | n:  28
IntGELU    | n:  2
IntSoftmax | n:  28
IntGELU    | n:  2
IntSoftmax | n:  28
IntGELU    | n:  2
IntSoftmax | n:  28
IntGELU    | n:  2
IntSoftmax | n:  28
IntGELU    | n:  2
IntSoftmax | n:  28
IntGELU    | n:  2
IntSoftmax | n:  28
IntGELU    | n:  2
IntSoftmax | n:  28
IntGELU    | n:  2
IntSoftmax | n:  28
IntGELU    | n:  2
IntSoftmax | n:  28
IntGELU    | n:  2
IntSoftmax | n:  28
IntGELU    | n:  2
.calib done
Test: [  0/391]	Time  3.942 ( 3.942)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.02
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=28, intgelu_exp_n=3
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=28, intgelu_exp_n=3, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  28
IntGELU    | n:  3
IntSoftmax | n:  28
IntGELU    | n:  3
IntSoftmax | n:  28
IntGELU    | n:  3
IntSoftmax | n:  28
IntGELU    | n:  3
IntSoftmax | n:  28
IntGELU    | n:  3
IntSoftmax | n:  28
IntGELU    | n:  3
IntSoftmax | n:  28
IntGELU    | n:  3
IntSoftmax | n:  28
IntGELU    | n:  3
IntSoftmax | n:  28
IntGELU    | n:  3
IntSoftmax | n:  28
IntGELU    | n:  3
IntSoftmax | n:  28
IntGELU    | n:  3
IntSoftmax | n:  28
IntGELU    | n:  3
.calib done
Test: [  0/391]	Time  3.823 ( 3.823)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.90
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=28, intgelu_exp_n=4
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=28, intgelu_exp_n=4, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  28
IntGELU    | n:  4
IntSoftmax | n:  28
IntGELU    | n:  4
IntSoftmax | n:  28
IntGELU    | n:  4
IntSoftmax | n:  28
IntGELU    | n:  4
IntSoftmax | n:  28
IntGELU    | n:  4
IntSoftmax | n:  28
IntGELU    | n:  4
IntSoftmax | n:  28
IntGELU    | n:  4
IntSoftmax | n:  28
IntGELU    | n:  4
IntSoftmax | n:  28
IntGELU    | n:  4
IntSoftmax | n:  28
IntGELU    | n:  4
IntSoftmax | n:  28
IntGELU    | n:  4
IntSoftmax | n:  28
IntGELU    | n:  4
.calib done
Test: [  0/391]	Time  3.710 ( 3.710)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.80
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=28, intgelu_exp_n=5
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=28, intgelu_exp_n=5, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  28
IntGELU    | n:  5
IntSoftmax | n:  28
IntGELU    | n:  5
IntSoftmax | n:  28
IntGELU    | n:  5
IntSoftmax | n:  28
IntGELU    | n:  5
IntSoftmax | n:  28
IntGELU    | n:  5
IntSoftmax | n:  28
IntGELU    | n:  5
IntSoftmax | n:  28
IntGELU    | n:  5
IntSoftmax | n:  28
IntGELU    | n:  5
IntSoftmax | n:  28
IntGELU    | n:  5
IntSoftmax | n:  28
IntGELU    | n:  5
IntSoftmax | n:  28
IntGELU    | n:  5
IntSoftmax | n:  28
IntGELU    | n:  5
.calib done
Test: [  0/391]	Time  3.912 ( 3.912)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.02
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=28, intgelu_exp_n=6
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=28, intgelu_exp_n=6, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  28
IntGELU    | n:  6
IntSoftmax | n:  28
IntGELU    | n:  6
IntSoftmax | n:  28
IntGELU    | n:  6
IntSoftmax | n:  28
IntGELU    | n:  6
IntSoftmax | n:  28
IntGELU    | n:  6
IntSoftmax | n:  28
IntGELU    | n:  6
IntSoftmax | n:  28
IntGELU    | n:  6
IntSoftmax | n:  28
IntGELU    | n:  6
IntSoftmax | n:  28
IntGELU    | n:  6
IntSoftmax | n:  28
IntGELU    | n:  6
IntSoftmax | n:  28
IntGELU    | n:  6
IntSoftmax | n:  28
IntGELU    | n:  6
.calib done
Test: [  0/391]	Time  3.927 ( 3.927)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.01
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=28, intgelu_exp_n=7
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=28, intgelu_exp_n=7, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  28
IntGELU    | n:  7
IntSoftmax | n:  28
IntGELU    | n:  7
IntSoftmax | n:  28
IntGELU    | n:  7
IntSoftmax | n:  28
IntGELU    | n:  7
IntSoftmax | n:  28
IntGELU    | n:  7
IntSoftmax | n:  28
IntGELU    | n:  7
IntSoftmax | n:  28
IntGELU    | n:  7
IntSoftmax | n:  28
IntGELU    | n:  7
IntSoftmax | n:  28
IntGELU    | n:  7
IntSoftmax | n:  28
IntGELU    | n:  7
IntSoftmax | n:  28
IntGELU    | n:  7
IntSoftmax | n:  28
IntGELU    | n:  7
.calib done
Test: [  0/391]	Time  3.642 ( 3.642)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.64
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=28, intgelu_exp_n=8
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=28, intgelu_exp_n=8, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  28
IntGELU    | n:  8
IntSoftmax | n:  28
IntGELU    | n:  8
IntSoftmax | n:  28
IntGELU    | n:  8
IntSoftmax | n:  28
IntGELU    | n:  8
IntSoftmax | n:  28
IntGELU    | n:  8
IntSoftmax | n:  28
IntGELU    | n:  8
IntSoftmax | n:  28
IntGELU    | n:  8
IntSoftmax | n:  28
IntGELU    | n:  8
IntSoftmax | n:  28
IntGELU    | n:  8
IntSoftmax | n:  28
IntGELU    | n:  8
IntSoftmax | n:  28
IntGELU    | n:  8
IntSoftmax | n:  28
IntGELU    | n:  8
.calib done
Test: [  0/391]	Time  3.894 ( 3.894)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.88
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=28, intgelu_exp_n=9
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=28, intgelu_exp_n=9, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  28
IntGELU    | n:  9
IntSoftmax | n:  28
IntGELU    | n:  9
IntSoftmax | n:  28
IntGELU    | n:  9
IntSoftmax | n:  28
IntGELU    | n:  9
IntSoftmax | n:  28
IntGELU    | n:  9
IntSoftmax | n:  28
IntGELU    | n:  9
IntSoftmax | n:  28
IntGELU    | n:  9
IntSoftmax | n:  28
IntGELU    | n:  9
IntSoftmax | n:  28
IntGELU    | n:  9
IntSoftmax | n:  28
IntGELU    | n:  9
IntSoftmax | n:  28
IntGELU    | n:  9
IntSoftmax | n:  28
IntGELU    | n:  9
.calib done
Test: [  0/391]	Time  3.709 ( 3.709)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.72
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=28, intgelu_exp_n=10
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=28, intgelu_exp_n=10, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  28
IntGELU    | n:  10
IntSoftmax | n:  28
IntGELU    | n:  10
IntSoftmax | n:  28
IntGELU    | n:  10
IntSoftmax | n:  28
IntGELU    | n:  10
IntSoftmax | n:  28
IntGELU    | n:  10
IntSoftmax | n:  28
IntGELU    | n:  10
IntSoftmax | n:  28
IntGELU    | n:  10
IntSoftmax | n:  28
IntGELU    | n:  10
IntSoftmax | n:  28
IntGELU    | n:  10
IntSoftmax | n:  28
IntGELU    | n:  10
IntSoftmax | n:  28
IntGELU    | n:  10
IntSoftmax | n:  28
IntGELU    | n:  10
.calib done
Test: [  0/391]	Time  3.658 ( 3.658)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.69
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=28, intgelu_exp_n=11
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=28, intgelu_exp_n=11, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  28
IntGELU    | n:  11
IntSoftmax | n:  28
IntGELU    | n:  11
IntSoftmax | n:  28
IntGELU    | n:  11
IntSoftmax | n:  28
IntGELU    | n:  11
IntSoftmax | n:  28
IntGELU    | n:  11
IntSoftmax | n:  28
IntGELU    | n:  11
IntSoftmax | n:  28
IntGELU    | n:  11
IntSoftmax | n:  28
IntGELU    | n:  11
IntSoftmax | n:  28
IntGELU    | n:  11
IntSoftmax | n:  28
IntGELU    | n:  11
IntSoftmax | n:  28
IntGELU    | n:  11
IntSoftmax | n:  28
IntGELU    | n:  11
.calib done
Test: [  0/391]	Time  3.858 ( 3.858)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.93
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=28, intgelu_exp_n=12
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=28, intgelu_exp_n=12, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  28
IntGELU    | n:  12
IntSoftmax | n:  28
IntGELU    | n:  12
IntSoftmax | n:  28
IntGELU    | n:  12
IntSoftmax | n:  28
IntGELU    | n:  12
IntSoftmax | n:  28
IntGELU    | n:  12
IntSoftmax | n:  28
IntGELU    | n:  12
IntSoftmax | n:  28
IntGELU    | n:  12
IntSoftmax | n:  28
IntGELU    | n:  12
IntSoftmax | n:  28
IntGELU    | n:  12
IntSoftmax | n:  28
IntGELU    | n:  12
IntSoftmax | n:  28
IntGELU    | n:  12
IntSoftmax | n:  28
IntGELU    | n:  12
.calib done
Test: [  0/391]	Time  3.924 ( 3.924)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.92
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=28, intgelu_exp_n=13
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=28, intgelu_exp_n=13, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  28
IntGELU    | n:  13
IntSoftmax | n:  28
IntGELU    | n:  13
IntSoftmax | n:  28
IntGELU    | n:  13
IntSoftmax | n:  28
IntGELU    | n:  13
IntSoftmax | n:  28
IntGELU    | n:  13
IntSoftmax | n:  28
IntGELU    | n:  13
IntSoftmax | n:  28
IntGELU    | n:  13
IntSoftmax | n:  28
IntGELU    | n:  13
IntSoftmax | n:  28
IntGELU    | n:  13
IntSoftmax | n:  28
IntGELU    | n:  13
IntSoftmax | n:  28
IntGELU    | n:  13
IntSoftmax | n:  28
IntGELU    | n:  13
.calib done
Test: [  0/391]	Time  3.839 ( 3.839)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.89
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=28, intgelu_exp_n=14
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=28, intgelu_exp_n=14, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  28
IntGELU    | n:  14
IntSoftmax | n:  28
IntGELU    | n:  14
IntSoftmax | n:  28
IntGELU    | n:  14
IntSoftmax | n:  28
IntGELU    | n:  14
IntSoftmax | n:  28
IntGELU    | n:  14
IntSoftmax | n:  28
IntGELU    | n:  14
IntSoftmax | n:  28
IntGELU    | n:  14
IntSoftmax | n:  28
IntGELU    | n:  14
IntSoftmax | n:  28
IntGELU    | n:  14
IntSoftmax | n:  28
IntGELU    | n:  14
IntSoftmax | n:  28
IntGELU    | n:  14
IntSoftmax | n:  28
IntGELU    | n:  14
.calib done
Test: [  0/391]	Time  3.915 ( 3.915)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.94
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=28, intgelu_exp_n=15
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=28, intgelu_exp_n=15, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  28
IntGELU    | n:  15
IntSoftmax | n:  28
IntGELU    | n:  15
IntSoftmax | n:  28
IntGELU    | n:  15
IntSoftmax | n:  28
IntGELU    | n:  15
IntSoftmax | n:  28
IntGELU    | n:  15
IntSoftmax | n:  28
IntGELU    | n:  15
IntSoftmax | n:  28
IntGELU    | n:  15
IntSoftmax | n:  28
IntGELU    | n:  15
IntSoftmax | n:  28
IntGELU    | n:  15
IntSoftmax | n:  28
IntGELU    | n:  15
IntSoftmax | n:  28
IntGELU    | n:  15
IntSoftmax | n:  28
IntGELU    | n:  15
.calib done
Test: [  0/391]	Time  3.729 ( 3.729)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.79
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=28, intgelu_exp_n=16
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=28, intgelu_exp_n=16, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  28
IntGELU    | n:  16
IntSoftmax | n:  28
IntGELU    | n:  16
IntSoftmax | n:  28
IntGELU    | n:  16
IntSoftmax | n:  28
IntGELU    | n:  16
IntSoftmax | n:  28
IntGELU    | n:  16
IntSoftmax | n:  28
IntGELU    | n:  16
IntSoftmax | n:  28
IntGELU    | n:  16
IntSoftmax | n:  28
IntGELU    | n:  16
IntSoftmax | n:  28
IntGELU    | n:  16
IntSoftmax | n:  28
IntGELU    | n:  16
IntSoftmax | n:  28
IntGELU    | n:  16
IntSoftmax | n:  28
IntGELU    | n:  16
.calib done
Test: [  0/391]	Time  4.033 ( 4.033)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.08
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=28, intgelu_exp_n=17
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=28, intgelu_exp_n=17, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  28
IntGELU    | n:  17
IntSoftmax | n:  28
IntGELU    | n:  17
IntSoftmax | n:  28
IntGELU    | n:  17
IntSoftmax | n:  28
IntGELU    | n:  17
IntSoftmax | n:  28
IntGELU    | n:  17
IntSoftmax | n:  28
IntGELU    | n:  17
IntSoftmax | n:  28
IntGELU    | n:  17
IntSoftmax | n:  28
IntGELU    | n:  17
IntSoftmax | n:  28
IntGELU    | n:  17
IntSoftmax | n:  28
IntGELU    | n:  17
IntSoftmax | n:  28
IntGELU    | n:  17
IntSoftmax | n:  28
IntGELU    | n:  17
.calib done
Test: [  0/391]	Time  3.876 ( 3.876)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.92
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=28, intgelu_exp_n=18
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=28, intgelu_exp_n=18, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  28
IntGELU    | n:  18
IntSoftmax | n:  28
IntGELU    | n:  18
IntSoftmax | n:  28
IntGELU    | n:  18
IntSoftmax | n:  28
IntGELU    | n:  18
IntSoftmax | n:  28
IntGELU    | n:  18
IntSoftmax | n:  28
IntGELU    | n:  18
IntSoftmax | n:  28
IntGELU    | n:  18
IntSoftmax | n:  28
IntGELU    | n:  18
IntSoftmax | n:  28
IntGELU    | n:  18
IntSoftmax | n:  28
IntGELU    | n:  18
IntSoftmax | n:  28
IntGELU    | n:  18
IntSoftmax | n:  28
IntGELU    | n:  18
.calib done
Test: [  0/391]	Time  3.969 ( 3.969)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.09
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=28, intgelu_exp_n=19
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=28, intgelu_exp_n=19, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  28
IntGELU    | n:  19
IntSoftmax | n:  28
IntGELU    | n:  19
IntSoftmax | n:  28
IntGELU    | n:  19
IntSoftmax | n:  28
IntGELU    | n:  19
IntSoftmax | n:  28
IntGELU    | n:  19
IntSoftmax | n:  28
IntGELU    | n:  19
IntSoftmax | n:  28
IntGELU    | n:  19
IntSoftmax | n:  28
IntGELU    | n:  19
IntSoftmax | n:  28
IntGELU    | n:  19
IntSoftmax | n:  28
IntGELU    | n:  19
IntSoftmax | n:  28
IntGELU    | n:  19
IntSoftmax | n:  28
IntGELU    | n:  19
.calib done
Test: [  0/391]	Time  3.959 ( 3.959)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.99
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=28, intgelu_exp_n=20
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=28, intgelu_exp_n=20, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  28
IntGELU    | n:  20
IntSoftmax | n:  28
IntGELU    | n:  20
IntSoftmax | n:  28
IntGELU    | n:  20
IntSoftmax | n:  28
IntGELU    | n:  20
IntSoftmax | n:  28
IntGELU    | n:  20
IntSoftmax | n:  28
IntGELU    | n:  20
IntSoftmax | n:  28
IntGELU    | n:  20
IntSoftmax | n:  28
IntGELU    | n:  20
IntSoftmax | n:  28
IntGELU    | n:  20
IntSoftmax | n:  28
IntGELU    | n:  20
IntSoftmax | n:  28
IntGELU    | n:  20
IntSoftmax | n:  28
IntGELU    | n:  20
.calib done
Test: [  0/391]	Time  3.850 ( 3.850)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.83
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=28, intgelu_exp_n=21
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=28, intgelu_exp_n=21, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  28
IntGELU    | n:  21
IntSoftmax | n:  28
IntGELU    | n:  21
IntSoftmax | n:  28
IntGELU    | n:  21
IntSoftmax | n:  28
IntGELU    | n:  21
IntSoftmax | n:  28
IntGELU    | n:  21
IntSoftmax | n:  28
IntGELU    | n:  21
IntSoftmax | n:  28
IntGELU    | n:  21
IntSoftmax | n:  28
IntGELU    | n:  21
IntSoftmax | n:  28
IntGELU    | n:  21
IntSoftmax | n:  28
IntGELU    | n:  21
IntSoftmax | n:  28
IntGELU    | n:  21
IntSoftmax | n:  28
IntGELU    | n:  21
.calib done
Test: [  0/391]	Time  3.925 ( 3.925)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.99
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=28, intgelu_exp_n=22
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=28, intgelu_exp_n=22, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  28
IntGELU    | n:  22
IntSoftmax | n:  28
IntGELU    | n:  22
IntSoftmax | n:  28
IntGELU    | n:  22
IntSoftmax | n:  28
IntGELU    | n:  22
IntSoftmax | n:  28
IntGELU    | n:  22
IntSoftmax | n:  28
IntGELU    | n:  22
IntSoftmax | n:  28
IntGELU    | n:  22
IntSoftmax | n:  28
IntGELU    | n:  22
IntSoftmax | n:  28
IntGELU    | n:  22
IntSoftmax | n:  28
IntGELU    | n:  22
IntSoftmax | n:  28
IntGELU    | n:  22
IntSoftmax | n:  28
IntGELU    | n:  22
.calib done
Test: [  0/391]	Time  3.900 ( 3.900)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.90
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=28, intgelu_exp_n=23
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=28, intgelu_exp_n=23, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  28
IntGELU    | n:  23
IntSoftmax | n:  28
IntGELU    | n:  23
IntSoftmax | n:  28
IntGELU    | n:  23
IntSoftmax | n:  28
IntGELU    | n:  23
IntSoftmax | n:  28
IntGELU    | n:  23
IntSoftmax | n:  28
IntGELU    | n:  23
IntSoftmax | n:  28
IntGELU    | n:  23
IntSoftmax | n:  28
IntGELU    | n:  23
IntSoftmax | n:  28
IntGELU    | n:  23
IntSoftmax | n:  28
IntGELU    | n:  23
IntSoftmax | n:  28
IntGELU    | n:  23
IntSoftmax | n:  28
IntGELU    | n:  23
.calib done
Test: [  0/391]	Time  3.953 ( 3.953)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.02
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=28, intgelu_exp_n=24
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=28, intgelu_exp_n=24, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  28
IntGELU    | n:  24
IntSoftmax | n:  28
IntGELU    | n:  24
IntSoftmax | n:  28
IntGELU    | n:  24
IntSoftmax | n:  28
IntGELU    | n:  24
IntSoftmax | n:  28
IntGELU    | n:  24
IntSoftmax | n:  28
IntGELU    | n:  24
IntSoftmax | n:  28
IntGELU    | n:  24
IntSoftmax | n:  28
IntGELU    | n:  24
IntSoftmax | n:  28
IntGELU    | n:  24
IntSoftmax | n:  28
IntGELU    | n:  24
IntSoftmax | n:  28
IntGELU    | n:  24
IntSoftmax | n:  28
IntGELU    | n:  24
.calib done
Test: [  0/391]	Time  3.833 ( 3.833)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.87
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=28, intgelu_exp_n=25
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=28, intgelu_exp_n=25, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  28
IntGELU    | n:  25
IntSoftmax | n:  28
IntGELU    | n:  25
IntSoftmax | n:  28
IntGELU    | n:  25
IntSoftmax | n:  28
IntGELU    | n:  25
IntSoftmax | n:  28
IntGELU    | n:  25
IntSoftmax | n:  28
IntGELU    | n:  25
IntSoftmax | n:  28
IntGELU    | n:  25
IntSoftmax | n:  28
IntGELU    | n:  25
IntSoftmax | n:  28
IntGELU    | n:  25
IntSoftmax | n:  28
IntGELU    | n:  25
IntSoftmax | n:  28
IntGELU    | n:  25
IntSoftmax | n:  28
IntGELU    | n:  25
.calib done
Test: [  0/391]	Time  3.781 ( 3.781)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.83
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=28, intgelu_exp_n=26
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=28, intgelu_exp_n=26, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  28
IntGELU    | n:  26
IntSoftmax | n:  28
IntGELU    | n:  26
IntSoftmax | n:  28
IntGELU    | n:  26
IntSoftmax | n:  28
IntGELU    | n:  26
IntSoftmax | n:  28
IntGELU    | n:  26
IntSoftmax | n:  28
IntGELU    | n:  26
IntSoftmax | n:  28
IntGELU    | n:  26
IntSoftmax | n:  28
IntGELU    | n:  26
IntSoftmax | n:  28
IntGELU    | n:  26
IntSoftmax | n:  28
IntGELU    | n:  26
IntSoftmax | n:  28
IntGELU    | n:  26
IntSoftmax | n:  28
IntGELU    | n:  26
.calib done
Test: [  0/391]	Time  3.948 ( 3.948)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.94
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=28, intgelu_exp_n=27
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=28, intgelu_exp_n=27, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  28
IntGELU    | n:  27
IntSoftmax | n:  28
IntGELU    | n:  27
IntSoftmax | n:  28
IntGELU    | n:  27
IntSoftmax | n:  28
IntGELU    | n:  27
IntSoftmax | n:  28
IntGELU    | n:  27
IntSoftmax | n:  28
IntGELU    | n:  27
IntSoftmax | n:  28
IntGELU    | n:  27
IntSoftmax | n:  28
IntGELU    | n:  27
IntSoftmax | n:  28
IntGELU    | n:  27
IntSoftmax | n:  28
IntGELU    | n:  27
IntSoftmax | n:  28
IntGELU    | n:  27
IntSoftmax | n:  28
IntGELU    | n:  27
.calib done
Test: [  0/391]	Time  3.981 ( 3.981)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.03
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=28, intgelu_exp_n=28
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=28, intgelu_exp_n=28, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  28
IntGELU    | n:  28
IntSoftmax | n:  28
IntGELU    | n:  28
IntSoftmax | n:  28
IntGELU    | n:  28
IntSoftmax | n:  28
IntGELU    | n:  28
IntSoftmax | n:  28
IntGELU    | n:  28
IntSoftmax | n:  28
IntGELU    | n:  28
IntSoftmax | n:  28
IntGELU    | n:  28
IntSoftmax | n:  28
IntGELU    | n:  28
IntSoftmax | n:  28
IntGELU    | n:  28
IntSoftmax | n:  28
IntGELU    | n:  28
IntSoftmax | n:  28
IntGELU    | n:  28
IntSoftmax | n:  28
IntGELU    | n:  28
.calib done
Test: [  0/391]	Time  3.937 ( 3.937)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.02
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=28, intgelu_exp_n=29
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=28, intgelu_exp_n=29, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  28
IntGELU    | n:  29
IntSoftmax | n:  28
IntGELU    | n:  29
IntSoftmax | n:  28
IntGELU    | n:  29
IntSoftmax | n:  28
IntGELU    | n:  29
IntSoftmax | n:  28
IntGELU    | n:  29
IntSoftmax | n:  28
IntGELU    | n:  29
IntSoftmax | n:  28
IntGELU    | n:  29
IntSoftmax | n:  28
IntGELU    | n:  29
IntSoftmax | n:  28
IntGELU    | n:  29
IntSoftmax | n:  28
IntGELU    | n:  29
IntSoftmax | n:  28
IntGELU    | n:  29
IntSoftmax | n:  28
IntGELU    | n:  29
.calib done
Test: [  0/391]	Time  3.988 ( 3.988)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.09
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=28, intgelu_exp_n=30
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=28, intgelu_exp_n=30, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  28
IntGELU    | n:  30
IntSoftmax | n:  28
IntGELU    | n:  30
IntSoftmax | n:  28
IntGELU    | n:  30
IntSoftmax | n:  28
IntGELU    | n:  30
IntSoftmax | n:  28
IntGELU    | n:  30
IntSoftmax | n:  28
IntGELU    | n:  30
IntSoftmax | n:  28
IntGELU    | n:  30
IntSoftmax | n:  28
IntGELU    | n:  30
IntSoftmax | n:  28
IntGELU    | n:  30
IntSoftmax | n:  28
IntGELU    | n:  30
IntSoftmax | n:  28
IntGELU    | n:  30
IntSoftmax | n:  28
IntGELU    | n:  30
.calib done
Test: [  0/391]	Time  3.658 ( 3.658)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.76
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=28, intgelu_exp_n=31
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=28, intgelu_exp_n=31, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  28
IntGELU    | n:  31
IntSoftmax | n:  28
IntGELU    | n:  31
IntSoftmax | n:  28
IntGELU    | n:  31
IntSoftmax | n:  28
IntGELU    | n:  31
IntSoftmax | n:  28
IntGELU    | n:  31
IntSoftmax | n:  28
IntGELU    | n:  31
IntSoftmax | n:  28
IntGELU    | n:  31
IntSoftmax | n:  28
IntGELU    | n:  31
IntSoftmax | n:  28
IntGELU    | n:  31
IntSoftmax | n:  28
IntGELU    | n:  31
IntSoftmax | n:  28
IntGELU    | n:  31
IntSoftmax | n:  28
IntGELU    | n:  31
.calib done
Test: [  0/391]	Time  4.065 ( 4.065)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.14
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=29, intgelu_exp_n=0
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=29, intgelu_exp_n=0, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  29
IntGELU    | n:  0
IntSoftmax | n:  29
IntGELU    | n:  0
IntSoftmax | n:  29
IntGELU    | n:  0
IntSoftmax | n:  29
IntGELU    | n:  0
IntSoftmax | n:  29
IntGELU    | n:  0
IntSoftmax | n:  29
IntGELU    | n:  0
IntSoftmax | n:  29
IntGELU    | n:  0
IntSoftmax | n:  29
IntGELU    | n:  0
IntSoftmax | n:  29
IntGELU    | n:  0
IntSoftmax | n:  29
IntGELU    | n:  0
IntSoftmax | n:  29
IntGELU    | n:  0
IntSoftmax | n:  29
IntGELU    | n:  0
.calib done
Test: [  0/391]	Time  3.735 ( 3.735)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.79
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=29, intgelu_exp_n=1
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=29, intgelu_exp_n=1, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  29
IntGELU    | n:  1
IntSoftmax | n:  29
IntGELU    | n:  1
IntSoftmax | n:  29
IntGELU    | n:  1
IntSoftmax | n:  29
IntGELU    | n:  1
IntSoftmax | n:  29
IntGELU    | n:  1
IntSoftmax | n:  29
IntGELU    | n:  1
IntSoftmax | n:  29
IntGELU    | n:  1
IntSoftmax | n:  29
IntGELU    | n:  1
IntSoftmax | n:  29
IntGELU    | n:  1
IntSoftmax | n:  29
IntGELU    | n:  1
IntSoftmax | n:  29
IntGELU    | n:  1
IntSoftmax | n:  29
IntGELU    | n:  1
.calib done
Test: [  0/391]	Time  4.025 ( 4.025)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.06
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=29, intgelu_exp_n=2
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=29, intgelu_exp_n=2, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  29
IntGELU    | n:  2
IntSoftmax | n:  29
IntGELU    | n:  2
IntSoftmax | n:  29
IntGELU    | n:  2
IntSoftmax | n:  29
IntGELU    | n:  2
IntSoftmax | n:  29
IntGELU    | n:  2
IntSoftmax | n:  29
IntGELU    | n:  2
IntSoftmax | n:  29
IntGELU    | n:  2
IntSoftmax | n:  29
IntGELU    | n:  2
IntSoftmax | n:  29
IntGELU    | n:  2
IntSoftmax | n:  29
IntGELU    | n:  2
IntSoftmax | n:  29
IntGELU    | n:  2
IntSoftmax | n:  29
IntGELU    | n:  2
.calib done
Test: [  0/391]	Time  3.915 ( 3.915)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.90
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=29, intgelu_exp_n=3
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=29, intgelu_exp_n=3, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  29
IntGELU    | n:  3
IntSoftmax | n:  29
IntGELU    | n:  3
IntSoftmax | n:  29
IntGELU    | n:  3
IntSoftmax | n:  29
IntGELU    | n:  3
IntSoftmax | n:  29
IntGELU    | n:  3
IntSoftmax | n:  29
IntGELU    | n:  3
IntSoftmax | n:  29
IntGELU    | n:  3
IntSoftmax | n:  29
IntGELU    | n:  3
IntSoftmax | n:  29
IntGELU    | n:  3
IntSoftmax | n:  29
IntGELU    | n:  3
IntSoftmax | n:  29
IntGELU    | n:  3
IntSoftmax | n:  29
IntGELU    | n:  3
.calib done
Test: [  0/391]	Time  3.816 ( 3.816)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.79
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=29, intgelu_exp_n=4
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=29, intgelu_exp_n=4, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  29
IntGELU    | n:  4
IntSoftmax | n:  29
IntGELU    | n:  4
IntSoftmax | n:  29
IntGELU    | n:  4
IntSoftmax | n:  29
IntGELU    | n:  4
IntSoftmax | n:  29
IntGELU    | n:  4
IntSoftmax | n:  29
IntGELU    | n:  4
IntSoftmax | n:  29
IntGELU    | n:  4
IntSoftmax | n:  29
IntGELU    | n:  4
IntSoftmax | n:  29
IntGELU    | n:  4
IntSoftmax | n:  29
IntGELU    | n:  4
IntSoftmax | n:  29
IntGELU    | n:  4
IntSoftmax | n:  29
IntGELU    | n:  4
.calib done
Test: [  0/391]	Time  3.876 ( 3.876)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.94
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=29, intgelu_exp_n=5
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=29, intgelu_exp_n=5, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  29
IntGELU    | n:  5
IntSoftmax | n:  29
IntGELU    | n:  5
IntSoftmax | n:  29
IntGELU    | n:  5
IntSoftmax | n:  29
IntGELU    | n:  5
IntSoftmax | n:  29
IntGELU    | n:  5
IntSoftmax | n:  29
IntGELU    | n:  5
IntSoftmax | n:  29
IntGELU    | n:  5
IntSoftmax | n:  29
IntGELU    | n:  5
IntSoftmax | n:  29
IntGELU    | n:  5
IntSoftmax | n:  29
IntGELU    | n:  5
IntSoftmax | n:  29
IntGELU    | n:  5
IntSoftmax | n:  29
IntGELU    | n:  5
.calib done
Test: [  0/391]	Time  3.731 ( 3.731)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.82
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=29, intgelu_exp_n=6
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=29, intgelu_exp_n=6, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  29
IntGELU    | n:  6
IntSoftmax | n:  29
IntGELU    | n:  6
IntSoftmax | n:  29
IntGELU    | n:  6
IntSoftmax | n:  29
IntGELU    | n:  6
IntSoftmax | n:  29
IntGELU    | n:  6
IntSoftmax | n:  29
IntGELU    | n:  6
IntSoftmax | n:  29
IntGELU    | n:  6
IntSoftmax | n:  29
IntGELU    | n:  6
IntSoftmax | n:  29
IntGELU    | n:  6
IntSoftmax | n:  29
IntGELU    | n:  6
IntSoftmax | n:  29
IntGELU    | n:  6
IntSoftmax | n:  29
IntGELU    | n:  6
.calib done
Test: [  0/391]	Time  3.881 ( 3.881)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.95
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=29, intgelu_exp_n=7
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=29, intgelu_exp_n=7, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  29
IntGELU    | n:  7
IntSoftmax | n:  29
IntGELU    | n:  7
IntSoftmax | n:  29
IntGELU    | n:  7
IntSoftmax | n:  29
IntGELU    | n:  7
IntSoftmax | n:  29
IntGELU    | n:  7
IntSoftmax | n:  29
IntGELU    | n:  7
IntSoftmax | n:  29
IntGELU    | n:  7
IntSoftmax | n:  29
IntGELU    | n:  7
IntSoftmax | n:  29
IntGELU    | n:  7
IntSoftmax | n:  29
IntGELU    | n:  7
IntSoftmax | n:  29
IntGELU    | n:  7
IntSoftmax | n:  29
IntGELU    | n:  7
.calib done
Test: [  0/391]	Time  3.903 ( 3.903)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.95
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=29, intgelu_exp_n=8
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=29, intgelu_exp_n=8, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  29
IntGELU    | n:  8
IntSoftmax | n:  29
IntGELU    | n:  8
IntSoftmax | n:  29
IntGELU    | n:  8
IntSoftmax | n:  29
IntGELU    | n:  8
IntSoftmax | n:  29
IntGELU    | n:  8
IntSoftmax | n:  29
IntGELU    | n:  8
IntSoftmax | n:  29
IntGELU    | n:  8
IntSoftmax | n:  29
IntGELU    | n:  8
IntSoftmax | n:  29
IntGELU    | n:  8
IntSoftmax | n:  29
IntGELU    | n:  8
IntSoftmax | n:  29
IntGELU    | n:  8
IntSoftmax | n:  29
IntGELU    | n:  8
.calib done
Test: [  0/391]	Time  3.671 ( 3.671)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.69
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=29, intgelu_exp_n=9
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=29, intgelu_exp_n=9, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  29
IntGELU    | n:  9
IntSoftmax | n:  29
IntGELU    | n:  9
IntSoftmax | n:  29
IntGELU    | n:  9
IntSoftmax | n:  29
IntGELU    | n:  9
IntSoftmax | n:  29
IntGELU    | n:  9
IntSoftmax | n:  29
IntGELU    | n:  9
IntSoftmax | n:  29
IntGELU    | n:  9
IntSoftmax | n:  29
IntGELU    | n:  9
IntSoftmax | n:  29
IntGELU    | n:  9
IntSoftmax | n:  29
IntGELU    | n:  9
IntSoftmax | n:  29
IntGELU    | n:  9
IntSoftmax | n:  29
IntGELU    | n:  9
.calib done
Test: [  0/391]	Time  3.956 ( 3.956)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.94
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=29, intgelu_exp_n=10
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=29, intgelu_exp_n=10, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  29
IntGELU    | n:  10
IntSoftmax | n:  29
IntGELU    | n:  10
IntSoftmax | n:  29
IntGELU    | n:  10
IntSoftmax | n:  29
IntGELU    | n:  10
IntSoftmax | n:  29
IntGELU    | n:  10
IntSoftmax | n:  29
IntGELU    | n:  10
IntSoftmax | n:  29
IntGELU    | n:  10
IntSoftmax | n:  29
IntGELU    | n:  10
IntSoftmax | n:  29
IntGELU    | n:  10
IntSoftmax | n:  29
IntGELU    | n:  10
IntSoftmax | n:  29
IntGELU    | n:  10
IntSoftmax | n:  29
IntGELU    | n:  10
.calib done
Test: [  0/391]	Time  3.875 ( 3.875)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.87
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=29, intgelu_exp_n=11
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=29, intgelu_exp_n=11, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  29
IntGELU    | n:  11
IntSoftmax | n:  29
IntGELU    | n:  11
IntSoftmax | n:  29
IntGELU    | n:  11
IntSoftmax | n:  29
IntGELU    | n:  11
IntSoftmax | n:  29
IntGELU    | n:  11
IntSoftmax | n:  29
IntGELU    | n:  11
IntSoftmax | n:  29
IntGELU    | n:  11
IntSoftmax | n:  29
IntGELU    | n:  11
IntSoftmax | n:  29
IntGELU    | n:  11
IntSoftmax | n:  29
IntGELU    | n:  11
IntSoftmax | n:  29
IntGELU    | n:  11
IntSoftmax | n:  29
IntGELU    | n:  11
.calib done
Test: [  0/391]	Time  3.701 ( 3.701)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.79
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=29, intgelu_exp_n=12
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=29, intgelu_exp_n=12, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  29
IntGELU    | n:  12
IntSoftmax | n:  29
IntGELU    | n:  12
IntSoftmax | n:  29
IntGELU    | n:  12
IntSoftmax | n:  29
IntGELU    | n:  12
IntSoftmax | n:  29
IntGELU    | n:  12
IntSoftmax | n:  29
IntGELU    | n:  12
IntSoftmax | n:  29
IntGELU    | n:  12
IntSoftmax | n:  29
IntGELU    | n:  12
IntSoftmax | n:  29
IntGELU    | n:  12
IntSoftmax | n:  29
IntGELU    | n:  12
IntSoftmax | n:  29
IntGELU    | n:  12
IntSoftmax | n:  29
IntGELU    | n:  12
.calib done
Test: [  0/391]	Time  3.973 ( 3.973)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.02
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=29, intgelu_exp_n=13
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=29, intgelu_exp_n=13, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  29
IntGELU    | n:  13
IntSoftmax | n:  29
IntGELU    | n:  13
IntSoftmax | n:  29
IntGELU    | n:  13
IntSoftmax | n:  29
IntGELU    | n:  13
IntSoftmax | n:  29
IntGELU    | n:  13
IntSoftmax | n:  29
IntGELU    | n:  13
IntSoftmax | n:  29
IntGELU    | n:  13
IntSoftmax | n:  29
IntGELU    | n:  13
IntSoftmax | n:  29
IntGELU    | n:  13
IntSoftmax | n:  29
IntGELU    | n:  13
IntSoftmax | n:  29
IntGELU    | n:  13
IntSoftmax | n:  29
IntGELU    | n:  13
.calib done
Test: [  0/391]	Time  3.941 ( 3.941)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.99
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=29, intgelu_exp_n=14
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=29, intgelu_exp_n=14, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  29
IntGELU    | n:  14
IntSoftmax | n:  29
IntGELU    | n:  14
IntSoftmax | n:  29
IntGELU    | n:  14
IntSoftmax | n:  29
IntGELU    | n:  14
IntSoftmax | n:  29
IntGELU    | n:  14
IntSoftmax | n:  29
IntGELU    | n:  14
IntSoftmax | n:  29
IntGELU    | n:  14
IntSoftmax | n:  29
IntGELU    | n:  14
IntSoftmax | n:  29
IntGELU    | n:  14
IntSoftmax | n:  29
IntGELU    | n:  14
IntSoftmax | n:  29
IntGELU    | n:  14
IntSoftmax | n:  29
IntGELU    | n:  14
.calib done
Test: [  0/391]	Time  3.673 ( 3.673)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.74
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=29, intgelu_exp_n=15
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=29, intgelu_exp_n=15, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  29
IntGELU    | n:  15
IntSoftmax | n:  29
IntGELU    | n:  15
IntSoftmax | n:  29
IntGELU    | n:  15
IntSoftmax | n:  29
IntGELU    | n:  15
IntSoftmax | n:  29
IntGELU    | n:  15
IntSoftmax | n:  29
IntGELU    | n:  15
IntSoftmax | n:  29
IntGELU    | n:  15
IntSoftmax | n:  29
IntGELU    | n:  15
IntSoftmax | n:  29
IntGELU    | n:  15
IntSoftmax | n:  29
IntGELU    | n:  15
IntSoftmax | n:  29
IntGELU    | n:  15
IntSoftmax | n:  29
IntGELU    | n:  15
.calib done
Test: [  0/391]	Time  3.755 ( 3.755)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.81
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=29, intgelu_exp_n=16
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=29, intgelu_exp_n=16, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  29
IntGELU    | n:  16
IntSoftmax | n:  29
IntGELU    | n:  16
IntSoftmax | n:  29
IntGELU    | n:  16
IntSoftmax | n:  29
IntGELU    | n:  16
IntSoftmax | n:  29
IntGELU    | n:  16
IntSoftmax | n:  29
IntGELU    | n:  16
IntSoftmax | n:  29
IntGELU    | n:  16
IntSoftmax | n:  29
IntGELU    | n:  16
IntSoftmax | n:  29
IntGELU    | n:  16
IntSoftmax | n:  29
IntGELU    | n:  16
IntSoftmax | n:  29
IntGELU    | n:  16
IntSoftmax | n:  29
IntGELU    | n:  16
.calib done
Test: [  0/391]	Time  4.043 ( 4.043)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.08
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=29, intgelu_exp_n=17
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=29, intgelu_exp_n=17, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  29
IntGELU    | n:  17
IntSoftmax | n:  29
IntGELU    | n:  17
IntSoftmax | n:  29
IntGELU    | n:  17
IntSoftmax | n:  29
IntGELU    | n:  17
IntSoftmax | n:  29
IntGELU    | n:  17
IntSoftmax | n:  29
IntGELU    | n:  17
IntSoftmax | n:  29
IntGELU    | n:  17
IntSoftmax | n:  29
IntGELU    | n:  17
IntSoftmax | n:  29
IntGELU    | n:  17
IntSoftmax | n:  29
IntGELU    | n:  17
IntSoftmax | n:  29
IntGELU    | n:  17
IntSoftmax | n:  29
IntGELU    | n:  17
.calib done
Test: [  0/391]	Time  3.820 ( 3.820)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.84
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=29, intgelu_exp_n=18
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=29, intgelu_exp_n=18, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  29
IntGELU    | n:  18
IntSoftmax | n:  29
IntGELU    | n:  18
IntSoftmax | n:  29
IntGELU    | n:  18
IntSoftmax | n:  29
IntGELU    | n:  18
IntSoftmax | n:  29
IntGELU    | n:  18
IntSoftmax | n:  29
IntGELU    | n:  18
IntSoftmax | n:  29
IntGELU    | n:  18
IntSoftmax | n:  29
IntGELU    | n:  18
IntSoftmax | n:  29
IntGELU    | n:  18
IntSoftmax | n:  29
IntGELU    | n:  18
IntSoftmax | n:  29
IntGELU    | n:  18
IntSoftmax | n:  29
IntGELU    | n:  18
.calib done
Test: [  0/391]	Time  3.834 ( 3.834)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.83
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=29, intgelu_exp_n=19
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=29, intgelu_exp_n=19, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  29
IntGELU    | n:  19
IntSoftmax | n:  29
IntGELU    | n:  19
IntSoftmax | n:  29
IntGELU    | n:  19
IntSoftmax | n:  29
IntGELU    | n:  19
IntSoftmax | n:  29
IntGELU    | n:  19
IntSoftmax | n:  29
IntGELU    | n:  19
IntSoftmax | n:  29
IntGELU    | n:  19
IntSoftmax | n:  29
IntGELU    | n:  19
IntSoftmax | n:  29
IntGELU    | n:  19
IntSoftmax | n:  29
IntGELU    | n:  19
IntSoftmax | n:  29
IntGELU    | n:  19
IntSoftmax | n:  29
IntGELU    | n:  19
.calib done
Test: [  0/391]	Time  3.722 ( 3.722)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.74
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=29, intgelu_exp_n=20
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=29, intgelu_exp_n=20, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  29
IntGELU    | n:  20
IntSoftmax | n:  29
IntGELU    | n:  20
IntSoftmax | n:  29
IntGELU    | n:  20
IntSoftmax | n:  29
IntGELU    | n:  20
IntSoftmax | n:  29
IntGELU    | n:  20
IntSoftmax | n:  29
IntGELU    | n:  20
IntSoftmax | n:  29
IntGELU    | n:  20
IntSoftmax | n:  29
IntGELU    | n:  20
IntSoftmax | n:  29
IntGELU    | n:  20
IntSoftmax | n:  29
IntGELU    | n:  20
IntSoftmax | n:  29
IntGELU    | n:  20
IntSoftmax | n:  29
IntGELU    | n:  20
.calib done
Test: [  0/391]	Time  3.890 ( 3.890)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.93
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=29, intgelu_exp_n=21
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=29, intgelu_exp_n=21, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  29
IntGELU    | n:  21
IntSoftmax | n:  29
IntGELU    | n:  21
IntSoftmax | n:  29
IntGELU    | n:  21
IntSoftmax | n:  29
IntGELU    | n:  21
IntSoftmax | n:  29
IntGELU    | n:  21
IntSoftmax | n:  29
IntGELU    | n:  21
IntSoftmax | n:  29
IntGELU    | n:  21
IntSoftmax | n:  29
IntGELU    | n:  21
IntSoftmax | n:  29
IntGELU    | n:  21
IntSoftmax | n:  29
IntGELU    | n:  21
IntSoftmax | n:  29
IntGELU    | n:  21
IntSoftmax | n:  29
IntGELU    | n:  21
.calib done
Test: [  0/391]	Time  3.882 ( 3.882)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.02
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=29, intgelu_exp_n=22
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=29, intgelu_exp_n=22, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  29
IntGELU    | n:  22
IntSoftmax | n:  29
IntGELU    | n:  22
IntSoftmax | n:  29
IntGELU    | n:  22
IntSoftmax | n:  29
IntGELU    | n:  22
IntSoftmax | n:  29
IntGELU    | n:  22
IntSoftmax | n:  29
IntGELU    | n:  22
IntSoftmax | n:  29
IntGELU    | n:  22
IntSoftmax | n:  29
IntGELU    | n:  22
IntSoftmax | n:  29
IntGELU    | n:  22
IntSoftmax | n:  29
IntGELU    | n:  22
IntSoftmax | n:  29
IntGELU    | n:  22
IntSoftmax | n:  29
IntGELU    | n:  22
.calib done
Test: [  0/391]	Time  4.019 ( 4.019)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.04
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=29, intgelu_exp_n=23
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=29, intgelu_exp_n=23, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  29
IntGELU    | n:  23
IntSoftmax | n:  29
IntGELU    | n:  23
IntSoftmax | n:  29
IntGELU    | n:  23
IntSoftmax | n:  29
IntGELU    | n:  23
IntSoftmax | n:  29
IntGELU    | n:  23
IntSoftmax | n:  29
IntGELU    | n:  23
IntSoftmax | n:  29
IntGELU    | n:  23
IntSoftmax | n:  29
IntGELU    | n:  23
IntSoftmax | n:  29
IntGELU    | n:  23
IntSoftmax | n:  29
IntGELU    | n:  23
IntSoftmax | n:  29
IntGELU    | n:  23
IntSoftmax | n:  29
IntGELU    | n:  23
.calib done
Test: [  0/391]	Time  3.883 ( 3.883)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.94
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=29, intgelu_exp_n=24
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=29, intgelu_exp_n=24, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  29
IntGELU    | n:  24
IntSoftmax | n:  29
IntGELU    | n:  24
IntSoftmax | n:  29
IntGELU    | n:  24
IntSoftmax | n:  29
IntGELU    | n:  24
IntSoftmax | n:  29
IntGELU    | n:  24
IntSoftmax | n:  29
IntGELU    | n:  24
IntSoftmax | n:  29
IntGELU    | n:  24
IntSoftmax | n:  29
IntGELU    | n:  24
IntSoftmax | n:  29
IntGELU    | n:  24
IntSoftmax | n:  29
IntGELU    | n:  24
IntSoftmax | n:  29
IntGELU    | n:  24
IntSoftmax | n:  29
IntGELU    | n:  24
.calib done
Test: [  0/391]	Time  3.706 ( 3.706)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.76
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=29, intgelu_exp_n=25
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=29, intgelu_exp_n=25, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  29
IntGELU    | n:  25
IntSoftmax | n:  29
IntGELU    | n:  25
IntSoftmax | n:  29
IntGELU    | n:  25
IntSoftmax | n:  29
IntGELU    | n:  25
IntSoftmax | n:  29
IntGELU    | n:  25
IntSoftmax | n:  29
IntGELU    | n:  25
IntSoftmax | n:  29
IntGELU    | n:  25
IntSoftmax | n:  29
IntGELU    | n:  25
IntSoftmax | n:  29
IntGELU    | n:  25
IntSoftmax | n:  29
IntGELU    | n:  25
IntSoftmax | n:  29
IntGELU    | n:  25
IntSoftmax | n:  29
IntGELU    | n:  25
.calib done
Test: [  0/391]	Time  3.895 ( 3.895)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=29, intgelu_exp_n=26
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=29, intgelu_exp_n=26, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  29
IntGELU    | n:  26
IntSoftmax | n:  29
IntGELU    | n:  26
IntSoftmax | n:  29
IntGELU    | n:  26
IntSoftmax | n:  29
IntGELU    | n:  26
IntSoftmax | n:  29
IntGELU    | n:  26
IntSoftmax | n:  29
IntGELU    | n:  26
IntSoftmax | n:  29
IntGELU    | n:  26
IntSoftmax | n:  29
IntGELU    | n:  26
IntSoftmax | n:  29
IntGELU    | n:  26
IntSoftmax | n:  29
IntGELU    | n:  26
IntSoftmax | n:  29
IntGELU    | n:  26
IntSoftmax | n:  29
IntGELU    | n:  26
.calib done
Test: [  0/391]	Time  3.836 ( 3.836)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.86
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=29, intgelu_exp_n=27
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=29, intgelu_exp_n=27, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  29
IntGELU    | n:  27
IntSoftmax | n:  29
IntGELU    | n:  27
IntSoftmax | n:  29
IntGELU    | n:  27
IntSoftmax | n:  29
IntGELU    | n:  27
IntSoftmax | n:  29
IntGELU    | n:  27
IntSoftmax | n:  29
IntGELU    | n:  27
IntSoftmax | n:  29
IntGELU    | n:  27
IntSoftmax | n:  29
IntGELU    | n:  27
IntSoftmax | n:  29
IntGELU    | n:  27
IntSoftmax | n:  29
IntGELU    | n:  27
IntSoftmax | n:  29
IntGELU    | n:  27
IntSoftmax | n:  29
IntGELU    | n:  27
.calib done
Test: [  0/391]	Time  3.956 ( 3.956)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.08
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=29, intgelu_exp_n=28
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=29, intgelu_exp_n=28, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  29
IntGELU    | n:  28
IntSoftmax | n:  29
IntGELU    | n:  28
IntSoftmax | n:  29
IntGELU    | n:  28
IntSoftmax | n:  29
IntGELU    | n:  28
IntSoftmax | n:  29
IntGELU    | n:  28
IntSoftmax | n:  29
IntGELU    | n:  28
IntSoftmax | n:  29
IntGELU    | n:  28
IntSoftmax | n:  29
IntGELU    | n:  28
IntSoftmax | n:  29
IntGELU    | n:  28
IntSoftmax | n:  29
IntGELU    | n:  28
IntSoftmax | n:  29
IntGELU    | n:  28
IntSoftmax | n:  29
IntGELU    | n:  28
.calib done
Test: [  0/391]	Time  3.959 ( 3.959)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.95
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=29, intgelu_exp_n=29
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=29, intgelu_exp_n=29, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  29
IntGELU    | n:  29
IntSoftmax | n:  29
IntGELU    | n:  29
IntSoftmax | n:  29
IntGELU    | n:  29
IntSoftmax | n:  29
IntGELU    | n:  29
IntSoftmax | n:  29
IntGELU    | n:  29
IntSoftmax | n:  29
IntGELU    | n:  29
IntSoftmax | n:  29
IntGELU    | n:  29
IntSoftmax | n:  29
IntGELU    | n:  29
IntSoftmax | n:  29
IntGELU    | n:  29
IntSoftmax | n:  29
IntGELU    | n:  29
IntSoftmax | n:  29
IntGELU    | n:  29
IntSoftmax | n:  29
IntGELU    | n:  29
.calib done
Test: [  0/391]	Time  3.670 ( 3.670)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.77
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=29, intgelu_exp_n=30
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=29, intgelu_exp_n=30, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  29
IntGELU    | n:  30
IntSoftmax | n:  29
IntGELU    | n:  30
IntSoftmax | n:  29
IntGELU    | n:  30
IntSoftmax | n:  29
IntGELU    | n:  30
IntSoftmax | n:  29
IntGELU    | n:  30
IntSoftmax | n:  29
IntGELU    | n:  30
IntSoftmax | n:  29
IntGELU    | n:  30
IntSoftmax | n:  29
IntGELU    | n:  30
IntSoftmax | n:  29
IntGELU    | n:  30
IntSoftmax | n:  29
IntGELU    | n:  30
IntSoftmax | n:  29
IntGELU    | n:  30
IntSoftmax | n:  29
IntGELU    | n:  30
.calib done
Test: [  0/391]	Time  3.863 ( 3.863)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=29, intgelu_exp_n=31
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=29, intgelu_exp_n=31, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  29
IntGELU    | n:  31
IntSoftmax | n:  29
IntGELU    | n:  31
IntSoftmax | n:  29
IntGELU    | n:  31
IntSoftmax | n:  29
IntGELU    | n:  31
IntSoftmax | n:  29
IntGELU    | n:  31
IntSoftmax | n:  29
IntGELU    | n:  31
IntSoftmax | n:  29
IntGELU    | n:  31
IntSoftmax | n:  29
IntGELU    | n:  31
IntSoftmax | n:  29
IntGELU    | n:  31
IntSoftmax | n:  29
IntGELU    | n:  31
IntSoftmax | n:  29
IntGELU    | n:  31
IntSoftmax | n:  29
IntGELU    | n:  31
.calib done
Test: [  0/391]	Time  3.841 ( 3.841)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.83
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=30, intgelu_exp_n=0
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=30, intgelu_exp_n=0, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  30
IntGELU    | n:  0
IntSoftmax | n:  30
IntGELU    | n:  0
IntSoftmax | n:  30
IntGELU    | n:  0
IntSoftmax | n:  30
IntGELU    | n:  0
IntSoftmax | n:  30
IntGELU    | n:  0
IntSoftmax | n:  30
IntGELU    | n:  0
IntSoftmax | n:  30
IntGELU    | n:  0
IntSoftmax | n:  30
IntGELU    | n:  0
IntSoftmax | n:  30
IntGELU    | n:  0
IntSoftmax | n:  30
IntGELU    | n:  0
IntSoftmax | n:  30
IntGELU    | n:  0
IntSoftmax | n:  30
IntGELU    | n:  0
.calib done
Test: [  0/391]	Time  3.917 ( 3.917)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=30, intgelu_exp_n=1
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=30, intgelu_exp_n=1, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  30
IntGELU    | n:  1
IntSoftmax | n:  30
IntGELU    | n:  1
IntSoftmax | n:  30
IntGELU    | n:  1
IntSoftmax | n:  30
IntGELU    | n:  1
IntSoftmax | n:  30
IntGELU    | n:  1
IntSoftmax | n:  30
IntGELU    | n:  1
IntSoftmax | n:  30
IntGELU    | n:  1
IntSoftmax | n:  30
IntGELU    | n:  1
IntSoftmax | n:  30
IntGELU    | n:  1
IntSoftmax | n:  30
IntGELU    | n:  1
IntSoftmax | n:  30
IntGELU    | n:  1
IntSoftmax | n:  30
IntGELU    | n:  1
.calib done
Test: [  0/391]	Time  3.818 ( 3.818)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.82
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=30, intgelu_exp_n=2
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=30, intgelu_exp_n=2, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  30
IntGELU    | n:  2
IntSoftmax | n:  30
IntGELU    | n:  2
IntSoftmax | n:  30
IntGELU    | n:  2
IntSoftmax | n:  30
IntGELU    | n:  2
IntSoftmax | n:  30
IntGELU    | n:  2
IntSoftmax | n:  30
IntGELU    | n:  2
IntSoftmax | n:  30
IntGELU    | n:  2
IntSoftmax | n:  30
IntGELU    | n:  2
IntSoftmax | n:  30
IntGELU    | n:  2
IntSoftmax | n:  30
IntGELU    | n:  2
IntSoftmax | n:  30
IntGELU    | n:  2
IntSoftmax | n:  30
IntGELU    | n:  2
.calib done
Test: [  0/391]	Time  3.836 ( 3.836)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.87
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=30, intgelu_exp_n=3
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=30, intgelu_exp_n=3, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  30
IntGELU    | n:  3
IntSoftmax | n:  30
IntGELU    | n:  3
IntSoftmax | n:  30
IntGELU    | n:  3
IntSoftmax | n:  30
IntGELU    | n:  3
IntSoftmax | n:  30
IntGELU    | n:  3
IntSoftmax | n:  30
IntGELU    | n:  3
IntSoftmax | n:  30
IntGELU    | n:  3
IntSoftmax | n:  30
IntGELU    | n:  3
IntSoftmax | n:  30
IntGELU    | n:  3
IntSoftmax | n:  30
IntGELU    | n:  3
IntSoftmax | n:  30
IntGELU    | n:  3
IntSoftmax | n:  30
IntGELU    | n:  3
.calib done
Test: [  0/391]	Time  3.968 ( 3.968)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=30, intgelu_exp_n=4
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=30, intgelu_exp_n=4, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  30
IntGELU    | n:  4
IntSoftmax | n:  30
IntGELU    | n:  4
IntSoftmax | n:  30
IntGELU    | n:  4
IntSoftmax | n:  30
IntGELU    | n:  4
IntSoftmax | n:  30
IntGELU    | n:  4
IntSoftmax | n:  30
IntGELU    | n:  4
IntSoftmax | n:  30
IntGELU    | n:  4
IntSoftmax | n:  30
IntGELU    | n:  4
IntSoftmax | n:  30
IntGELU    | n:  4
IntSoftmax | n:  30
IntGELU    | n:  4
IntSoftmax | n:  30
IntGELU    | n:  4
IntSoftmax | n:  30
IntGELU    | n:  4
.calib done
Test: [  0/391]	Time  3.855 ( 3.855)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.88
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=30, intgelu_exp_n=5
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=30, intgelu_exp_n=5, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  30
IntGELU    | n:  5
IntSoftmax | n:  30
IntGELU    | n:  5
IntSoftmax | n:  30
IntGELU    | n:  5
IntSoftmax | n:  30
IntGELU    | n:  5
IntSoftmax | n:  30
IntGELU    | n:  5
IntSoftmax | n:  30
IntGELU    | n:  5
IntSoftmax | n:  30
IntGELU    | n:  5
IntSoftmax | n:  30
IntGELU    | n:  5
IntSoftmax | n:  30
IntGELU    | n:  5
IntSoftmax | n:  30
IntGELU    | n:  5
IntSoftmax | n:  30
IntGELU    | n:  5
IntSoftmax | n:  30
IntGELU    | n:  5
.calib done
Test: [  0/391]	Time  3.675 ( 3.675)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.71
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=30, intgelu_exp_n=6
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=30, intgelu_exp_n=6, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  30
IntGELU    | n:  6
IntSoftmax | n:  30
IntGELU    | n:  6
IntSoftmax | n:  30
IntGELU    | n:  6
IntSoftmax | n:  30
IntGELU    | n:  6
IntSoftmax | n:  30
IntGELU    | n:  6
IntSoftmax | n:  30
IntGELU    | n:  6
IntSoftmax | n:  30
IntGELU    | n:  6
IntSoftmax | n:  30
IntGELU    | n:  6
IntSoftmax | n:  30
IntGELU    | n:  6
IntSoftmax | n:  30
IntGELU    | n:  6
IntSoftmax | n:  30
IntGELU    | n:  6
IntSoftmax | n:  30
IntGELU    | n:  6
.calib done
Test: [  0/391]	Time  3.675 ( 3.675)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.76
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=30, intgelu_exp_n=7
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=30, intgelu_exp_n=7, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  30
IntGELU    | n:  7
IntSoftmax | n:  30
IntGELU    | n:  7
IntSoftmax | n:  30
IntGELU    | n:  7
IntSoftmax | n:  30
IntGELU    | n:  7
IntSoftmax | n:  30
IntGELU    | n:  7
IntSoftmax | n:  30
IntGELU    | n:  7
IntSoftmax | n:  30
IntGELU    | n:  7
IntSoftmax | n:  30
IntGELU    | n:  7
IntSoftmax | n:  30
IntGELU    | n:  7
IntSoftmax | n:  30
IntGELU    | n:  7
IntSoftmax | n:  30
IntGELU    | n:  7
IntSoftmax | n:  30
IntGELU    | n:  7
.calib done
Test: [  0/391]	Time  3.893 ( 3.893)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.95
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=30, intgelu_exp_n=8
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=30, intgelu_exp_n=8, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  30
IntGELU    | n:  8
IntSoftmax | n:  30
IntGELU    | n:  8
IntSoftmax | n:  30
IntGELU    | n:  8
IntSoftmax | n:  30
IntGELU    | n:  8
IntSoftmax | n:  30
IntGELU    | n:  8
IntSoftmax | n:  30
IntGELU    | n:  8
IntSoftmax | n:  30
IntGELU    | n:  8
IntSoftmax | n:  30
IntGELU    | n:  8
IntSoftmax | n:  30
IntGELU    | n:  8
IntSoftmax | n:  30
IntGELU    | n:  8
IntSoftmax | n:  30
IntGELU    | n:  8
IntSoftmax | n:  30
IntGELU    | n:  8
.calib done
Test: [  0/391]	Time  3.948 ( 3.948)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.98
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=30, intgelu_exp_n=9
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=30, intgelu_exp_n=9, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  30
IntGELU    | n:  9
IntSoftmax | n:  30
IntGELU    | n:  9
IntSoftmax | n:  30
IntGELU    | n:  9
IntSoftmax | n:  30
IntGELU    | n:  9
IntSoftmax | n:  30
IntGELU    | n:  9
IntSoftmax | n:  30
IntGELU    | n:  9
IntSoftmax | n:  30
IntGELU    | n:  9
IntSoftmax | n:  30
IntGELU    | n:  9
IntSoftmax | n:  30
IntGELU    | n:  9
IntSoftmax | n:  30
IntGELU    | n:  9
IntSoftmax | n:  30
IntGELU    | n:  9
IntSoftmax | n:  30
IntGELU    | n:  9
.calib done
Test: [  0/391]	Time  3.682 ( 3.682)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.74
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=30, intgelu_exp_n=10
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=30, intgelu_exp_n=10, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  30
IntGELU    | n:  10
IntSoftmax | n:  30
IntGELU    | n:  10
IntSoftmax | n:  30
IntGELU    | n:  10
IntSoftmax | n:  30
IntGELU    | n:  10
IntSoftmax | n:  30
IntGELU    | n:  10
IntSoftmax | n:  30
IntGELU    | n:  10
IntSoftmax | n:  30
IntGELU    | n:  10
IntSoftmax | n:  30
IntGELU    | n:  10
IntSoftmax | n:  30
IntGELU    | n:  10
IntSoftmax | n:  30
IntGELU    | n:  10
IntSoftmax | n:  30
IntGELU    | n:  10
IntSoftmax | n:  30
IntGELU    | n:  10
.calib done
Test: [  0/391]	Time  3.954 ( 3.954)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.99
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=30, intgelu_exp_n=11
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=30, intgelu_exp_n=11, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  30
IntGELU    | n:  11
IntSoftmax | n:  30
IntGELU    | n:  11
IntSoftmax | n:  30
IntGELU    | n:  11
IntSoftmax | n:  30
IntGELU    | n:  11
IntSoftmax | n:  30
IntGELU    | n:  11
IntSoftmax | n:  30
IntGELU    | n:  11
IntSoftmax | n:  30
IntGELU    | n:  11
IntSoftmax | n:  30
IntGELU    | n:  11
IntSoftmax | n:  30
IntGELU    | n:  11
IntSoftmax | n:  30
IntGELU    | n:  11
IntSoftmax | n:  30
IntGELU    | n:  11
IntSoftmax | n:  30
IntGELU    | n:  11
.calib done
Test: [  0/391]	Time  3.840 ( 3.840)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.88
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=30, intgelu_exp_n=12
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=30, intgelu_exp_n=12, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  30
IntGELU    | n:  12
IntSoftmax | n:  30
IntGELU    | n:  12
IntSoftmax | n:  30
IntGELU    | n:  12
IntSoftmax | n:  30
IntGELU    | n:  12
IntSoftmax | n:  30
IntGELU    | n:  12
IntSoftmax | n:  30
IntGELU    | n:  12
IntSoftmax | n:  30
IntGELU    | n:  12
IntSoftmax | n:  30
IntGELU    | n:  12
IntSoftmax | n:  30
IntGELU    | n:  12
IntSoftmax | n:  30
IntGELU    | n:  12
IntSoftmax | n:  30
IntGELU    | n:  12
IntSoftmax | n:  30
IntGELU    | n:  12
.calib done
Test: [  0/391]	Time  3.881 ( 3.881)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=30, intgelu_exp_n=13
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=30, intgelu_exp_n=13, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  30
IntGELU    | n:  13
IntSoftmax | n:  30
IntGELU    | n:  13
IntSoftmax | n:  30
IntGELU    | n:  13
IntSoftmax | n:  30
IntGELU    | n:  13
IntSoftmax | n:  30
IntGELU    | n:  13
IntSoftmax | n:  30
IntGELU    | n:  13
IntSoftmax | n:  30
IntGELU    | n:  13
IntSoftmax | n:  30
IntGELU    | n:  13
IntSoftmax | n:  30
IntGELU    | n:  13
IntSoftmax | n:  30
IntGELU    | n:  13
IntSoftmax | n:  30
IntGELU    | n:  13
IntSoftmax | n:  30
IntGELU    | n:  13
.calib done
Test: [  0/391]	Time  3.833 ( 3.833)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.88
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=30, intgelu_exp_n=14
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=30, intgelu_exp_n=14, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  30
IntGELU    | n:  14
IntSoftmax | n:  30
IntGELU    | n:  14
IntSoftmax | n:  30
IntGELU    | n:  14
IntSoftmax | n:  30
IntGELU    | n:  14
IntSoftmax | n:  30
IntGELU    | n:  14
IntSoftmax | n:  30
IntGELU    | n:  14
IntSoftmax | n:  30
IntGELU    | n:  14
IntSoftmax | n:  30
IntGELU    | n:  14
IntSoftmax | n:  30
IntGELU    | n:  14
IntSoftmax | n:  30
IntGELU    | n:  14
IntSoftmax | n:  30
IntGELU    | n:  14
IntSoftmax | n:  30
IntGELU    | n:  14
.calib done
Test: [  0/391]	Time  3.813 ( 3.813)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.90
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=30, intgelu_exp_n=15
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=30, intgelu_exp_n=15, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  30
IntGELU    | n:  15
IntSoftmax | n:  30
IntGELU    | n:  15
IntSoftmax | n:  30
IntGELU    | n:  15
IntSoftmax | n:  30
IntGELU    | n:  15
IntSoftmax | n:  30
IntGELU    | n:  15
IntSoftmax | n:  30
IntGELU    | n:  15
IntSoftmax | n:  30
IntGELU    | n:  15
IntSoftmax | n:  30
IntGELU    | n:  15
IntSoftmax | n:  30
IntGELU    | n:  15
IntSoftmax | n:  30
IntGELU    | n:  15
IntSoftmax | n:  30
IntGELU    | n:  15
IntSoftmax | n:  30
IntGELU    | n:  15
.calib done
Test: [  0/391]	Time  3.892 ( 3.892)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=30, intgelu_exp_n=16
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=30, intgelu_exp_n=16, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  30
IntGELU    | n:  16
IntSoftmax | n:  30
IntGELU    | n:  16
IntSoftmax | n:  30
IntGELU    | n:  16
IntSoftmax | n:  30
IntGELU    | n:  16
IntSoftmax | n:  30
IntGELU    | n:  16
IntSoftmax | n:  30
IntGELU    | n:  16
IntSoftmax | n:  30
IntGELU    | n:  16
IntSoftmax | n:  30
IntGELU    | n:  16
IntSoftmax | n:  30
IntGELU    | n:  16
IntSoftmax | n:  30
IntGELU    | n:  16
IntSoftmax | n:  30
IntGELU    | n:  16
IntSoftmax | n:  30
IntGELU    | n:  16
.calib done
Test: [  0/391]	Time  3.949 ( 3.949)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.03
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=30, intgelu_exp_n=17
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=30, intgelu_exp_n=17, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  30
IntGELU    | n:  17
IntSoftmax | n:  30
IntGELU    | n:  17
IntSoftmax | n:  30
IntGELU    | n:  17
IntSoftmax | n:  30
IntGELU    | n:  17
IntSoftmax | n:  30
IntGELU    | n:  17
IntSoftmax | n:  30
IntGELU    | n:  17
IntSoftmax | n:  30
IntGELU    | n:  17
IntSoftmax | n:  30
IntGELU    | n:  17
IntSoftmax | n:  30
IntGELU    | n:  17
IntSoftmax | n:  30
IntGELU    | n:  17
IntSoftmax | n:  30
IntGELU    | n:  17
IntSoftmax | n:  30
IntGELU    | n:  17
.calib done
Test: [  0/391]	Time  3.868 ( 3.868)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.93
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=30, intgelu_exp_n=18
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=30, intgelu_exp_n=18, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  30
IntGELU    | n:  18
IntSoftmax | n:  30
IntGELU    | n:  18
IntSoftmax | n:  30
IntGELU    | n:  18
IntSoftmax | n:  30
IntGELU    | n:  18
IntSoftmax | n:  30
IntGELU    | n:  18
IntSoftmax | n:  30
IntGELU    | n:  18
IntSoftmax | n:  30
IntGELU    | n:  18
IntSoftmax | n:  30
IntGELU    | n:  18
IntSoftmax | n:  30
IntGELU    | n:  18
IntSoftmax | n:  30
IntGELU    | n:  18
IntSoftmax | n:  30
IntGELU    | n:  18
IntSoftmax | n:  30
IntGELU    | n:  18
.calib done
Test: [  0/391]	Time  3.932 ( 3.932)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.01
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=30, intgelu_exp_n=19
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=30, intgelu_exp_n=19, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  30
IntGELU    | n:  19
IntSoftmax | n:  30
IntGELU    | n:  19
IntSoftmax | n:  30
IntGELU    | n:  19
IntSoftmax | n:  30
IntGELU    | n:  19
IntSoftmax | n:  30
IntGELU    | n:  19
IntSoftmax | n:  30
IntGELU    | n:  19
IntSoftmax | n:  30
IntGELU    | n:  19
IntSoftmax | n:  30
IntGELU    | n:  19
IntSoftmax | n:  30
IntGELU    | n:  19
IntSoftmax | n:  30
IntGELU    | n:  19
IntSoftmax | n:  30
IntGELU    | n:  19
IntSoftmax | n:  30
IntGELU    | n:  19
.calib done
Test: [  0/391]	Time  3.897 ( 3.897)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=30, intgelu_exp_n=20
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=30, intgelu_exp_n=20, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  30
IntGELU    | n:  20
IntSoftmax | n:  30
IntGELU    | n:  20
IntSoftmax | n:  30
IntGELU    | n:  20
IntSoftmax | n:  30
IntGELU    | n:  20
IntSoftmax | n:  30
IntGELU    | n:  20
IntSoftmax | n:  30
IntGELU    | n:  20
IntSoftmax | n:  30
IntGELU    | n:  20
IntSoftmax | n:  30
IntGELU    | n:  20
IntSoftmax | n:  30
IntGELU    | n:  20
IntSoftmax | n:  30
IntGELU    | n:  20
IntSoftmax | n:  30
IntGELU    | n:  20
IntSoftmax | n:  30
IntGELU    | n:  20
.calib done
Test: [  0/391]	Time  3.989 ( 3.989)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.02
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=30, intgelu_exp_n=21
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=30, intgelu_exp_n=21, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  30
IntGELU    | n:  21
IntSoftmax | n:  30
IntGELU    | n:  21
IntSoftmax | n:  30
IntGELU    | n:  21
IntSoftmax | n:  30
IntGELU    | n:  21
IntSoftmax | n:  30
IntGELU    | n:  21
IntSoftmax | n:  30
IntGELU    | n:  21
IntSoftmax | n:  30
IntGELU    | n:  21
IntSoftmax | n:  30
IntGELU    | n:  21
IntSoftmax | n:  30
IntGELU    | n:  21
IntSoftmax | n:  30
IntGELU    | n:  21
IntSoftmax | n:  30
IntGELU    | n:  21
IntSoftmax | n:  30
IntGELU    | n:  21
.calib done
Test: [  0/391]	Time  3.804 ( 3.804)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.84
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=30, intgelu_exp_n=22
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=30, intgelu_exp_n=22, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  30
IntGELU    | n:  22
IntSoftmax | n:  30
IntGELU    | n:  22
IntSoftmax | n:  30
IntGELU    | n:  22
IntSoftmax | n:  30
IntGELU    | n:  22
IntSoftmax | n:  30
IntGELU    | n:  22
IntSoftmax | n:  30
IntGELU    | n:  22
IntSoftmax | n:  30
IntGELU    | n:  22
IntSoftmax | n:  30
IntGELU    | n:  22
IntSoftmax | n:  30
IntGELU    | n:  22
IntSoftmax | n:  30
IntGELU    | n:  22
IntSoftmax | n:  30
IntGELU    | n:  22
IntSoftmax | n:  30
IntGELU    | n:  22
.calib done
Test: [  0/391]	Time  3.827 ( 3.827)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.95
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=30, intgelu_exp_n=23
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=30, intgelu_exp_n=23, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  30
IntGELU    | n:  23
IntSoftmax | n:  30
IntGELU    | n:  23
IntSoftmax | n:  30
IntGELU    | n:  23
IntSoftmax | n:  30
IntGELU    | n:  23
IntSoftmax | n:  30
IntGELU    | n:  23
IntSoftmax | n:  30
IntGELU    | n:  23
IntSoftmax | n:  30
IntGELU    | n:  23
IntSoftmax | n:  30
IntGELU    | n:  23
IntSoftmax | n:  30
IntGELU    | n:  23
IntSoftmax | n:  30
IntGELU    | n:  23
IntSoftmax | n:  30
IntGELU    | n:  23
IntSoftmax | n:  30
IntGELU    | n:  23
.calib done
Test: [  0/391]	Time  3.973 ( 3.973)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.03
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=30, intgelu_exp_n=24
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=30, intgelu_exp_n=24, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  30
IntGELU    | n:  24
IntSoftmax | n:  30
IntGELU    | n:  24
IntSoftmax | n:  30
IntGELU    | n:  24
IntSoftmax | n:  30
IntGELU    | n:  24
IntSoftmax | n:  30
IntGELU    | n:  24
IntSoftmax | n:  30
IntGELU    | n:  24
IntSoftmax | n:  30
IntGELU    | n:  24
IntSoftmax | n:  30
IntGELU    | n:  24
IntSoftmax | n:  30
IntGELU    | n:  24
IntSoftmax | n:  30
IntGELU    | n:  24
IntSoftmax | n:  30
IntGELU    | n:  24
IntSoftmax | n:  30
IntGELU    | n:  24
.calib done
Test: [  0/391]	Time  3.858 ( 3.858)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=30, intgelu_exp_n=25
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=30, intgelu_exp_n=25, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  30
IntGELU    | n:  25
IntSoftmax | n:  30
IntGELU    | n:  25
IntSoftmax | n:  30
IntGELU    | n:  25
IntSoftmax | n:  30
IntGELU    | n:  25
IntSoftmax | n:  30
IntGELU    | n:  25
IntSoftmax | n:  30
IntGELU    | n:  25
IntSoftmax | n:  30
IntGELU    | n:  25
IntSoftmax | n:  30
IntGELU    | n:  25
IntSoftmax | n:  30
IntGELU    | n:  25
IntSoftmax | n:  30
IntGELU    | n:  25
IntSoftmax | n:  30
IntGELU    | n:  25
IntSoftmax | n:  30
IntGELU    | n:  25
.calib done
Test: [  0/391]	Time  3.825 ( 3.825)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.83
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=30, intgelu_exp_n=26
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=30, intgelu_exp_n=26, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  30
IntGELU    | n:  26
IntSoftmax | n:  30
IntGELU    | n:  26
IntSoftmax | n:  30
IntGELU    | n:  26
IntSoftmax | n:  30
IntGELU    | n:  26
IntSoftmax | n:  30
IntGELU    | n:  26
IntSoftmax | n:  30
IntGELU    | n:  26
IntSoftmax | n:  30
IntGELU    | n:  26
IntSoftmax | n:  30
IntGELU    | n:  26
IntSoftmax | n:  30
IntGELU    | n:  26
IntSoftmax | n:  30
IntGELU    | n:  26
IntSoftmax | n:  30
IntGELU    | n:  26
IntSoftmax | n:  30
IntGELU    | n:  26
.calib done
Test: [  0/391]	Time  3.880 ( 3.880)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.93
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=30, intgelu_exp_n=27
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=30, intgelu_exp_n=27, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  30
IntGELU    | n:  27
IntSoftmax | n:  30
IntGELU    | n:  27
IntSoftmax | n:  30
IntGELU    | n:  27
IntSoftmax | n:  30
IntGELU    | n:  27
IntSoftmax | n:  30
IntGELU    | n:  27
IntSoftmax | n:  30
IntGELU    | n:  27
IntSoftmax | n:  30
IntGELU    | n:  27
IntSoftmax | n:  30
IntGELU    | n:  27
IntSoftmax | n:  30
IntGELU    | n:  27
IntSoftmax | n:  30
IntGELU    | n:  27
IntSoftmax | n:  30
IntGELU    | n:  27
IntSoftmax | n:  30
IntGELU    | n:  27
.calib done
Test: [  0/391]	Time  3.895 ( 3.895)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.95
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=30, intgelu_exp_n=28
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=30, intgelu_exp_n=28, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  30
IntGELU    | n:  28
IntSoftmax | n:  30
IntGELU    | n:  28
IntSoftmax | n:  30
IntGELU    | n:  28
IntSoftmax | n:  30
IntGELU    | n:  28
IntSoftmax | n:  30
IntGELU    | n:  28
IntSoftmax | n:  30
IntGELU    | n:  28
IntSoftmax | n:  30
IntGELU    | n:  28
IntSoftmax | n:  30
IntGELU    | n:  28
IntSoftmax | n:  30
IntGELU    | n:  28
IntSoftmax | n:  30
IntGELU    | n:  28
IntSoftmax | n:  30
IntGELU    | n:  28
IntSoftmax | n:  30
IntGELU    | n:  28
.calib done
Test: [  0/391]	Time  3.727 ( 3.727)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.75
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=30, intgelu_exp_n=29
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=30, intgelu_exp_n=29, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  30
IntGELU    | n:  29
IntSoftmax | n:  30
IntGELU    | n:  29
IntSoftmax | n:  30
IntGELU    | n:  29
IntSoftmax | n:  30
IntGELU    | n:  29
IntSoftmax | n:  30
IntGELU    | n:  29
IntSoftmax | n:  30
IntGELU    | n:  29
IntSoftmax | n:  30
IntGELU    | n:  29
IntSoftmax | n:  30
IntGELU    | n:  29
IntSoftmax | n:  30
IntGELU    | n:  29
IntSoftmax | n:  30
IntGELU    | n:  29
IntSoftmax | n:  30
IntGELU    | n:  29
IntSoftmax | n:  30
IntGELU    | n:  29
.calib done
Test: [  0/391]	Time  3.889 ( 3.889)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.95
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=30, intgelu_exp_n=30
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=30, intgelu_exp_n=30, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  30
IntGELU    | n:  30
IntSoftmax | n:  30
IntGELU    | n:  30
IntSoftmax | n:  30
IntGELU    | n:  30
IntSoftmax | n:  30
IntGELU    | n:  30
IntSoftmax | n:  30
IntGELU    | n:  30
IntSoftmax | n:  30
IntGELU    | n:  30
IntSoftmax | n:  30
IntGELU    | n:  30
IntSoftmax | n:  30
IntGELU    | n:  30
IntSoftmax | n:  30
IntGELU    | n:  30
IntSoftmax | n:  30
IntGELU    | n:  30
IntSoftmax | n:  30
IntGELU    | n:  30
IntSoftmax | n:  30
IntGELU    | n:  30
.calib done
Test: [  0/391]	Time  3.995 ( 3.995)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.02
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=30, intgelu_exp_n=31
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=30, intgelu_exp_n=31, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  30
IntGELU    | n:  31
IntSoftmax | n:  30
IntGELU    | n:  31
IntSoftmax | n:  30
IntGELU    | n:  31
IntSoftmax | n:  30
IntGELU    | n:  31
IntSoftmax | n:  30
IntGELU    | n:  31
IntSoftmax | n:  30
IntGELU    | n:  31
IntSoftmax | n:  30
IntGELU    | n:  31
IntSoftmax | n:  30
IntGELU    | n:  31
IntSoftmax | n:  30
IntGELU    | n:  31
IntSoftmax | n:  30
IntGELU    | n:  31
IntSoftmax | n:  30
IntGELU    | n:  31
IntSoftmax | n:  30
IntGELU    | n:  31
.calib done
Test: [  0/391]	Time  3.866 ( 3.866)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.92
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=31, intgelu_exp_n=0
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=31, intgelu_exp_n=0, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  31
IntGELU    | n:  0
IntSoftmax | n:  31
IntGELU    | n:  0
IntSoftmax | n:  31
IntGELU    | n:  0
IntSoftmax | n:  31
IntGELU    | n:  0
IntSoftmax | n:  31
IntGELU    | n:  0
IntSoftmax | n:  31
IntGELU    | n:  0
IntSoftmax | n:  31
IntGELU    | n:  0
IntSoftmax | n:  31
IntGELU    | n:  0
IntSoftmax | n:  31
IntGELU    | n:  0
IntSoftmax | n:  31
IntGELU    | n:  0
IntSoftmax | n:  31
IntGELU    | n:  0
IntSoftmax | n:  31
IntGELU    | n:  0
.calib done
Test: [  0/391]	Time  3.876 ( 3.876)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.94
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=31, intgelu_exp_n=1
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=31, intgelu_exp_n=1, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  31
IntGELU    | n:  1
IntSoftmax | n:  31
IntGELU    | n:  1
IntSoftmax | n:  31
IntGELU    | n:  1
IntSoftmax | n:  31
IntGELU    | n:  1
IntSoftmax | n:  31
IntGELU    | n:  1
IntSoftmax | n:  31
IntGELU    | n:  1
IntSoftmax | n:  31
IntGELU    | n:  1
IntSoftmax | n:  31
IntGELU    | n:  1
IntSoftmax | n:  31
IntGELU    | n:  1
IntSoftmax | n:  31
IntGELU    | n:  1
IntSoftmax | n:  31
IntGELU    | n:  1
IntSoftmax | n:  31
IntGELU    | n:  1
.calib done
Test: [  0/391]	Time  3.818 ( 3.818)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.87
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=31, intgelu_exp_n=2
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=31, intgelu_exp_n=2, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  31
IntGELU    | n:  2
IntSoftmax | n:  31
IntGELU    | n:  2
IntSoftmax | n:  31
IntGELU    | n:  2
IntSoftmax | n:  31
IntGELU    | n:  2
IntSoftmax | n:  31
IntGELU    | n:  2
IntSoftmax | n:  31
IntGELU    | n:  2
IntSoftmax | n:  31
IntGELU    | n:  2
IntSoftmax | n:  31
IntGELU    | n:  2
IntSoftmax | n:  31
IntGELU    | n:  2
IntSoftmax | n:  31
IntGELU    | n:  2
IntSoftmax | n:  31
IntGELU    | n:  2
IntSoftmax | n:  31
IntGELU    | n:  2
.calib done
Test: [  0/391]	Time  3.848 ( 3.848)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.87
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=31, intgelu_exp_n=3
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=31, intgelu_exp_n=3, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  31
IntGELU    | n:  3
IntSoftmax | n:  31
IntGELU    | n:  3
IntSoftmax | n:  31
IntGELU    | n:  3
IntSoftmax | n:  31
IntGELU    | n:  3
IntSoftmax | n:  31
IntGELU    | n:  3
IntSoftmax | n:  31
IntGELU    | n:  3
IntSoftmax | n:  31
IntGELU    | n:  3
IntSoftmax | n:  31
IntGELU    | n:  3
IntSoftmax | n:  31
IntGELU    | n:  3
IntSoftmax | n:  31
IntGELU    | n:  3
IntSoftmax | n:  31
IntGELU    | n:  3
IntSoftmax | n:  31
IntGELU    | n:  3
.calib done
Test: [  0/391]	Time  3.815 ( 3.815)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.86
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=31, intgelu_exp_n=4
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=31, intgelu_exp_n=4, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  31
IntGELU    | n:  4
IntSoftmax | n:  31
IntGELU    | n:  4
IntSoftmax | n:  31
IntGELU    | n:  4
IntSoftmax | n:  31
IntGELU    | n:  4
IntSoftmax | n:  31
IntGELU    | n:  4
IntSoftmax | n:  31
IntGELU    | n:  4
IntSoftmax | n:  31
IntGELU    | n:  4
IntSoftmax | n:  31
IntGELU    | n:  4
IntSoftmax | n:  31
IntGELU    | n:  4
IntSoftmax | n:  31
IntGELU    | n:  4
IntSoftmax | n:  31
IntGELU    | n:  4
IntSoftmax | n:  31
IntGELU    | n:  4
.calib done
Test: [  0/391]	Time  3.845 ( 3.845)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=31, intgelu_exp_n=5
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=31, intgelu_exp_n=5, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  31
IntGELU    | n:  5
IntSoftmax | n:  31
IntGELU    | n:  5
IntSoftmax | n:  31
IntGELU    | n:  5
IntSoftmax | n:  31
IntGELU    | n:  5
IntSoftmax | n:  31
IntGELU    | n:  5
IntSoftmax | n:  31
IntGELU    | n:  5
IntSoftmax | n:  31
IntGELU    | n:  5
IntSoftmax | n:  31
IntGELU    | n:  5
IntSoftmax | n:  31
IntGELU    | n:  5
IntSoftmax | n:  31
IntGELU    | n:  5
IntSoftmax | n:  31
IntGELU    | n:  5
IntSoftmax | n:  31
IntGELU    | n:  5
.calib done
Test: [  0/391]	Time  3.941 ( 3.941)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.99
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=31, intgelu_exp_n=6
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=31, intgelu_exp_n=6, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  31
IntGELU    | n:  6
IntSoftmax | n:  31
IntGELU    | n:  6
IntSoftmax | n:  31
IntGELU    | n:  6
IntSoftmax | n:  31
IntGELU    | n:  6
IntSoftmax | n:  31
IntGELU    | n:  6
IntSoftmax | n:  31
IntGELU    | n:  6
IntSoftmax | n:  31
IntGELU    | n:  6
IntSoftmax | n:  31
IntGELU    | n:  6
IntSoftmax | n:  31
IntGELU    | n:  6
IntSoftmax | n:  31
IntGELU    | n:  6
IntSoftmax | n:  31
IntGELU    | n:  6
IntSoftmax | n:  31
IntGELU    | n:  6
.calib done
Test: [  0/391]	Time  3.881 ( 3.881)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.89
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=31, intgelu_exp_n=7
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=31, intgelu_exp_n=7, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  31
IntGELU    | n:  7
IntSoftmax | n:  31
IntGELU    | n:  7
IntSoftmax | n:  31
IntGELU    | n:  7
IntSoftmax | n:  31
IntGELU    | n:  7
IntSoftmax | n:  31
IntGELU    | n:  7
IntSoftmax | n:  31
IntGELU    | n:  7
IntSoftmax | n:  31
IntGELU    | n:  7
IntSoftmax | n:  31
IntGELU    | n:  7
IntSoftmax | n:  31
IntGELU    | n:  7
IntSoftmax | n:  31
IntGELU    | n:  7
IntSoftmax | n:  31
IntGELU    | n:  7
IntSoftmax | n:  31
IntGELU    | n:  7
.calib done
Test: [  0/391]	Time  3.906 ( 3.906)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=31, intgelu_exp_n=8
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=31, intgelu_exp_n=8, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  31
IntGELU    | n:  8
IntSoftmax | n:  31
IntGELU    | n:  8
IntSoftmax | n:  31
IntGELU    | n:  8
IntSoftmax | n:  31
IntGELU    | n:  8
IntSoftmax | n:  31
IntGELU    | n:  8
IntSoftmax | n:  31
IntGELU    | n:  8
IntSoftmax | n:  31
IntGELU    | n:  8
IntSoftmax | n:  31
IntGELU    | n:  8
IntSoftmax | n:  31
IntGELU    | n:  8
IntSoftmax | n:  31
IntGELU    | n:  8
IntSoftmax | n:  31
IntGELU    | n:  8
IntSoftmax | n:  31
IntGELU    | n:  8
.calib done
Test: [  0/391]	Time  3.680 ( 3.680)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.79
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=31, intgelu_exp_n=9
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=31, intgelu_exp_n=9, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  31
IntGELU    | n:  9
IntSoftmax | n:  31
IntGELU    | n:  9
IntSoftmax | n:  31
IntGELU    | n:  9
IntSoftmax | n:  31
IntGELU    | n:  9
IntSoftmax | n:  31
IntGELU    | n:  9
IntSoftmax | n:  31
IntGELU    | n:  9
IntSoftmax | n:  31
IntGELU    | n:  9
IntSoftmax | n:  31
IntGELU    | n:  9
IntSoftmax | n:  31
IntGELU    | n:  9
IntSoftmax | n:  31
IntGELU    | n:  9
IntSoftmax | n:  31
IntGELU    | n:  9
IntSoftmax | n:  31
IntGELU    | n:  9
.calib done
Test: [  0/391]	Time  4.023 ( 4.023)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.11
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=31, intgelu_exp_n=10
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=31, intgelu_exp_n=10, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  31
IntGELU    | n:  10
IntSoftmax | n:  31
IntGELU    | n:  10
IntSoftmax | n:  31
IntGELU    | n:  10
IntSoftmax | n:  31
IntGELU    | n:  10
IntSoftmax | n:  31
IntGELU    | n:  10
IntSoftmax | n:  31
IntGELU    | n:  10
IntSoftmax | n:  31
IntGELU    | n:  10
IntSoftmax | n:  31
IntGELU    | n:  10
IntSoftmax | n:  31
IntGELU    | n:  10
IntSoftmax | n:  31
IntGELU    | n:  10
IntSoftmax | n:  31
IntGELU    | n:  10
IntSoftmax | n:  31
IntGELU    | n:  10
.calib done
Test: [  0/391]	Time  3.886 ( 3.886)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.86
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=31, intgelu_exp_n=11
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=31, intgelu_exp_n=11, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  31
IntGELU    | n:  11
IntSoftmax | n:  31
IntGELU    | n:  11
IntSoftmax | n:  31
IntGELU    | n:  11
IntSoftmax | n:  31
IntGELU    | n:  11
IntSoftmax | n:  31
IntGELU    | n:  11
IntSoftmax | n:  31
IntGELU    | n:  11
IntSoftmax | n:  31
IntGELU    | n:  11
IntSoftmax | n:  31
IntGELU    | n:  11
IntSoftmax | n:  31
IntGELU    | n:  11
IntSoftmax | n:  31
IntGELU    | n:  11
IntSoftmax | n:  31
IntGELU    | n:  11
IntSoftmax | n:  31
IntGELU    | n:  11
.calib done
Test: [  0/391]	Time  3.898 ( 3.898)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.94
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=31, intgelu_exp_n=12
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=31, intgelu_exp_n=12, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  31
IntGELU    | n:  12
IntSoftmax | n:  31
IntGELU    | n:  12
IntSoftmax | n:  31
IntGELU    | n:  12
IntSoftmax | n:  31
IntGELU    | n:  12
IntSoftmax | n:  31
IntGELU    | n:  12
IntSoftmax | n:  31
IntGELU    | n:  12
IntSoftmax | n:  31
IntGELU    | n:  12
IntSoftmax | n:  31
IntGELU    | n:  12
IntSoftmax | n:  31
IntGELU    | n:  12
IntSoftmax | n:  31
IntGELU    | n:  12
IntSoftmax | n:  31
IntGELU    | n:  12
IntSoftmax | n:  31
IntGELU    | n:  12
.calib done
Test: [  0/391]	Time  3.758 ( 3.758)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.81
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=31, intgelu_exp_n=13
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=31, intgelu_exp_n=13, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  31
IntGELU    | n:  13
IntSoftmax | n:  31
IntGELU    | n:  13
IntSoftmax | n:  31
IntGELU    | n:  13
IntSoftmax | n:  31
IntGELU    | n:  13
IntSoftmax | n:  31
IntGELU    | n:  13
IntSoftmax | n:  31
IntGELU    | n:  13
IntSoftmax | n:  31
IntGELU    | n:  13
IntSoftmax | n:  31
IntGELU    | n:  13
IntSoftmax | n:  31
IntGELU    | n:  13
IntSoftmax | n:  31
IntGELU    | n:  13
IntSoftmax | n:  31
IntGELU    | n:  13
IntSoftmax | n:  31
IntGELU    | n:  13
.calib done
Test: [  0/391]	Time  3.887 ( 3.887)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.91
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=31, intgelu_exp_n=14
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=31, intgelu_exp_n=14, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  31
IntGELU    | n:  14
IntSoftmax | n:  31
IntGELU    | n:  14
IntSoftmax | n:  31
IntGELU    | n:  14
IntSoftmax | n:  31
IntGELU    | n:  14
IntSoftmax | n:  31
IntGELU    | n:  14
IntSoftmax | n:  31
IntGELU    | n:  14
IntSoftmax | n:  31
IntGELU    | n:  14
IntSoftmax | n:  31
IntGELU    | n:  14
IntSoftmax | n:  31
IntGELU    | n:  14
IntSoftmax | n:  31
IntGELU    | n:  14
IntSoftmax | n:  31
IntGELU    | n:  14
IntSoftmax | n:  31
IntGELU    | n:  14
.calib done
Test: [  0/391]	Time  3.888 ( 3.888)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=31, intgelu_exp_n=15
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=31, intgelu_exp_n=15, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  31
IntGELU    | n:  15
IntSoftmax | n:  31
IntGELU    | n:  15
IntSoftmax | n:  31
IntGELU    | n:  15
IntSoftmax | n:  31
IntGELU    | n:  15
IntSoftmax | n:  31
IntGELU    | n:  15
IntSoftmax | n:  31
IntGELU    | n:  15
IntSoftmax | n:  31
IntGELU    | n:  15
IntSoftmax | n:  31
IntGELU    | n:  15
IntSoftmax | n:  31
IntGELU    | n:  15
IntSoftmax | n:  31
IntGELU    | n:  15
IntSoftmax | n:  31
IntGELU    | n:  15
IntSoftmax | n:  31
IntGELU    | n:  15
.calib done
Test: [  0/391]	Time  3.835 ( 3.835)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.86
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=31, intgelu_exp_n=16
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=31, intgelu_exp_n=16, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  31
IntGELU    | n:  16
IntSoftmax | n:  31
IntGELU    | n:  16
IntSoftmax | n:  31
IntGELU    | n:  16
IntSoftmax | n:  31
IntGELU    | n:  16
IntSoftmax | n:  31
IntGELU    | n:  16
IntSoftmax | n:  31
IntGELU    | n:  16
IntSoftmax | n:  31
IntGELU    | n:  16
IntSoftmax | n:  31
IntGELU    | n:  16
IntSoftmax | n:  31
IntGELU    | n:  16
IntSoftmax | n:  31
IntGELU    | n:  16
IntSoftmax | n:  31
IntGELU    | n:  16
IntSoftmax | n:  31
IntGELU    | n:  16
.calib done
Test: [  0/391]	Time  3.901 ( 3.901)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=31, intgelu_exp_n=17
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=31, intgelu_exp_n=17, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  31
IntGELU    | n:  17
IntSoftmax | n:  31
IntGELU    | n:  17
IntSoftmax | n:  31
IntGELU    | n:  17
IntSoftmax | n:  31
IntGELU    | n:  17
IntSoftmax | n:  31
IntGELU    | n:  17
IntSoftmax | n:  31
IntGELU    | n:  17
IntSoftmax | n:  31
IntGELU    | n:  17
IntSoftmax | n:  31
IntGELU    | n:  17
IntSoftmax | n:  31
IntGELU    | n:  17
IntSoftmax | n:  31
IntGELU    | n:  17
IntSoftmax | n:  31
IntGELU    | n:  17
IntSoftmax | n:  31
IntGELU    | n:  17
.calib done
Test: [  0/391]	Time  3.741 ( 3.741)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.81
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=31, intgelu_exp_n=18
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=31, intgelu_exp_n=18, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  31
IntGELU    | n:  18
IntSoftmax | n:  31
IntGELU    | n:  18
IntSoftmax | n:  31
IntGELU    | n:  18
IntSoftmax | n:  31
IntGELU    | n:  18
IntSoftmax | n:  31
IntGELU    | n:  18
IntSoftmax | n:  31
IntGELU    | n:  18
IntSoftmax | n:  31
IntGELU    | n:  18
IntSoftmax | n:  31
IntGELU    | n:  18
IntSoftmax | n:  31
IntGELU    | n:  18
IntSoftmax | n:  31
IntGELU    | n:  18
IntSoftmax | n:  31
IntGELU    | n:  18
IntSoftmax | n:  31
IntGELU    | n:  18
.calib done
Test: [  0/391]	Time  3.893 ( 3.893)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.94
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=31, intgelu_exp_n=19
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=31, intgelu_exp_n=19, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  31
IntGELU    | n:  19
IntSoftmax | n:  31
IntGELU    | n:  19
IntSoftmax | n:  31
IntGELU    | n:  19
IntSoftmax | n:  31
IntGELU    | n:  19
IntSoftmax | n:  31
IntGELU    | n:  19
IntSoftmax | n:  31
IntGELU    | n:  19
IntSoftmax | n:  31
IntGELU    | n:  19
IntSoftmax | n:  31
IntGELU    | n:  19
IntSoftmax | n:  31
IntGELU    | n:  19
IntSoftmax | n:  31
IntGELU    | n:  19
IntSoftmax | n:  31
IntGELU    | n:  19
IntSoftmax | n:  31
IntGELU    | n:  19
.calib done
Test: [  0/391]	Time  3.897 ( 3.897)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.96
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=31, intgelu_exp_n=20
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=31, intgelu_exp_n=20, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  31
IntGELU    | n:  20
IntSoftmax | n:  31
IntGELU    | n:  20
IntSoftmax | n:  31
IntGELU    | n:  20
IntSoftmax | n:  31
IntGELU    | n:  20
IntSoftmax | n:  31
IntGELU    | n:  20
IntSoftmax | n:  31
IntGELU    | n:  20
IntSoftmax | n:  31
IntGELU    | n:  20
IntSoftmax | n:  31
IntGELU    | n:  20
IntSoftmax | n:  31
IntGELU    | n:  20
IntSoftmax | n:  31
IntGELU    | n:  20
IntSoftmax | n:  31
IntGELU    | n:  20
IntSoftmax | n:  31
IntGELU    | n:  20
.calib done
Test: [  0/391]	Time  3.703 ( 3.703)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.80
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=31, intgelu_exp_n=21
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=31, intgelu_exp_n=21, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  31
IntGELU    | n:  21
IntSoftmax | n:  31
IntGELU    | n:  21
IntSoftmax | n:  31
IntGELU    | n:  21
IntSoftmax | n:  31
IntGELU    | n:  21
IntSoftmax | n:  31
IntGELU    | n:  21
IntSoftmax | n:  31
IntGELU    | n:  21
IntSoftmax | n:  31
IntGELU    | n:  21
IntSoftmax | n:  31
IntGELU    | n:  21
IntSoftmax | n:  31
IntGELU    | n:  21
IntSoftmax | n:  31
IntGELU    | n:  21
IntSoftmax | n:  31
IntGELU    | n:  21
IntSoftmax | n:  31
IntGELU    | n:  21
.calib done
Test: [  0/391]	Time  3.835 ( 3.835)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=31, intgelu_exp_n=22
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=31, intgelu_exp_n=22, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  31
IntGELU    | n:  22
IntSoftmax | n:  31
IntGELU    | n:  22
IntSoftmax | n:  31
IntGELU    | n:  22
IntSoftmax | n:  31
IntGELU    | n:  22
IntSoftmax | n:  31
IntGELU    | n:  22
IntSoftmax | n:  31
IntGELU    | n:  22
IntSoftmax | n:  31
IntGELU    | n:  22
IntSoftmax | n:  31
IntGELU    | n:  22
IntSoftmax | n:  31
IntGELU    | n:  22
IntSoftmax | n:  31
IntGELU    | n:  22
IntSoftmax | n:  31
IntGELU    | n:  22
IntSoftmax | n:  31
IntGELU    | n:  22
.calib done
Test: [  0/391]	Time  3.887 ( 3.887)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.98
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=31, intgelu_exp_n=23
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=31, intgelu_exp_n=23, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  31
IntGELU    | n:  23
IntSoftmax | n:  31
IntGELU    | n:  23
IntSoftmax | n:  31
IntGELU    | n:  23
IntSoftmax | n:  31
IntGELU    | n:  23
IntSoftmax | n:  31
IntGELU    | n:  23
IntSoftmax | n:  31
IntGELU    | n:  23
IntSoftmax | n:  31
IntGELU    | n:  23
IntSoftmax | n:  31
IntGELU    | n:  23
IntSoftmax | n:  31
IntGELU    | n:  23
IntSoftmax | n:  31
IntGELU    | n:  23
IntSoftmax | n:  31
IntGELU    | n:  23
IntSoftmax | n:  31
IntGELU    | n:  23
.calib done
Test: [  0/391]	Time  3.801 ( 3.801)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.86
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=31, intgelu_exp_n=24
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=31, intgelu_exp_n=24, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  31
IntGELU    | n:  24
IntSoftmax | n:  31
IntGELU    | n:  24
IntSoftmax | n:  31
IntGELU    | n:  24
IntSoftmax | n:  31
IntGELU    | n:  24
IntSoftmax | n:  31
IntGELU    | n:  24
IntSoftmax | n:  31
IntGELU    | n:  24
IntSoftmax | n:  31
IntGELU    | n:  24
IntSoftmax | n:  31
IntGELU    | n:  24
IntSoftmax | n:  31
IntGELU    | n:  24
IntSoftmax | n:  31
IntGELU    | n:  24
IntSoftmax | n:  31
IntGELU    | n:  24
IntSoftmax | n:  31
IntGELU    | n:  24
.calib done
Test: [  0/391]	Time  3.905 ( 3.905)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.98
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=31, intgelu_exp_n=25
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=31, intgelu_exp_n=25, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  31
IntGELU    | n:  25
IntSoftmax | n:  31
IntGELU    | n:  25
IntSoftmax | n:  31
IntGELU    | n:  25
IntSoftmax | n:  31
IntGELU    | n:  25
IntSoftmax | n:  31
IntGELU    | n:  25
IntSoftmax | n:  31
IntGELU    | n:  25
IntSoftmax | n:  31
IntGELU    | n:  25
IntSoftmax | n:  31
IntGELU    | n:  25
IntSoftmax | n:  31
IntGELU    | n:  25
IntSoftmax | n:  31
IntGELU    | n:  25
IntSoftmax | n:  31
IntGELU    | n:  25
IntSoftmax | n:  31
IntGELU    | n:  25
.calib done
Test: [  0/391]	Time  3.979 ( 3.979)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.00
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=31, intgelu_exp_n=26
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=31, intgelu_exp_n=26, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  31
IntGELU    | n:  26
IntSoftmax | n:  31
IntGELU    | n:  26
IntSoftmax | n:  31
IntGELU    | n:  26
IntSoftmax | n:  31
IntGELU    | n:  26
IntSoftmax | n:  31
IntGELU    | n:  26
IntSoftmax | n:  31
IntGELU    | n:  26
IntSoftmax | n:  31
IntGELU    | n:  26
IntSoftmax | n:  31
IntGELU    | n:  26
IntSoftmax | n:  31
IntGELU    | n:  26
IntSoftmax | n:  31
IntGELU    | n:  26
IntSoftmax | n:  31
IntGELU    | n:  26
IntSoftmax | n:  31
IntGELU    | n:  26
.calib done
Test: [  0/391]	Time  4.000 ( 4.000)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 9.05
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=31, intgelu_exp_n=27
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=31, intgelu_exp_n=27, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  31
IntGELU    | n:  27
IntSoftmax | n:  31
IntGELU    | n:  27
IntSoftmax | n:  31
IntGELU    | n:  27
IntSoftmax | n:  31
IntGELU    | n:  27
IntSoftmax | n:  31
IntGELU    | n:  27
IntSoftmax | n:  31
IntGELU    | n:  27
IntSoftmax | n:  31
IntGELU    | n:  27
IntSoftmax | n:  31
IntGELU    | n:  27
IntSoftmax | n:  31
IntGELU    | n:  27
IntSoftmax | n:  31
IntGELU    | n:  27
IntSoftmax | n:  31
IntGELU    | n:  27
IntSoftmax | n:  31
IntGELU    | n:  27
.calib done
Test: [  0/391]	Time  3.857 ( 3.857)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.93
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=31, intgelu_exp_n=28
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=31, intgelu_exp_n=28, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  31
IntGELU    | n:  28
IntSoftmax | n:  31
IntGELU    | n:  28
IntSoftmax | n:  31
IntGELU    | n:  28
IntSoftmax | n:  31
IntGELU    | n:  28
IntSoftmax | n:  31
IntGELU    | n:  28
IntSoftmax | n:  31
IntGELU    | n:  28
IntSoftmax | n:  31
IntGELU    | n:  28
IntSoftmax | n:  31
IntGELU    | n:  28
IntSoftmax | n:  31
IntGELU    | n:  28
IntSoftmax | n:  31
IntGELU    | n:  28
IntSoftmax | n:  31
IntGELU    | n:  28
IntSoftmax | n:  31
IntGELU    | n:  28
.calib done
Test: [  0/391]	Time  3.740 ( 3.740)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.77
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=31, intgelu_exp_n=29
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=31, intgelu_exp_n=29, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  31
IntGELU    | n:  29
IntSoftmax | n:  31
IntGELU    | n:  29
IntSoftmax | n:  31
IntGELU    | n:  29
IntSoftmax | n:  31
IntGELU    | n:  29
IntSoftmax | n:  31
IntGELU    | n:  29
IntSoftmax | n:  31
IntGELU    | n:  29
IntSoftmax | n:  31
IntGELU    | n:  29
IntSoftmax | n:  31
IntGELU    | n:  29
IntSoftmax | n:  31
IntGELU    | n:  29
IntSoftmax | n:  31
IntGELU    | n:  29
IntSoftmax | n:  31
IntGELU    | n:  29
IntSoftmax | n:  31
IntGELU    | n:  29
.calib done
Test: [  0/391]	Time  3.897 ( 3.897)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.97
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=31, intgelu_exp_n=30
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=31, intgelu_exp_n=30, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  31
IntGELU    | n:  30
IntSoftmax | n:  31
IntGELU    | n:  30
IntSoftmax | n:  31
IntGELU    | n:  30
IntSoftmax | n:  31
IntGELU    | n:  30
IntSoftmax | n:  31
IntGELU    | n:  30
IntSoftmax | n:  31
IntGELU    | n:  30
IntSoftmax | n:  31
IntGELU    | n:  30
IntSoftmax | n:  31
IntGELU    | n:  30
IntSoftmax | n:  31
IntGELU    | n:  30
IntSoftmax | n:  31
IntGELU    | n:  30
IntSoftmax | n:  31
IntGELU    | n:  30
IntSoftmax | n:  31
IntGELU    | n:  30
.calib done
Test: [  0/391]	Time  3.842 ( 3.842)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.89
-----------------------------------------------------
Running experiment with attn_quant=NoQuant, intsoftmax_exp_n=31, intgelu_exp_n=31
Namespace(model='vit_base', dataset='data/ImageNet', nb_classes=1000, device='cuda', print_freq=50, seed=0, output_dir='results/', calib_batchsize=32, calib_images=32, val_batchsize=128, num_workers=8, intsoftmax_exp_n=31, intgelu_exp_n=31, attn_quant='NoQuant')
    Number of QuantLinear: 49
    Number of QuantConv2d: 1
    Number of QuantAct: 137
    Number of IntLayerNorm: 25
    Number of IntGELU: 12
    Number of IntSoftmax: 12
    Number of Log2_Quantizer: 0
    - type : NoQuant
    Number of QuantMatMul: 24
Model: vit_base_patch16_224
IntSoftmax | n:  31
IntGELU    | n:  31
IntSoftmax | n:  31
IntGELU    | n:  31
IntSoftmax | n:  31
IntGELU    | n:  31
IntSoftmax | n:  31
IntGELU    | n:  31
IntSoftmax | n:  31
IntGELU    | n:  31
IntSoftmax | n:  31
IntGELU    | n:  31
IntSoftmax | n:  31
IntGELU    | n:  31
IntSoftmax | n:  31
IntGELU    | n:  31
IntSoftmax | n:  31
IntGELU    | n:  31
IntSoftmax | n:  31
IntGELU    | n:  31
IntSoftmax | n:  31
IntGELU    | n:  31
IntSoftmax | n:  31
IntGELU    | n:  31
.calib done
Test: [  0/391]	Time  3.789 ( 3.789)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
NAHNAHANAHNANHHNAHNAHNAHNAHNHNAHNAHN
 * Prec@1 0.000 Prec@5 0.000
Time: 8.85
-----------------------------------------------------
